[{"title":"Spring注解","path":"/2023/12/29/Spring/Conditional注解/","content":"注解 功能 @Bean 容器中注册组件 @Primary 同类组件如果有多个，标注主组件 @DependsOn 组件之间声明依赖关系 @Lazy 组件懒加载（最后使用的时候才创建） @Scope 声明组件的作用范围(SCOPE_PROTOTYPE,SCOPE_SINGLETON) @Configuration 声明这是一个配置类，替换以前配置文件 @Component @Controller、@Service、@Repository @Indexed 加速注解，所有标注了 @Indexed 的组件，直接会启动快速加载 @Order 数字越小优先级越高，越先工作 @ComponentScan 包扫描 @Conditional 条件注入 @Import 导入第三方jar包中的组件，或定制批量导入组件逻辑 注解 功能 @ImportResource 导入以前的xml配置文件，让其生效 @Profile 基于多环境激活 @PropertySource 外部properties配置文件和JavaBean进行绑定.结合ConfigurationProperties @PropertySources @PropertySource组合注解 @Autowired 自动装配 @Qualifier 精确指定 @Value 取值、计算机环境变量、JVM系统。xxxx。@Value(“${xx}”) @Lookup 单例组件依赖非单例组件，非单例组件获取需要使用方法 @Conditional注解​ 作用是按照一定的条件进行判断，满足条件给容器注册bean。 @Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Conditional &#123;\t/** * All &#123;@link Condition&#125; classes that must &#123;@linkplain Condition#matches match&#125; * in order for the component to be registered. */\tClass&lt;? extends Condition&gt;[] value();&#125; @FunctionalInterfacepublic interface Condition &#123;\t/** * Determine if the condition matches. * @param context the condition context * @param metadata the metadata of the &#123;@link org.springframework.core.type.AnnotationMetadata class&#125; * or &#123;@link org.springframework.core.type.MethodMetadata method&#125; being checked * @return &#123;@code true&#125; if the condition matches and the component can be registered, * or &#123;@code false&#125; to veto the annotated component&#x27;s registration */\tboolean matches(ConditionContext context, AnnotatedTypeMetadata metadata);&#125; matches方法返回true则注入bean，返回false则不注入bean。 &#x3D;&#x3D;一个方法只能注入一个bean实例，所以@Conditional标注在方法上只能控制一个bean实例是否注入。&#x3D;&#x3D; &#x3D;&#x3D;一个类中可以注入很多实例，@Conditional标注在类上就决定了一批bean是否注入。&#x3D;&#x3D; 多个条件类：前言中说，@Conditional注解传入的是一个Class数组，存在多种条件类的情况。 @Conditional(&#123;WindowsCondition.class,ObstinateCondition.class&#125;)@Configurationpublic class BeanConfig &#123; @Bean(name = &quot;bill&quot;) public Person person1()&#123; return new Person(&quot;Bill Gates&quot;,62); &#125; @Bean(&quot;linus&quot;) public Person person2()&#123; return new Person(&quot;Linus&quot;,48); &#125;&#125; ​ 第一个条件类实现的方法返回true，第二个返回false，则结果false，不注入进容器。 ​ 第一个条件类实现的方法返回true，第二个返回true，则结果true，注入进容器中。","categories":["Spring"]},{"title":"Spring5.3.X源码搭建","path":"/2023/12/29/Spring/Spring5.3.X源码搭建/","content":"Spring5.3.X源码环境搭建一、准备阶段Spring5.3.X JDK11:https://www.oracle.com/java/technologies/downloads/#java11 Gradel:7.5.1:https://gradle.org/releases/ IDEA 克隆代码Gitee:https://gitee.com/mirrors/spring-frameworkGitHub:https://github.com/spring-projects/spring-framework 具体方法不再赘述 二、修改配置2.1、修改Gradle配置 修改 源码根目录下settings.gradle 文件，添加上阿里云的 maven 仓库 maven &#123; url &quot;https://maven.aliyun.com/repository/public&quot; &#125; 修改 源码根目录下的build.gradle文件，添加上阿里云的 maven 仓库 maven &#123; url &#x27;https://maven.aliyun.com/repository/public&#x27; &#125;maven &#123; url &#x27;https://maven.aliyun.com/repository/gradle-plugin&#x27;&#125; 2.2、修改ideaGradle配置 ​ 导入spring5源码的时候，会自动下载gradle的版本，建议大家直接让idea去拉取gradle的版本（我们配置了阿里云镜像，大概十几分钟），gradle的版本和spring5的版本不匹配，会有各种各样的问题，建议大家这么操作。 2.2.1 本地下载​ 这一步一定要注意一点，你的gradle版本不要自己去搜索最新版本的安装，一定要按照你clone下来的代码中官方所用的版本来，具体到这里，在clone下来的代码对应分支的gradle包下，有一个gradle-wrapper.properties文件中的如下参数指定： 2.3、导入依赖​ 此时可以导入Gradle的依赖 2.3.1、异常情况 出现’io.spring.gradle-enterprise-conventions’ version ‘0.0.2’ 相关异常的话，可以直接在根目录的build.gradle注释掉它： Kotlin: warnings found and -Weeror specified 缺少cglib、objenesis包，解决步骤如下：双击加载cglibRepackJar和objenesisRepackJar； &#x3D;&#x3D;若上述操作后，依旧报错，可使用如下强制措施，关闭Kotlin的-Weeror校验，删除-Weeror。&#x3D;&#x3D; &#x3D;&#x3D;！！！！！如果依旧报错，注释掉Spring-beans.gradle配置文件的 options.compilerArgs +&#x3D; “-Werror”&#x3D;&#x3D; 三、测试阶段1、使用gradle创建一个新模块 jdk一定要和项目的jdk相同！！！ 2、在Resource目录下创建beans.xml配置文件&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt;\t&lt;bean class=&quot;com.miller.test.bean.Person&quot; id=&quot;person&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;张三&quot;/&gt;\t&lt;/bean&gt;\t&lt;bean class=&quot;com.miller.test.bean.Cat&quot; id=&quot;cat&quot; &gt;&lt;!-- &lt;constructor-arg--&gt; &lt;property name=&quot;name&quot; value=&quot;张三的猫&quot;/&gt; &lt;/bean&gt;&lt;!--\txml 转为 BeanDefinition 这个java对象--&gt;&lt;!--\t&lt;import resource=&quot;&quot;--&gt;&lt;/beans&gt; 3、新建一个person类package com.miller.test.bean;import org.springframework.beans.BeansException;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Lookup;import org.springframework.context.ApplicationContext;import org.springframework.context.ApplicationContextAware;import org.springframework.context.MessageSource;import org.springframework.context.MessageSourceAware;import org.springframework.stereotype.Component;/** * Aware接口；帮我们装配Spring底层的一些组件 * 1、Bean的功能增强全部都是有 BeanPostProcessor+InitializingBean （合起来完成的） * 2、骚操作就是 BeanPostProcessor+InitializingBean * * 你猜Autowired是怎么完成的 * */@Componentpublic class Person implements ApplicationContextAware, MessageSourceAware &#123;//\t@Autowired\tApplicationContext context; //可以要到ioc容器\tMessageSource messageSource;\tpublic Person()&#123; System.out.println(&quot;person创建....&quot;);\t&#125;////\tpublic ApplicationContext getContext() &#123;// return context;//\t&#125;\tprivate String name;//\t@Autowired 依赖的组件是多实例就不能Autowired\tprivate Cat cat;\tpublic void setName(String name) &#123; this.name = name;\t&#125;\tpublic String getName() &#123; return name;\t&#125;\t@Autowired //去发现一下.....\tpublic void setCat(Cat cat) &#123; this.cat = cat;\t&#125;\t//\t@Lookup //去容器中找。@Bean的这种方式注册的Person @Lookup不生效\tpublic Cat getCat() &#123; return cat;\t&#125;\t@Override\tpublic String toString() &#123; return &quot;Person&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;;\t&#125;\tpublic ApplicationContext getContext() &#123; return context;\t&#125;\t@Override\tpublic void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; //利用回调机制，把ioc容器传入 this.context = applicationContext;\t&#125;\t@Override\tpublic void setMessageSource(MessageSource messageSource) &#123; this.messageSource = messageSource;\t&#125;&#125; 4、创建一个main方法package com.miller.test;import com.miller.test.bean.Person;import org.springframework.context.support.ClassPathXmlApplicationContext;import java.io.IOException;public class MainTest &#123;\tpublic static void main(String[] args) throws IOException &#123; ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); Person bean = context.getBean(Person.class); System.out.println(bean);\t&#125;&#125; 四、大功告成​ 执行main方法，等待一段时间，第一次编译比较慢。。。。。 ​ 如果出现以下输出，说明搭建成功，剩下的就是一入源码深似海，从此Spring不再是路人。。。","categories":["Spring"]},{"title":"ConditionalOnMissingBean注解","path":"/2023/12/29/SpringBoot/@ConditionalOnMissingBean/","content":"@ConditionalOnMissingBean注解​ 配置类中有两个Computer类的bean，一个是笔记本电脑，一个是备用电脑。如果当前容器中已经有电脑bean了，就不注入备用电脑，如果没有，则注入备用电脑，这里需要使用到@ConditionalOnMissingBean。 //可以标注在类和方法上@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented//使用了@Conditional注解，条件类是OnBeanCondition@Conditional(&#123;OnBeanCondition.class&#125;)public @interface ConditionalOnMissingBean &#123; Class&lt;?&gt;[] value() default &#123;&#125;; String[] type() default &#123;&#125;; Class&lt;?&gt;[] ignored() default &#123;&#125;; String[] ignoredType() default &#123;&#125;; Class&lt;? extends Annotation&gt;[] annotation() default &#123;&#125;; String[] name() default &#123;&#125;; SearchStrategy search() default SearchStrategy.ALL;&#125; 两者区别：@ConditionOnBean在判断list的时候，如果list没有值，返回false，否则返回true @ConditionOnMissingBean在判断list的时候，如果list没有值，返回true，否则返回false，其他逻辑都一样 例子：@ConditionalOnBean(javax.sql.DataSource.class)Spring容器或者所有父容器中需要存在至少一个javax.sql.DataSource类的实例","categories":["Spring"]},{"title":"5、JVM执行引擎","path":"/2023/12/05/JVM/5、执行引擎/","content":"","categories":["JVM"]},{"title":"6、对象存活判定算法、GC算法、STW、GC种类详解","path":"/2023/12/05/JVM/6、对象存活判定算法、GC算法、STW、GC种类详解/","content":"引言​ 程序计数器、虚拟机栈、本地方法栈三个区域随线程而生，伴线程而亡。而运行期间，每个栈桢所需的空间大小在编译期就大致确定。因此这几个区域的内存分配和回收都具备确定性，在这些区域内不需要过多考虑回收的问题，因为会随着线程或方法栈桢的销毁而自动回收。 ​ Java堆空间和元数据空间，这部分区域的内存分配和回收都是动态的，而GC机制所关注的是就是这块区域。 ​ 这两块区域是运行时数据区中的共享区，并且因为多态的概念，在运行时，一个类不同的子类实例，所需的内存空间是不同的，也包括一个方法不同的方法版本所需的空间也是不同的，所以只有在程序处于运行期间时才能知道会具体创建哪些对象。 一、如何判定存活对象​ “垃圾”是指运行过程中已经没有任何指针指向的对象。 ​ 主要是两种判定算法，引用计数算法，可达性分析算法 1.1、引用计数算法​ 创建出的每个对象自身都携带一个引用计数器，主要用于记录自身的引用情况。当一个指针指向当前对象时，该计数器会+1，如下： Object obj &#x3D; new Object(); ​ 当Object的对象实例被创建出来后，计数器会被初始化为1，因为局部变量obj的指针引用了该实例对象。而后续执行过程中，又有另外一个变量引用该实例时，该对象的引用计数器会+1。而当方法执行结束，栈帧中局部变量表中引用该对象的指针随之销毁时，当前对象的引用计数器会-1。当一个对象的计数器为0时，代表当前对象已经没有指针引用它了，那么在GC发生时，该对象会被判定为“垃圾”，然后会被回收。 这种判断算法的优势在于：实现简单，垃圾便于辨识，判断效率高，回收没有延迟性。 该算法一方面因为需要额外存储计数器，以及每次引用指向或消失时都需要同步更新计数器，所以增加了存储成本和时间开销； 另一方面存在一个致命缺陷，这种算法无法处理两个对象相互引用这种引用循环的状况。 1.2、可达性分析算法（或根搜索算法、追踪性垃圾收集） 可达性分析算法不仅同样具备实现简单和执行高效，而且可以解决循环引用的问题，防止内存泄漏的发生 Java使用的算法 ​ 在该算法中存在一个GCRoots的概念，在GC发生时，会以这些GCRoots作为根节点，然后从上至下的方式进行搜索分析，搜索走过的路线则被称为Reference Chain引用链。当一个对象没有任何引用链相连时，则会被判定为该对象是不可达的，即代表着此对象不可用，最终该对象会被判定为“垃圾”对象等待回收。 1.2.1可以作为GCRoots的对象 虚拟机栈中引用的对象 各个线程被调用的方法中使用的参数、局部变量等 本地方法栈JNI（通常说的本地方法）引用的对象 类静态属性引用的对象 Java类的引用类型静态变量 方法区中常量引用的对象 字符串常量池（String Table） 里的引用 所有被同步锁（Sychronized）持有的对象 Java虚拟机内部的引用 基本数据类型对应的class对象，一些异常(NullPointerException,OutOfMemoryException)，系统类加载器 二、垃圾回收算法2.1、标记-清除算法​ 在标记阶段会根据可达性分析算法，通过根节点标记堆中所有的可达对象，而这些对象则被称为堆中存活对象，反之，未被标记的则为垃圾对象。然后在清除阶段，会对于所有未标记的对象进行清除。 ​ 标记-清除算法是最初的GC算法，因为在标记阶段需要停下所有用户线程，也就是发生STW，而标记的时候又需要遍历整个堆空间中的所有GcRoots，所以耗时比较长，对于客户端而言，可能会导致GC发生时，造成很长一段时间内无响应。同时，因为堆空间中的垃圾对象是会分散在内存的各个角落，所以一次GC之后，会造成大量的内存碎片，也就是通过标记-清除算法清理出来的内存是不连续的，为了解决这个问题，JVM就不得不再额外维持一个内存的空闲列表，这又是一种开销。而且在分配数组对象或大对象时，连续的内存空间资源又会变得很匮乏。 2.2、复制算法​ 复制算法会将原有的堆内存分成两块，在同一时刻只会使用一块内存用于对象分配。在发生GC时，首先会将使用的那块内存区域的存货对象复制到未使用的内存中。等复制完之后，对当前内存进行全面清除回收，清除完之后，交换两块内存的角色，最后GC结束。 复制算法带来的好处是显而易见的，因为每次GC都是直接对半边区域进行回收，所以回收之后不需要考虑内存碎片的复杂情况，在内存分配时直接可以使用简单高效的 指针碰撞 方式分配对象。 ​ 这种算法最大的问题在于对内存的浪费，因为实际内存分配时只会使用一块内存，所以在内存分配时，内存直接缩水一半。同时，对象的移动的开销也需要考虑在内，所以想使用这种算法，对象的存活率要非常低才行。 一般采用复制算法来收集新生代空间，因为新生代中95%的对象是朝生夕死的。 2.3、标记-压缩算法​ 标记-压缩算法适用于存活率较高的场景。 ①标记阶段：和标-清算法一样。在标记阶段时也会基于GcRoots节点遍历整个内存中的所有对象，然后对所有存活对象做一次标记。 ②整理阶段：在整理阶段该算法并不会和标-清算法一样简单的清理内存，而是会将所有存活对象移动（压缩）到内存的一端，然后对于存活对象边界之外的内存进行统一回收。 ​ 标记 ​ 触发GC ​ 当所有存活对象全部被压缩到内存的一端后，GC机制会开始对于存活对象边界之外的内存区域进行统一回收，回收掉这些内存区域之后，最后再把存活对象的GC标志复位，然后GC结束 它的整体收集效率并不高。因为标-整算法不仅仅要标记对象，同时还要移动存活对象，所以整个GC过程下来，它所需要耗费的时间资源开销必然是不小的。 2.4、垃圾收集器总结 收集速度：复制算法 &gt; 标-清算法 &gt; 标-整算法 内存整齐度：复制算法 &#x3D; 标-整算法 &gt; 标-清算法 内存利用率：标-整算法 &gt; 标-清算法 &gt; 复制算法 三、内存泄漏与内存溢出3.1、内存溢出​ 垃圾回收跟不上内存消耗的速度。没有空闲内存，并且垃圾收集器也无法提供更多内存。 3.1.1、内存不够的原因​ （1）Java虚拟机的堆内存设置不够 ​ 比如我们要处理比较可观的数据量，但是没有显式指定JVM堆大小或者指定数值偏小。我们可以通过设置 -Xms和-Xmx来调整。 ​ （2）代码中创建了大量大对象，并且长时间不能被垃圾收集器收集（存在被引用） 3.2、内存泄漏3.2.1、内存泄漏的8种情况​ （1）静态集合类 ​ 如静态的LinkedList和HashMap。他们的生命周期与JVM一致。 ​ 长生命周期对象持有短生命周期对象的引用，尽管短生命周期对象不再使用，但是因为长生命周期对象持有它的引用而不能被回收。 public class MemoryLeak&#123; static List list = new ArrayList(); public void oomTest()&#123; Object obj = new Object();//局部变量 list.add(obj); &#125;&#125; ​ （2）单例模式 ​ 因为单例模式的静态性，生命周期和JVM一样。所以如果单例模式持有外部对象的引用，那么这个外部对象也不会被回收。 ​ （3）内部类持有外部类 ​ （4）各种连接，如数据库连接、网络连接和IO连接 ​ （5）变量不合理的作用域 ​ （6）改变哈希值 ​ p1不会被移除 ​ （7）缓存泄漏 ​ 项目启动时，加载数据到缓存。 ​ 对于这个问题，可以使用WeakHashMap代表缓存，此种map的特点是，当除了自身有对key的引用外，此key没有其他引用那么此map会自动丢弃该值。 ​ （8）监听器和回调","categories":["JVM"]},{"title":"7、分代GC器、CMS收集器及YoungGC、FullGC日志剖析","path":"/2023/12/05/JVM/7、分代GC器、CMS收集器及YoungGC、FullGC日志剖析/","content":"引言","categories":["JVM"]},{"title":"4、JVM对象内存布局","path":"/2023/12/05/JVM/4、对象内存布局/","content":"一、Java对象在内存中的布局 Java对象一般在内存中的布局通常由对象头、实例数据、对齐填充三部分组成。 1.1、对象头（Object Header）​ Java对象头包含了运行时元数据MarkWord和类型指针（ClassMetadataAddress&#x2F;KlassWord），如果是数组对象，还会存在数组长度 虚拟机位数 对象头结构信息 说明 大小 32位 MarkWord HashCode、分代年龄、是否偏向锁和锁标记位 4byte&#x2F;32bit 32位 ClassMetadataAddress&#x2F;KlassWord 类型指针指向对象的类元数据，JVM通过这个指针确定该对象是哪个类的实例 4byte&#x2F;32bit 32位 ArrayLenght 如果是数组对象存储数组长度，非数组对象不存在 4byte&#x2F;32bit 虚拟机位数 对象头结构信息 说明 大小 64位 MarkWord unused、HashCode、分代年龄、是否偏向锁和锁标记位 8byte&#x2F;64bit 64位 ClassMetadataAddress&#x2F;KlassWord 类型指针指向对象的类元数据，JVM通过这个指针确定该对象是哪个类的实例 8byte&#x2F;64bit 开启指针压缩的情况下为4byte&#x2F;32bit 64位 ArrayLenght 如果是数组对象存储数组长度，非数组对象不存在 4byte&#x2F;32bit 1.2、实例数据（Instance Data）​ 实例数据是指一个聚合量所有标量的总和，也就是是指当前对象属性成员数据以及父类属性成员数据。 1.3、对齐填充（Padding）​ 对象填充在一个对象中可能存在，也可能是不存在的。因为在64bit的虚拟机中，《虚拟机规范》中规定了：为了方便内存的单元读取、寻址、分配，Java对象的总大小必须为8的整数倍，所以当一个对象的对象头+实例数据大小不为8的整数倍的时候，就需要对齐填充，填充为8的整数倍。 1.4、指针压缩（CompressedOops）​ 指针压缩属于JVM的一种优化思想，一方面可以节省很大的内存开支，第二方面也可以方便JVM跳跃寻址（稍后分析），在64bit的虚拟机中为了提升内存的利用率，所以出现了指针压缩这一技术，指针压缩的技术会将Java程序中的所有引用指针(类型指针、堆引用指针、栈帧内变量引用指针等)都会压缩一半，而在Java中一个指针的大小是占一个字宽单位的，在64bit的虚拟机中一个字宽的大小为64bit，所以也就意味着在64位的虚拟机中，指针会从原本的64bit压缩为32bit的大小，而指针压缩这一技术在JDK1.7之后是默认开启的。 而在JVM中开启指针压缩后，对于对象位置的寻址计算存在三种方式，如下： ①如果堆的高位地址小于32GB，说明不需要基址base就能定位堆中任意对象，这种模式被称为Zero-based Compressed Oops Mode，计算公式如下： 计算公式：add&#x3D;0+offset∗8add &#x3D; 0 + offset * 8 add&#x3D;0+offset∗8 计算前提：highheap&lt;32GBhigh_{heap} &lt; 32GBhighheap&lt;32GB ②如果堆高位大于等于32GB，说明需要base基地址，这时如果堆空间小于4GB，说明基址+偏移能定位堆中任意对象，如下： 计算公式：add&#x3D;base+offsetadd &#x3D; base + offset add&#x3D;base+offset 计算前提：sizeheap&lt;4GBsize_{heap} &lt; 4GBsizeheap&lt;4GB ③如果堆空间大小处于4GB与32GB之间，这时只能通过基址+偏移x缩放scale（Java中缩放为8），才能定位堆中任意对象，如下： 计算公式：add&#x3D;base+offset∗8add &#x3D; base + offset * 8 add&#x3D;base+offset∗8 计算前提：4GB&lt;&#x3D;sizeheap&lt;32GB4GB &lt;&#x3D; size_{heap} &lt; 32GB4GB&lt;&#x3D;sizeheap&lt;32GB 二、Java对象分配过程​ Java中创建对象最常用的方式就是new关键字，但除了new关键字，也存在其他方式： 通过调用Class类的newInstance方法完成对象创建 通过反射机制调用Constructor类的newInstance方法完成对象创建 类实现Clonable接口，通过clone方法克隆对象完成创建 从本地文件、网络中读取二进制流数据，通过反序列化完成创建 使用第三方库Objenesis完成对象创建 ​ 无论哪种方式创建对象，虚拟机都会将创建对象分为三步：类加载检测、内存分配以及对象头设置。 ​ ​ &#x3D;&#x3D;创建对象的步骤&#x3D;&#x3D; 1、判断对象对应的类是否加载、链接、初始化 2、为对象分配内存 ​\t2.1、指针碰撞 ​\t2.2、空闲列表 3、处理并发安全问题 4、初始化分配到空间 5、设置对象的对象头 6、执行init方法进行初始化 2.1、类加载机制​ 当虚拟机遇到一条创建指令时，首先会去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，同时检测这个符号引用代表的类是否被加载解析初始化过。如果没有，在双亲委派机制下，使用当前类加载器以当前创建对象的全限定名作为key值进行查找对应的.class文件，如果没有找到，则抛出classNotFoundException异常，找到了则先完成类加载过程，完成加载后，再开始为其对象分配内存。 2.2、内存分配​ 当一个对象的类已经被加载后，会依据第一阶段分析的方式去计算出该对象所需的内存空间大小，计算出大小后会开始对象分配过程，而内存分配就是指在内存中划出一块与对象大小相等的区域出来，然后将对象放进去的过程。但需要额外注意的是：Java的对象并不是直接一开始就尝试在堆上进行分配的，分配过程如下： 2.2.1、栈上分配​ 如果对象被分配在栈上，那么该对象就无需GC机制回收它，该对象会随着方法栈桢的销毁随之自动回收。但如果一个对象的大小超过了栈可用空间（栈总大小-已使用空间），那么此时就不会尝试将对象进行栈上分配。 2.2.2、TLAB分配​ TLAB全称叫做Thread Local Allocation Buffer，是指JVM在Eden区为每条线程分配的一块私有缓冲内存（为了防止多个线程竞争同一块内存区域分配对象）。 2.2.3、年老代分配​ 初次分配时，大对象直接进入年老代。 一般对象进入年老代的情况只有三种：大对象、长期存活对象以及动态年龄判断符合条件的对象，在JVM启动的时候你可以通过-XX:PretenureSizeThreshold参数指定大对象的阈值，如果对象在分配时超出这个大小，会直接进入年老代。 2.2.4、新生代分配​ 如果栈上分配、TLAB分配、年老代分配都未成功，此时就会来到Eden区尝试新生代分配。而在新生代分配时，会存在两种分配方式： ①指针碰撞：指针碰撞是Java在为对象分配堆内存时的一种内存分配方式，一般适用于Serial、ParNew等不会产生内存碎片、堆内存完整的的垃圾收集器。 分配过程：堆中已用分配内存和为分配的空闲内存分别会处于不同的一侧，通过一个指针指向分界点区分，当JVM要为一个新的对象分配内存时，只需把指针往空闲的一端移动与对象大小相等的距离即可。 ②空闲列表：与指针碰撞一样，空闲列表同样是Java在为新对象分配堆内存时的一种内存分配方式，一般适用于CMS等一些会产生内存碎片、堆内存不完整的垃圾收集器。 分配过程：堆中的已用内存和空闲内存相互交错，JVM通过维护一张内存列表记录可用的空闲内存块信息，当创建新对象需要分配内存时，从列表中找到一个足够大的内存块分配给对象实例，并同步更新列表上的记录，当GC收集器发生GC时，也会将已回收的内存更新到内存列表 三、一个对象从生到死的历程3.1、对象的访问方式​ 在Java中对象都是通过reference访问的，reference主要分为两种访问方式，一种为句柄访问，另一种为直接指针访问。 3.1.1、句柄访问​ Java堆中会专门划分出一块内存区域作为句柄池，用于存储所有引用地址，reference中存储的就是对象的句柄地址，句柄包含对象实例数据和类型数据的信息，如下： ​ 当需要使用对象时，会先访问reference中存储的句柄地址，然后根据句柄地址中存储的实际内存地址再次定位后，访问对象在内存中的数据。 ​ 好处：reference中存储稳定句柄地址，对象被移动（垃圾收集时对象移动很普遍）时只会改变句柄中实例数据指针，reference本身不需要 3.1.2、直接指针访问​ 如果采用直接指针的方式访问，那么reference中存储的就是对象在堆中的内存地址，而类型指针则放到了对象头中存储 ​ 这种访问模式下，当需要使用对象时，可以直接通过reference中存储的堆内存地址定位并访问对象数据。 HotSpot虚拟机中是采用指针的访问方式，通过直接指针定位并访问对象数据（但使用Shenandoah收集器的话，也会有一次额外的转发）。 四、对象引用类型-强软弱虚全面分析4.1 强引用类型(StrongReference)​ 通过new指令创建出来的对象都属于强引用。 Object obj &#x3D; new Object(); ​ 通过new创建出来的Object实例会被分配在堆中存储，而变量obj会被放在当前方法对应的栈桢中的局部变量表中存储，在运行时可以直接通过obj变量操作堆中的实例对象，那么obj就是该Object实例对象的强引用。 4.2、软引用类型(SoftReference)软引用是指使用java.lang.ref.SoftReference类型修饰的对象，当一个对象只存在软引用时，在堆内存不足的情况下，该引用级别的对象将被GC机制回收。不过当堆内存还充足的情况下，该引用级别的对象是不会被回收的，所以平时如果需要实现JVM级别的简单缓存，那么可以使用该级别的引用类型实现。使用案例如下： SoftReference&lt;HashMap&gt; cacheSoftRef = new SoftReference&lt;HashMap&gt;(new HashMap&lt;Object,Object&gt;());cacheSoftRef.get().put(&quot;竹子&quot;,&quot;熊猫&quot;);System.out.println(cacheSoftRef.get().get(&quot;竹子&quot;)); 如上案例中便通过软引用类型实现了一个简单的缓存器。 4.3弱引用类型（WeakReference）​ 弱引用类型是指使用java.lang.ref.WeakReference类型修饰的对象，与软引用的区别在于：弱引用类型的对象生命周期更短，因为弱引用类型的对象只要被GC发现，不管当前的堆内存资源是否紧张，都会被GC机制回收。不过因为GC线程的优先级比用户线程更低，所以一般不会立马发现弱引用类型对象，因此一般弱引用类型的对象也会有一段不短的存活周期。 从软引和弱引的特性上来看，它们都适合用来实现简单的缓存机制，用于保存那些可有可无的缓存数据，内存充足时可以稍微增加程序的执行效率，而内存紧张时会被回收，不会因此导致OOM。 4.4、虚引用类型(PhantomReference)虚引用也在有些地方被称为幽灵引用，虚引用是指使用java.lang.ref.PhantomReference类型修饰的对象，不过在使用虚引用的时候是需要配合ReferenceQueue引用队列才能联合使用。与其他的几种引用类型不同的是：虚引用不会决定GC机制对一个对象的回收权，如果一个对象仅仅存在虚引用，那么GC机制将会把他当成一个没有任何引用类型的对象，随时随刻可以回收它。不过它还有个额外的用途：跟踪垃圾回收过程，也正是由于虚引用可以跟踪对象的回收时间，所以也可以将一些资源释放操作放置在虚引用中执行和记录。","categories":["JVM"]},{"title":"Mysql主从复制","path":"/2023/11/29/MySQL/Mysql主从复制/","content":"Mysql主从复制一.搭建mysql1、下载MySQL的安装包 [root@localhost]# wget https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-8.0.20-linux-glibc2.12-x86_64.tar.xz 2、解压MySQL的压缩包，并对解压后的目录重命名 [root@localhost]# tar -xvJf mysql-8.0.20-linux-glibc2.12-x86_64.tar.xz[root@localhost]# mv mysql-8.0.20-linux-glibc2.12-x86_64 mysql8.0 3、卸载Linux自带的MariaDB [root@localhost]# rpm -qa | grep mariadbmariadb-libs-5.5.68-1.el7.x86_64[root@localhost]# rpm -e --nodeps mariadb-libs-5.5.56-2.el7.x86_64 4、创建一个data文件夹，用于存放数据 [root@localhost]# cd mysql8.0 &amp;&amp; mkdir data root@localhost: PsdkKQK!I6;8 5、配置环境变量 [root@localhost]# vi /etc/profile# 在最后面加一行：# export PATH=$PATH:/soft/mysql/mysql8.0/bin:/soft/mysql/mysql8.0/lib# 按ESC键，输入 :wq 保存退出[root@localhost]# source /etc/profile 6、创建一个MySql用户组 [root@localhost]# groupadd mysql[root@localhost]# useradd -g mysql mysql[root@localhost]# chown -R mysql.mysql /soft/mysql/mysql8.0 7、初始化mysql信息 [root@localhost]# cd /soft/mysql/mysql8.0/bin[root@localhost]# ./mysqld --user=mysql --basedir=/soft/mysql/mysql8.0 --datadir=/soft/mysql/mysql8.0/data/ --initialize 临时密码已创建 8、配置一下mysql配置文件 [root@localhost]# vi /etc/my.cnf[mysqld]basedir=/soft/mysql/mysql8.0datadir=/soft/mysql/mysql8.0/dataport=3306socket=/tmp/mysql.sockcharacter-set-server=UTF8MB4 9、配置mysql目录 [root@localhost]# cd /soft/mysql/mysql8.0[root@localhost]# vi support-files/mysql.server# 其中安装目录和数据目录默认为空，启动时会去加载/usr/local/mysql/目录basedir=datadir=# 记得将其修改为你自定义的安装目录和数据目录（最后不要加斜杠）basedir=/soft/mysql/mysql8.0datadir=/soft/mysql/mysql8.0/data 10、启动mysql [root@localhost]# ./support-files/mysql.server startStarting MySQL.Logging to &#x27;/soft/mysql/mysql8.0/data/localhost.localdomain.err&#x27;... SUCCESS! 二、一主一从搭建1.配置mysql主节点（master）1、修改配置文件 [root@localhost]# service mysql stop[root@localhost]# vi /etc/my.cnf[mysqld]basedir=/soft/mysql/mysql8.0datadir=/soft/mysql/mysql8.0/dataport=3306socket=/tmp/mysql.sockcharacter-set-server=UTF8MB4# ------接下来的是主从集群配置-------# 主库在主从集群中的唯一标识server-id=1# 开启bin-log日志，并为bin-log日志取个前缀名（有默认值可不写）log-bin=mysql-bin-log# 同步复制时过滤的库（主要将一些不需要备份/同步库写进来）# 也可以通过binlog-do-db=xx1,xx2... 来指定要复制的目标库binlog-ignore-db=mysql# 指定bin-log日志的格式为混合模式（默认为statement）binlog_format=mixed# 设置单个binlog日志文件的最大容量max_binlog_size=1024M 2、开放主节点端口 [root@localhost]# firewall-cmd --zone=public --add-port=3306/tcp --permanent[root@localhost]# firewall-cmd --reload[root@localhost]# firewall-cmd --zone=public --list-ports 3、创建用于数据同步的账号 [root@localhost]# service mysql start[root@localhost]# mysql -uroot -pmysql&gt; create user &#x27;dongnan&#x27;@&#x27;%&#x27; identified with mysql_native_password by &#x27;12345678&#x27;;mysql&gt; grant replication slave on *.* to &#x27;dongnan&#x27;@&#x27;%&#x27;; 2.配置mysql从节点（slave）1、修改配置文件 [root@localhost]# service mysql stop[root@localhost]# vi /etc/my.cnf[mysqld]basedir=/soft/mysql/mysql8.0datadir=/soft/mysql/mysql8.0/dataport=3306socket=/tmp/mysql.sockcharacter-set-server=UTF8MB4# ------接下来的是主从配置-------# 从库在主从集群中的唯一标识server-id=2# 开启bin-log日志（为了主从切换时使用，不开启bin-log的从机只能当备库使用）log-bin=mysql-bin-log# 同步复制时要过滤的库binlog-ignore-db=mysql# 指定bin-log日志的格式为混合模式binlog_format=mixed# 设置单个binlog日志文件的最大容量max_binlog_size=1024M# 开启relay-log日志（同样可以指定前缀名）relay_log=mysql-relay-log# 开启存储过程、函数、触发器等内容的同步功能log_bin_trust_function_creators=true# 同步执行跳过一些错误码（防止同步写入时出现错误导致复制中断）slave_skip_errors=1062 2、开放从节点端口 [root@localhost]# firewall-cmd --zone=public --add-port=3306/tcp --permanent[root@localhost]# firewall-cmd --reload[root@localhost]# firewall-cmd --zone=public --list-ports 3、创建用于数据同步的账号 [root@localhost]# service mysql start[root@localhost]# mysql -uroot -pmysql&gt; change master to master_host=&#x27;10.211.55.7&#x27;, master_user=&#x27;dongnan&#x27;, master_password=&#x27;12345678&#x27;, master_port=3306, master_log_file=&#x27;mysql-bin-log.000001&#x27;, master_log_pos=658; 4、启动从库线程 mysql&gt; start slave;mysql&gt; show slave status\\G; Slave_IO_Running、Slave_SQL_Running两个线程的状态都为Yes时，说明主从集群搭建完成。 三、GTID复制、无损复制1、GTID复制 在前面配置从节点时，需要咱们手动指定master_log_pos=653同步点，而开启GTID复制后则可根治该问题 需要先停止从库的同步线程 mysql&gt; stop slave; 主节点退出连接，停止mysql服务 mysql&gt; quit[root@localhost]# service mysql stop 更改两个节点的&#x2F;etc&#x2F;my.cnf配置文件 # 开启GTID复制gtid_mode=on# 跳过一些可能导致执行出错的SQL语句enforce-gtid-consistency=true 接着重启两个节点，登陆从节点，与主库建立连接 change master to master_host=&#x27;10.211.55.7&#x27;, master_user=&#x27;dongnan&#x27;, master_password=&#x27;12345678&#x27;, master_port=3306, master_auto_position=1; 从库上启动slave线程 start slave; 2、无损复制安装插件 主节点执行：INSTALL PLUGIN rpl_semi_sync_master SONAME &#x27;semisync_master.so&#x27;;从节点执行：INSTALL PLUGIN rpl_semi_sync_slave SONAME &#x27;semisync_slave.so&#x27;; 停止同步线程、退出连接、关闭MySQL服务，接着再次修改主从的/etc/my.cnf配置文件 # 主节点上新增配置：# 开启无损复制rpl_semi_sync_master_enabled=on# 等待从节点ACK的时长（不指定默认为10s）rpl_semi_sync_master_timeout=3000# 从节点上新增配置：rpl_semi_sync_slave_enabled=on","categories":["MYSQL"]},{"title":"ElasticSearch","path":"/2023/11/27/ElasticSearch/","content":"一、前言Elasticsearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎。Elasticsearch用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。官方客户端在Java、.NET（C#）、PHP、Python、Apache Groovy、Ruby和许多其他语言中都是可用的。根据DB-Engines的排名显示，Elasticsearch是最受欢迎的企业搜索引擎&#x3D;&#x3D;，其次是Apache Solr，也是基于Lucene。小编也是菜，还是跟着尚硅谷的视频来一样的7.4.2版本，害怕学到后面再来个版本不兼容，直接凉凉哈！！ 二、安装ElasticSearch1. 拉去ES镜像 docker pull elasticsearch:7.4.2 请添加图片描述 2. 创建要挂载的文件与配置信息（&#x3D;&#x3D;为了数据共享，在虚拟机里修改自动同步到容器中&#x3D;&#x3D;） mkdir -p /mydata/elasticsearch/configmkdir -p /mydata/elasticsearch/data# 任何地址都可以访问echo &quot;http.host: 0.0.0.0&quot; &gt;/mydata/elasticsearch/config/elasticsearch.yml 3. 赋予文件写的权限，不然挂载不会同步 # 赋予最高权限chmod -R 777 /mydata/elasticsearch/ 请添加图片描述 4. 启动Elastic search docker run --name elasticsearch -p 9200:9200 -p 9300:9300 \\-e &quot;discovery.type=single-node&quot; \\# 配置参数，防止ES占用内存过大，大概1GB,我们给他缩小一下-e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot; \\-v /mydata/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\-v /mydata/elasticsearch/data:/usr/share/elasticsearch/data \\-v /mydata/elasticsearch/plugins:/usr/share/elasticsearch/plugins \\-d elasticsearch:7.4.2 请添加图片描述 5. windows测试访问（ip+9200） 请添加图片描述 6. 设置开机自启动 在这里插入图片描述 三、安装Kibana可视化工具1. 拉去Kibana镜像 docker pull kibana:7.4.2 请添加图片描述 2. 启动Kibana # 地址为自己虚拟机的ipdocker run --name kibana -e ELASTICSEARCH_HOSTS=http://192.168.17.130:9200 -p 5601:5601 -d kibana:7.4.2 请添加图片描述 3. windows测试访问（ip+5601，&#x3D;&#x3D;等待一分钟，不然出不来界面&#x3D;&#x3D;） **** 请添加图片描述 4. 设置开机自启动 在这里插入图片描述 四、总结这样我们就搭建完成了，可以进行测试了！本文是根据尚硅谷雷神的教学，特此记录一下，供以后查看！","categories":["ElasticSearch"]},{"title":"JDK1.8新特性","path":"/2023/11/27/JDK/","content":"OptionalOptional 类是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。 序号 方法 &amp; 描述 1 **static Optional empty()**返回空的 Optional 实例。 2 **boolean equals(Object obj)**判断其他对象是否等于 Optional。 3 **Optional filter(Predicate&lt;? super predicate)**如果值存在，并且这个值匹配给定的 predicate，返回一个Optional用以描述这个值，否则返回一个空的Optional。 4 ** Optional flatMap(Function&lt;? super T,Optional&gt; mapper)**如果值存在，返回基于Optional包含的映射方法的值，否则返回一个空的Optional 5 **T get()**如果在这个Optional中包含这个值，返回值，否则抛出异常：NoSuchElementException 6 **int hashCode()**返回存在值的哈希码，如果值不存在 返回 0。 7 **void ifPresent(Consumer&lt;? super T&gt; consumer)**如果值存在则使用该值调用 consumer , 否则不做任何事情。 8 **boolean isPresent()**如果值存在则方法会返回true，否则返回 false。 9 **Optional map(Function&lt;? super T,? extends U&gt; mapper)**如果有值，则对其执行调用映射函数得到返回值。如果返回值不为 null，则创建包含映射返回值的Optional作为map方法返回值，否则返回空Optional。 10 **static Optional of(T value)**返回一个指定非null值的Optional。 11 **static Optional ofNullable(T value)**如果为非空，返回 Optional 描述的指定值，否则返回空的 Optional。 12 **T orElse(T other)**如果存在该值，返回值， 否则返回 other。 13 **T orElseGet(Supplier&lt;? extends T&gt; other)**如果存在该值，返回值， 否则触发 other，并返回 other 调用的结果。 14 ** T orElseThrow(Supplier&lt;? extends X&gt; exceptionSupplier)**如果存在该值，返回包含的值，否则抛出由 Supplier 继承的异常 15 **String toString()**返回一个Optional的非空字符串，用来调试 StreamforEachStream 提供了新的方法 ‘forEach’ 来迭代流中的每个数据。以下代码片段使用 forEach 输出了10个随机数： Random random = new Random(); random.ints().limit(10).forEach(System.out::println); mapmap 方法用于映射每个元素到对应的结果，以下代码片段使用 map 输出了元素对应的平方数： List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5); // 获取对应的平方数 List&lt;Integer&gt; squaresList = numbers.stream().map( i -&gt; i*i).distinct().collect(Collectors.toList()); filterfilter 方法用于通过设置的条件过滤出元素。以下代码片段使用 filter 方法过滤出空字符串： List&lt;String&gt;strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;,&quot;&quot;, &quot;jkl&quot;); // 获取空字符串的数量 long count = strings.stream().filter(string -&gt; string.isEmpty()).count(); limitlimit 方法用于获取指定数量的流。 以下代码片段使用 limit 方法打印出 10 条数据： Random random = new Random(); random.ints().limit(10).forEach(System.out::println); sortedsorted 方法用于对流进行排序。以下代码片段使用 sorted 方法对输出的 10 个随机数进行排序： Random random = new Random(); random.ints().limit(10).sorted().forEach(System.out::println); 并行（parallel）程序并行（parallel）程序parallelStream 是流并行处理程序的代替方法。以下实例我们使用 parallelStream 来输出空字符串的数量： List&lt;String&gt; strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;,&quot;&quot;, &quot;jkl&quot;);// 获取空字符串的数量long count = strings.parallelStream().filter(string -&gt; string.isEmpty()).count(); 我们可以很容易的在顺序运行和并行直接切换。 CollectorsCollectors 类实现了很多归约操作，例如将流转换成集合和聚合元素。Collectors 可用于返回列表或字符串： List&lt;String&gt;strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;,&quot;&quot;, &quot;jkl&quot;);List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList()); System.out.println(&quot;筛选列表: &quot; + filtered);String mergedString = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.joining(&quot;, &quot;));System.out.println(&quot;合并字符串: &quot; + mergedString); 统计另外，一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上，它们可以用来产生类似如下的统计结果。 List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5); IntSummaryStatistics stats = numbers.stream().mapToInt((x) -&gt; x).summaryStatistics(); System.out.println(&quot;列表中最大的数 : &quot; + stats.getMax());System.out.println(&quot;列表中最小的数 : &quot; + stats.getMin());System.out.println(&quot;所有数之和 : &quot; + stats.getSum());System.out.println(&quot;平均数 : &quot; + stats.getAverage());","categories":["JDK"]},{"title":"JUC高级","path":"/2023/11/27/JUC笔记/JUC高级/","content":"JUC高级Synchronized关键字 ​ 一个对象里面如果有多个sychronized方法，某一个时刻内，只要一个线程去调用其中的一个sychronized方法了，其他线程都只能等待，换句话说，某一个时刻内，只能有唯一的一个线程去访问这些sychronized方法 ​ &#x3D;&#x3D;锁的是当前对象this&#x3D;&#x3D;，被锁定后，其它的线程都不能进入到当前对象的其它sychronized方法 ​ &#x3D;&#x3D;静态同步方法，锁的是类，不是对象&#x3D;&#x3D; ​ 对于同步方法块，锁的是 sychronized括号内的对象 悲观锁​ 认为自己在使用数据的时候一定有别的线程来修改数据，因此在获取数据的时候会先加锁，确保数据不会被别的线程修改。 ​ sychronized关键字和Lock的实现都是悲观锁 乐观锁​ &#x3D;&#x3D;乐观锁认为自己在修改数据时不会有别的线程修改数据&#x3D;&#x3D;，所以不会添加锁，只是在更新数据的时候去判断之前有没有别的线程更新了这个数据。如果这个数据没有被更新，当前线程将自己修改的数据成功写入。如果数据已被其他线程更新，则根据不同的实现方式执行不同的操作。 ​ 乐观锁在Java中是通过使用无锁编程来实现。&#x3D;&#x3D;最常采用的是CAS算法，Java原子类中的递增就是通过CAS自旋实现&#x3D;&#x3D; 公平锁和非公平锁 公平锁 按序排队，就是判断同步队列里是否还有先驱节点的存在，如果没有先驱节点才能获得锁 非公平锁 先占先得，只要能抢到同步状态就可以 Lock lock = new ReentrantLock();//默认非公平锁，分配不平均Lock lock = new ReentrantLock(true);//公平锁 可重入锁​ 可重入锁又名递归锁 是指在同一个线程在外层方法获取锁的时候，再进入该线程的内层方法会自动获取锁(前提，锁对象得是同一个对象)，不会因为之前已经获取过还没释放而阻塞。 ​ 如果是1个有 synchronized 修饰的递归调用方法，程序第2次进入被自己阻塞了岂不是天大的笑话，出现了作茧自缚。所以Java中ReentrantLock和synchronized都是可重入锁，可重入锁的一个优点是可一定程度避免死锁。 种类 ​ 隐式锁（即synchronized关键字使用的锁）默认是可重入锁 ​ 指的是可重复可递归调用的锁，在外层使用锁之后，在内层仍然可以使用，并且不发生死锁，这样的锁就叫做可重入锁。简单的来说就是：&#x3D;&#x3D;在一个synchronized修饰的方法或代码块的内部调用本类的其他synchronized修饰的方法或代码块时，是永远可以得到锁的 与可重入锁相反，不可重入锁不可递归调用，递归调用就发生死锁。&#x3D;&#x3D; public class ReEntryLockDemo&#123; public static void main(String[] args) &#123; final Object objectLockA = new Object(); new Thread(() -&gt; &#123; synchronized (objectLockA)&#123; System.out.println(&quot;-----外层调用&quot;); synchronized (objectLockA)&#123; System.out.println(&quot;-----中层调用&quot;); synchronized (objectLockA)&#123; System.out.println(&quot;-----内层调用&quot;); &#125; &#125; &#125; &#125;,&quot;a&quot;).start();&#125;&#125; jconsole命令 LockSupportwait和notify方法必须要在同步块或者方法里，且成对出现使用。先wait后notify才OK。 LockSupport是用来创建锁和其他同步类的基本线程阻塞原语 JMMp25","categories":["JUC"]},{"title":"线程中断机制","path":"/2023/11/27/JUC笔记/线程中断机制/","content":"线程中断​ 首先一个线程不应该由其他线程来强制中断或停止，而是应该由线程自己自行停止。所以，Thread.stop, Thread.suspend, Thread.resume 都已经被废弃了。 ​ 其次在Java中没有办法立即停止一条线程，然而停止线程却显得尤为重要，如取消一个耗时操作。因此，Java提供了一种用于停止线程的机制——中断。 ​ &#x3D;&#x3D;中断只是一种协作机制，Java没有给中断增加任何语法，中断的过程完全需要程序员自己实现。&#x3D;&#x3D; ​ 若要中断一个线程，你需要手动调用该线程的interrupt方法，该方法也仅仅是&#x3D;&#x3D;将线程对象的中断标识设成true；&#x3D;&#x3D;接着你需要自己写代码不断地检测当前线程的标识位，如果为true，表示别的线程要求这条线程中断，此时究竟该做什么需要你自己写代码实现。 ​ 每个线程对象中都有一个标识，用于表示线程是否被中断；该标识位为true表示中断，为false表示未中断；通过调用线程对象的interrupt方法将该线程的标识位设为true；可以在别的线程中调用，也可以在自己的线程中调用。 public void interrupt() 实例方法，实例方法interrupt()仅仅是设置线程的中断状态为true，不会停止线程 public static boolean interrupted() 静态方法，Thread.interrupted(); 判断线程是否被中断，并清除当前中断状态这个方法做了两件事： 1 返回当前线程的中断状态 2 将当前线程的中断状态设为false 这个方法有点不好理解，因为连续调用两次的结果可能不一样。 public boolean isInterrupted() 实例方法，判断当前线程是否被中断（通过检查中断标志位） volatile 1.线程的可见性：当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。 2.顺序一致性：禁止指令重排序。","categories":["JUC"]},{"title":"Docker","path":"/2023/11/27/Docker/","content":"Docker命令安装docker1、卸载旧版 sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 2、Install the yum-utils package (which provides the yum-config-manager utility) and set up the repository. sudo yum install -y yum-utils sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo 3、Install Docker Engine\tsudo yum install docker-ce docker-ce-cli containerd.io docker-compose-plugin4、Start Docker.\tsudo systemctl start docker5、删除包含某个字段的所有镜像\tdocker rmi $(docker images | grep &quot;dongnan&quot; | awk &#x27;&#123;print $3&#125;&#x27;) 6、停止包含某个字段的所有镜像\tdocker stop $(docker ps -a | grep &quot;xxx&quot; | awk &#x27;&#123;print $1&#125;&#x27;)7、删除所有未运行的容器（已经运行的删除不了，未运行的就一起被删除了）\tsudo docker rm $(sudo docker ps -a -q)1、docker停止所有正在运行的容器 docker stop $(docker ps -a | awk &#x27;&#123; print $1&#125;&#x27; | tail -n +2)2、docker删除所有容器 docker rm $(docker ps -a | awk &#x27;&#123; print $1&#125;&#x27; | tail -n +2)3、docker删除所有镜像 docker rmi $(docker images | awk &#x27;&#123;print $3&#125;&#x27; |tail -n +2) docker rmi $&#123;docker images -qa&#125; 下载镜像安装mysql1、拉取镜像\tsudo docker pull mysql:5.72、启动mysql容器\tsudo docker run --name mysql -v /mydata/mysql/data:/var/lib/mysql -v /mydata/mysql:/etc/mysql/conf.d -v /mydata/mysql/log:/var/log/mysql -e MYSQL_ROOT_PASSWORD=admin123456 -p 3306:3306 -d mysql:5.73、查看运行中的容器\tdocker ps4、进入mysql容器\tdocker exec -it mysql /bin/bash5、重启容器\tdocker restart mysql 安装redis1. 安装redisdocker pull redis:6.0.102. 修改需要自定义的配置(docker-redis默认没有配置文件，自己在宿主机建立后挂载映射)创建并修改/usr/local/redis/redis.confbind 0.0.0.0 开启远程权限appendonly yes 开启aof持久化3.docker run --name redis -v /usr/local/redis/data:/data -v /usr/local/redis/redis.conf:/usr/local/etc/redis/redis.conf -p 6379:6379 -d redis:6.0.10 redis-server /usr/local/etc/redis/redis.conf --requirepass admin123456docker run --restart=always --log-opt max-size=100m --log-opt max-file=2 -p 6379:6379 --name redis -v /home/redis/myredis/myredis.conf:/etc/redis/redis.conf -v /home/redis/myredis/data:/data -d redis redis-server /etc/redis/redis.conf --appendonly yes --requirepass 000415 安装ElasticSearch1.拉取镜像\tdocker pull elasticsearch:7.12.02.创建挂载目录\t比如在用户文件夹下创建\tmkdir -p /Users/dongnan/dockerMsg/elasticsearch/config\tmkdir -p /Users/dongnan/dockerMsg/elasticsearch/data\tcd /Users/dongnan/dockerMsg/elasticsearch/config\tvim elasticsearch.yml\t输入： http.host: 0.0.0.0\t保存退出\tsudo chmod -R 770 /Users/dongnan/dockerMsg/elasticsearch\t输入密码3.创建容器\tdocker run --name elasticsearch -p 9200:9200 -p 9300:9300 \\ -e &quot;discovery.type=single-node&quot; \\ -e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot; \\ -v /Users/dongnan/dockerMsg/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\ -v /Users/dongnan/dockerMsg/elasticsearch/data:/usr/share/elasticsearch/data \\ -v /Users/dongnan/dockerMsg/elasticsearch/plugins:/usr/share/elasticsearch/plugins \\ -d elasticsearch:7.16.24.查看es容器的ip\tdocker inspect containerId5.安装Kibana\tdocker run --name kibana -e ELASTICSEARCH_HOSTS=http://172.17.0.4:9200 -p 5601:5601 -d kibana:7.16.2docker安装RocketMQ启动nameserverdocker run -d -p 9876:9876 -v /Users/dongnan/namesrv/logs:/root/logs -v /Users/dongnan/namesrv/store:/root/store --name rmqnamesrv rocketmqinc/rocketmq sh mqnamesrv 启动brokerdocker run -d -p 10911:10911 -p 10909:10909 -v /Users/dongnan/broker/logs:/root/logs -v /Users/dongnan/broker/store:/root/store --name rmqbroker --link rmqnamesrv:namesrv -e &quot;NAMESRV_ADDR=127.0.0.1:9876&quot; rocketmqinc/rocketmq sh mqbroker -c /opt/rocketmq-4.4.0/conf/broker.conf 帮助启动类命令1、启动docker systemctl start docker2、停止docker systemctl stop docker3、查看docker状态 systemctl status docker4、开机启动 systemctl enable docker5、查看docker概要信息 docker info 容器命令--name=&quot;容器新名字&quot; 为容器指定一个名称-d:后台运行容器并返回容器id，即启动守护式容器(后台运行)-i:以交互模式运行容器，通常与-t同时使用-t:为容器重新分配一个伪输入终端，通常与-i同时使用；也即启动交互式容器（前台有伪终端，等待交互）-P:随机端口映射，大写P-p:指定端口映射，小写p交互式操作\tdocker run -it centos /bin/bash退出容器\texit: run进去容器，exit退出，容器停止\tctrl+p+q: 容器不停止查看容器日志\tdocker logs containerId查看容器内部细节\tdocker inspect containerId进入容器\tdocker exec -it containerId /bin/bash从容器内拷贝文件到主机\tdocker cp 容器id:容器内路径 目的主机路径导出容器\tdocker export containerId &gt; 名称.tar导入容器\tcat 名称.tar | docker import 镜像用户/镜像名:镜像版本号 network查看所有网络docker network ls创建网络(默认网桥模式)docker network create aa_network 网络模式 简介 bridge 为每个容器分配、设置IP等，并将容器连接到一个docker0虚拟网桥，默认为该模式 host 容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机等IP和端口 none 容器有独立的Network namespace，但并没有对其进行任何网络设置，如分配veth pair 和网桥连接，IP等 container 新创建的容器不会创建自己的网卡和配置自己的IP，而是和一个指定的容器共享IP、端口范围等 mogu 1、启动mysql容器\tdocker run -d -i -p 3306:3306 --name=mysql --restart=always -e MYSQL_ROOT_PASSWORD=root -d mysql:5.72、启动redis容器\tdocker run -d --name redis -p 6379:6379 redis --requirepass “redis”3、启动rabbitmq\tdocker run -d --name rabbitmq -p 15672:15672 -p 5673:5672 rabbitmq4、启动nacos\tdocker run -d \\-e MODE=standalone \\-e SPRING_DATASOURCE_PLATFORM=mysql \\-e MYSQL_SERVICE_HOST=172.17.0.2 \\-e MYSQL_SERVICE_PORT=3306 \\-e MYSQL_SERVICE_USER=root \\-e MYSQL_SERVICE_PASSWORD=root \\-e MYSQL_SERVICE_DB_NAME=nacos_config \\-p 8848:8848 \\--restart=always \\--name nacos acos/nacos-server:1.4.15、docker查看容器ip\tdocker inspect --format &#x27;&#123;&#123; .NetworkSettings.IPAddress &#125;&#125;&#x27; &lt;container id&gt;6、docker启动nginx\tdocker run --name nginx -p 80:80 -d nginx\t挂载nginx 创建文件路径 mkdir /usr/local/nginx mkdir /usr/local/nginx/www mkdir /usr/local/nginx/conf mkdir /usr/local/nginx/logs 复制nginx文件到本地 docker cp 5da957695512:/etc/nginx/nginx.conf /usr/local/nginx/nginx.conf docker cp 5da957695512:/etc/nginx/conf.d /usr/local/nginx/conf/ docker cp 5da957695512:/usr/share/nginx/html/ /usr/local/nginx/www/html/ docker cp 5da957695512:/var/log/nginx/access.log /usr/local/nginx/logs/access.log docker cp 5da957695512:/var/log/nginx/error.log /usr/local/nginx/logs/error.log 删除之前容器 启动nginx并挂载路径 docker run -d -p 80:80 --name nginx -v /etc/nginx:/etc/nginx nginx 7、查看容器ip mysql:172.17.0.2 redis:172.17.0.3 nacos:172.17.0.5 8、启动容器 docker run -d --name admin -p 8601:8601 gateway加一个 --platform linux/amd64 就可以使用苹果m1自带的模拟器运行了","categories":["Docker"]},{"title":"2、JVM类加载过程","path":"/2023/11/27/JVM/2、JVM类加载过程/","content":"一、 类的加载过程过程一、Loading(装载)阶段所谓装载，就是把Java类的字节码文件加载到机器内存中，并在内存中构建出Java类的原型————类模版对象。 ​\t装载完成的操作 ​\t装载阶段，简言之，查找并加载类的二进制数据，生成class的实例。 ​\t在加载类时，Java虚拟机必须完成以下3件事情： 通过类的全名，获取类的二进制数据流 解析类的二进制数据流为方法区内的数据结构（Java类模型） 创建java.long.Class类的实例，表示该模型。作为方法区这个类的各种数据的访问入口。 ​\t类模版对象 ​\t所谓类模版对象，其实就是Java类在JVM内存中的一个快照，JVM将从字节码文件中解析出来的常量池、类字段、类方法等信息存储到类模版中，这样JVM在运行期便能通过类模版而获取Java类中的基本信息，能够对Java类的成员变量进行遍历，也能进行Java方法的调用。 过程二、Linking（链接）阶段环节1：verification(验证)​ 当类加载到系统后，就开始链接操作，第一步就是验证 ​ 目的是保证加载到字节码是合法、合理并符合规范的 ​ 主要包括四种验证，文件格式验证，元数据验证，字节码验证，符号引用验证。 环节2：Preparation（准备）阶段 为类的静态变量分配内存，并将其初始化为默认值 这里不包含用final static修饰的，因为final在编译的时候就会分配了，准备阶段会显式初始化； 这里不会为实例变量分配初始化 ，类变量会分配在方法区中，而实例变量是会随着对象一起分配到Java堆中。 环节3:Resolution（解析）阶段 &#x3D;&#x3D;将常量池内的符号引用转换为直接引用的过程。&#x3D;&#x3D; &#x3D;&#x3D;事实上，解析操作往往会伴随着JVM在执行完初始化之后再执行。&#x3D;&#x3D; 符号引用就是一组符号来描述所引用的目标。符号引用的字面量形式明确定义在《java虚拟机规范》的Class文件格式中。 在解析阶段，jvm根据字符串的内容找到内存区域中相应的地址，然后把符号引用替换成直接指向目标的指针、句柄、偏移量等，这些直接指向目标的指针、句柄、偏移量就被成为 直接引用 。 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型等。对应常量池中的CONSTANT_Class_info、CONSTANT_Fieldref_info、CONSTANT_Methodref_info等。 过程三、Initialization（初始化）阶段​ 为类的静态变量赋予正确的初始值。到了初始化阶段，才真正开始执行类中定义的程序代码。 ​ 初始化阶段重要工作是执行类的初始化方法：()方法 ​ 过程四、类的Using（使用）过程五、类的unloading（卸载）二、JVM的类加载器 3.1、Bootstrap 引导类加载器引导类加载器在有些地方也被称为启动类加载器或根类加载器，但其实都是一个意思，都是在指BootstrapClassLoader。引导类加载器是使用C++语言实现的，是JVM自身的一部分，主要负责将&lt;JAVA_HOME&gt;\\lib路径下的核心类库或-Xbootclasspath参数指定的路径下的jar包加载到内存中。 注意：因为JVM是通过全限定名加载类库的，所以，如果你的文件名不被虚拟机识别，就算你把jar包丢入到lib目录下，引导类加载器也并不会加载它。出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类文件。 引导类加载器只为JVM提供加载服务，开发者不能直接使用它来加载自己的类。 3.2、Extension 拓展类加载器这个类加载器是由sun公司实现的，位于HotSpot源码目录中的sun.misc.Launcher$ExtClassLoader位置。它主要负责加载&lt;JAVA_HOME&gt;\\lib\\ext目录下或者由系统变量-Djava.ext.dir指定位路径中的类库。它可以直接被开发者使用。 3.3、Application 系统类加载器也被称为应用程序类加载器，也是由sun公司实现的，位于HotSpot源码目录中的sun.misc.Launcher$AppClassLoader位置。它负责加载系统类路径java -classpath或-D java.class.path指定路径下的类库，也就是经常用到的classpath路径。应用程序类加载器也可以直接被开发者使用。 一般情况下，该类加载器是程序的默认类加载器，我们可以通过ClassLoader.getSystemClassLoader()方法可以直接获取到它。 3.4、User 自定义类加载器在Java程序中，运行时一般都是通过如上三种类加载器相互配合执行的，当然，如果有特殊的加载需求也可以自定义类加载器，通过继承ClassLoader类实现（稍后分析）。 双亲委派机制 ①自下向上检查类是否已经被加载 ②从上至下尝试加载类 3.1、双亲委派类加载过程 ①当App尝试加载一个类时，它不会直接尝试加载这个类，首先会在自己的命名空间中查询是否已经加载过这个类，如果没有会先将这个类加载请求委派给父类加载器Ext完成 ②当Ext尝试加载一个类时，它也不会直接尝试加载这个类，也会在自己的命名空间中查询是否已经加载过这个类，没有的话也会先将这个类加载请求委派给父类加载器Bootstrap完成 ③如果Bootstrap加载失败，也就是代表着：这个需要被加载的类不在Bootstrap的加载范围内，那么Bootstrap会重新将这个类加载请求交由子类加载器Ext完成 ④如果Ext加载失败，代表着这个类也不在Ext的加载范围内，最后会重新将这个类加载请求交给子类加载器App完成 ⑤如果App加载器也加载失败，就代表这个类根据全限定名无法查找到，则会抛出ClassNotFoundException异常 核心是 ClassLoader类的loadClass方法","categories":["JVM"]},{"title":"1、JVM字节码","path":"/2023/11/27/JVM/1、Java字节码/","content":"JVM一、字节码文件概述1.什么是字节码指令（byte code）Java虚拟机的指令是由一个字节长度的、代表着某种特定操作含义的操作码（opcode）以及跟随在其后的零至多个代表此操作所需参数的操作数（operand）所构成。虚拟机中许多指令并不包含操作数，只有一个操作码。 包装类对象缓存问题 包装类 缓存问题 Byte -128~127 Short -128~127 Integer -128~127 Long -128~127 Float 没有 Double 没有 Character 0-127 Boolean true和false class文件的总体结构： 魔数 Class文件版本 常量池 访问标识 类索引、父类索引、接口索引集合 字段表集合 方法表集合 属性表集合 符号引用和直接引用 ​ Java代码在进行Javac编译的时候，并不像C和C++那样有“连接”这一步骤，而是在虚拟机加载Class文件的时候进行动态链接。也就是说，&#x3D;&#x3D;在Class文件中不会保存各个方法、字段的最终内存布局信息，因此这些字段、方法的符号引用不经过运行期转换的话无法得到真正的内存入口地址，也就无法直接被虚拟机使用。&#x3D;&#x3D;当虚拟机运行时，需要从常量池获得对应的符号引用再在类创建时或运行时解析、翻译到具体的内存地址之中。​ 虚拟机在加载类文件时才会进行动态链接，也就是说，类文件中不会保存各个方法和代码字段的最终内存布局信息，因此，这些字段和方法的符号引用不经过转换是无法直接被虚拟机使用的。&#x3D;&#x3D;当虚拟机运行时，需要从常量池中获得对应的符号引用，再在类加载过程中的解析阶段将其替换为直接引用，并翻译到具体的内存地址中。&#x3D;&#x3D;​ 这里说明下符号引用和直接引用的区别与关联:​ 符号引用:符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。符号引用与虚拟机实现的内存布局无关，引用的目标并不一定已经加载到了内存中。​ 直接引用:直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用是与虚拟机实现的内存布局相关的，同一个符号引用在不同虚拟机实例上翻译出来的直接引用一般不会相同。如果有了直接引用，那说明引用的目标必定已经存在于内存之中了。","categories":["JVM"]},{"title":"MYSQL内存","path":"/2023/11/27/MySQL/MySQL内存/","content":"本地内存 thread_stack：线程堆栈，主要用于暂时存储运行的SQL语句及运算数据，和Java虚拟机栈类似。 sort_buffer：排序缓冲区，执行排序SQL时，用于存放排序后数据的临时缓冲区。 join_buffer：连接缓冲区，做连表查询时，存放符合连表查询条件的数据临时缓冲区。 read_buffer：顺序读缓冲区，MySQL磁盘IO一次读一页数据，这个是顺序IO的数据临时缓冲区。 read_rnd_buffer：随机读缓冲区，当基于无序字段查询数据时，这里存放随机读到的数据。 net_buffer：网络连接缓冲区，这里主要是存放当前线程对应的客户端连接信息。 tmp_table：内存临时表，当SQL中用到了临时表时，这里存放临时表的结构及数据。 bulk_insert_buffer：MyISAM批量插入缓冲区，批量insert时，存放临时数据的缓冲区。 bin_log_buffer：bin-log日志缓冲区，《日志篇》提到过的，bin-log的缓冲区被设计在工作线程的本地内存中。 共享内存 Key Buffer：MyISAM表的索引缓冲区，提升MyISAM表的索引读写速度。 Query Cache：查询缓存区，缓冲SQL的查询结果，提升热点SQL的数据检索效率。 Thread Cache：线程缓存区，存放工作线程运行期间，一些需要被共享的临时数据。 Table Cache：表数据文件的文件描述符缓存，提升数据表的打开效率。 Table Definition Cache：表结构文件的文件描述符缓存，提升结构表的打开效率。 存储引擎缓存区 Data Page：写入缓冲区，主要用来缓冲磁盘的表数据，将写操作转移到内存进行。 Index Page：索引缓冲页，对于所有已创建的索引根节点，都会放入到内存，提升索引效率。 Lock Space：锁空间，主要是存放所有创建出的锁对象，详情可参考《MySQL锁机制实现原理》。 Dict Info：数据字典，主要用来存储MySQL-InnoDB引擎自带的系统表。 redo_log_buffer：redo-log缓冲区，存放写SQL执行时写入的redo记录。 undo_log_buffer：undo-log缓冲区，存放写SQL执行时写入的undo记录。 Adaptivity Hash：自适应哈希索引，InnoDB会为热点索引页，创建相应的哈希索引。 Insert Buffer：写入缓冲区，对于insert的数据，会先放在这里，然后定期刷写磁盘。 Lru List：内存淘汰页列表，对于整个缓冲池的内存管理列表（后续细聊）。 Free List：空闲内存列表，这里面记录着目前未被使用的内存页。 Flush List：脏页内存列表，这里主要记录未落盘的数据。","categories":["MYSQL"]},{"title":"MYSQL命令","path":"/2023/11/27/MySQL/MySQL命令/","content":"MySQL命令一、基础操作与库命令1.MySQL基础操作命令net start mysql：Windows系统启动MySQL服务。 安装目录/mysql start：Linux系统启动MySQL服务。 shutdown：后面的start换成这个，表示关闭MySQL服务。 restart：换成restart表示重启MySQL服务。 ps -ef | grep mysql：Linux查看MySQL后台进程的命令。 kill -9 MySQL进程ID：强杀MySQL服务的命令。 mysql -h地址 -p端口 -u账号 -p：客户端连接MySQL服务（需要二次输入密码）。 show status;：查看MySQL运行状态。 SHOW VARIABLES like %xxx%;：查看指定的系统变量。 show processlist;：查看当前库中正在运行的所有客户端连接&#x2F;工作线程。 show status like &quot;Threads%&quot;;：查看当前数据库的工作线程系统。 help data types;：查看当前版本MySQL支持的所有数据类型。 help xxx：查看MySQL的帮助信息。 quit：退出当前数据库连接。 2.MySQL库命令show databases;：查看目前MySQL中拥有的所有库。 show engines;：查看当前数据库支持的所有存储引擎。 use 库名;：使用&#x2F;进入指定的某个数据库。 show status;：查看当前数据库的状态信息。 show grants;：查看当前连接的权限信息。 show errors;：查看当前库中记录的错误信息。 show warnings：查看当前库抛出的所有警告信息。 show create database 库名;：查看创建某个库的SQL详细信息。 show create table 表名;：查看创建某张表的SQL详细信息。 show tables;：查看一个库中的所有表。 desc 表名;：查看一张表的字段结构。除开这种方式还有几种方式： describe 表名;：查看一张表的字段结构。 show columns from 表名;：查看一张表的字段结构。 explain 表名;：查看一张表的字段结构。 create database 库名;：新建一个数据库，后面还可以指定编码格式和排序规则。 drop database 库名;：删除一个数据库。 ALTER DATABASE 库名 DEFAULT CHARACTER SET 编码格式 DEFAULT COLLATE 排序规则：修改数据库的编码格式、排序规则。 3.MySQL表命令3.1 字段选项（可以不写，不选使用默认值）： NULL：表示该字段可以为空。 NOT NULL：表示改字段不允许为空。 DEFAULT 默认值：插入数据时若未对该字段赋值，则使用这个默认值。 AUTO_INCREMENT：是否将该字段声明为一个自增列。 PRIMARY KEY：将当前字段声明为表的主键。 UNIQUE KEY：为当前字段设置唯一约束，表示不允许重复。 CHARACTER SET 编码格式：指定该字段的编码格式，如utf8。 COLLATE 排序规则：指定该字段的排序规则（非数值类型生效）。 COMMENT 字段描述：为当前字段添加备注信息，类似于代码中的注释。 表选项（可以不写，不选使用默认值）： ENGINE = 存储引擎名称：指定表的存储引擎，如InnoDB、MyISAM等。 CHARACTER SET = 编码格式：指定表的编码格式，未指定使用库的编码格式。 COLLATE = 排序规则：指定表的排序规则，未指定则使用库的排序规则。 ROW_FORMAT = 格式：指定存储行数据的格式，如Compact、Redundant、Dynamic....。 AUTO_INCREMENT = n：设置自增列的步长，默认为1。 DATA DIRECTORY = 目录：指定表文件的存储路径。 INDEX DIRECTORY = 目录：指定索引文件的存储路径。 PARTITION BY ...：表分区选项，后续讲《MySQL表分区》再细聊。 COMMENT 表描述：表的注释信息，可以在这里添加一张表的备注。 3.2 表操作命令show table status like &#39;zz_users&#39;\\G;：纵排输出一张表的状态信息。 alter table 表名 表选项;：修改一张表的结构，如alter table xxx engine=MyISAM。 rename table 表名 to 新表名;：修改一张表的表名。 alter table 表名 字段操作;：修改一张表的字段结构，操作如下： add column 字段名 数据类型：向已有的表结构添加一个字段。 add primary key(字段名)：将某个字段声明为主键。 add foreing key 外键字段 表名.字段名：将一个字段设置为另一张表的外键。 add unique 索引名(字段名)：为一个字段创建唯一索引。 add index 索引名(字段名)：为一个字段创建普通索引。 drop column 字段名：在已有的表结构中删除一个字段。 modify column 字段名 字段选项：修改一个字段的字段选项。 change column 字段名 新字段名：修改一个字段的字段名称。 drop primary key：移除表中的主键。 drop index 索引名：删除表中的一个索引。 drop foreing key 外键：删除表中的一个外键。 drop table if exists 表名：如果一张表存在，则删除对应的表。 truncate table 表名：清空一张表的所有数据。 create table 表名 like 要复制的表名：复制一张表的结构，然后创建一张新表。 create table 表名 as select * from 要复制的表名：同时复制表结构和数据创建新表。 1.4、表的分析、检查、修复与优化操作MySQL本身提供了一系列关于表的分析、检查与优化命令： ①分析表：分析表中键的分布，如主键、唯一键、外键等是否合理。 ②检查表：检查表以及表的数据文件是否存在错误。 ③修复表：当一个表的数据或结构文件损坏时，可以修复表结构（仅支持MyISAM表）。 ④优化表：消除delete、update语句执行时造成的空间浪费。 分析表语法如下： sql复制代码analyze [local | no_write_to_binlog] table 表名1; 其中的可选参数local、no_write_to_binlog代表是否将本条SQL记录进bin-log日志，默认情况下是记录的，加上这两个参数中的其中一个后则不会记录，执行效果如下： 如果Msg_text显示的是OK，则代表这张表的键不存在问题，存在问题的情况我这边就不模拟了，后面举例聊。 检查表语法如下： check table 表名1,表名2... [检查选项]; 分析、检查、优化、修复的命令都支持同时操作多张表，不同的表之间只需用,逗号隔开即可。检查命令有多个可选项，如下： quick：不扫描行数据，不检查链接错误，仅检查表结构是否有问题。 fast：只检查表使用完成后，是否正确关闭了表文件的FD文件描述符。 changed：从上述检查过的位置开始，只检查被更改的表数据。 medium：检查行数据，收集每一行数据的键值（主键、外键…），并计算校验和，验证数据是否正确。 extended：对每行数据的所有字段值进行检查，检查完成后可确保数据100%正确。 先来看看执行结果吧，如下： 这回的结果出现了些许不同，Msg_text中出现了一个Error信息，提示咱们检查的zz_u表不存在，而对于一张存在的zz_users表，则返回OK，表示没有任何问题。 当然，这里对于其他的检查选项就不做测试了，大家可以自行实验，比如把表的结构文件或数据文件，在本地打开手动删除前面的一点点数据，然后再执行检查命令，其实你也可以观察到，提示“数据不完整”的信息（但需要先停止运行MySQL，并且用本地表测试，不要用线上表瞎搞）。 修复表语法如下： sql复制代码repair [local | no_write_to_binlog] table 表名 [quick] [extended] [use_frm]; 值得一提的是，修复表的命令不支持InnoDB引擎，仅支持MyISAM、CSV、引擎，比如基于InnoDB引擎的表执行修复命令时，提示如下： 上述Msg_text信息翻译过来的意思是：选择的表其引擎并不支持修复命令。 InnoDB引擎其实也有修复机制，可以在my.ini/my.conf文件中加一行配置：[mysqld]innodb_force_recovery = 1，这样在启动时会强制恢复InnoDB的数据。 上述这个修复机制默认是不开启的，因为InnoDB不需要这个恢复机制，毕竟之前在《引擎篇》中聊过：InnoDB有完善的事务和持久化机制，客户端提交的事务都会持久化到磁盘，除非你人为损坏InnoDB的数据文件，否则基本上不会出现InnoDB数据损坏的情况。 优化表语法如下： sql复制代码optimize [local | no_write_to_binlog] table 表名; 这里值得一提的是：此优化非彼优化，并不意味着你的表存在性能问题，执行后它会自动调优，而是指清除老数据，执行效果如下： 二、数据的导出、导入与备份、还原-- --------使用 mysqldump 工具做数据的逻辑备份（导出的是sql语句）------------- 导出MySQL中全部的库数据（使用--all-databases 或者 -A 参数）mysqldump -uroot -p密码 --all-databases &gt; 备份文件名.sql-- 导出MySQL中一部分的库数据（使用--databases 或者 -B 参数）mysqldump -uroot -p密码 --databases &gt; 备份文件名.sql-- 导出MySQL单库中的一部分表数据mysqldump –u 用户名 –h主机名 –p密码 库名[表名1,表名2...]&gt; 备份文件名.sql-- 导出MySQL单表的部分数据（使用 --where 参数）mysqldump -u用户名 -p 库名 表名 --where=&quot;条件&quot; &gt; 备份文件名.sql-- 排除某些表，导出库中其他的所有数据（使用 --ignore-table 参数）mysqldump -u用户名 -p 库名 --ignore-table=表名1,表名2... &gt; 备份文件名.sql-- 只导出表的结构（使用 --no-data 或者 -d 选项）mysqldump -u用户名 -p 库名 --no-data &gt; 备份文件名.sql-- 只导出表的数据（使用 --no-create-info 或者 -t 选项）mysqldump -u用户名 -p 库名 --no-create-info &gt; 备份文件名.sql-- 导出包含存储过程、函数的库数据（使用--routines 或者 -R选项）mysqldump -u用户名 -p -R --databases 库名 &gt; 备份文件名.sql-- 导出包含事件（触发器）的库数据（使用 --events 或者 -E选项）mysqldump -u用户名 -p -E --databases 库名 &gt; 备份文件名.sql-- --------使用 mysql 工具来恢复备份的数据（导入xx.sql文件执行）------------- 恢复库级别的数据（包含了建库语句的情况下使用）mysql -u用户名 -p &lt; xxx.sql-- 恢复库中表级别的数据mysql -u用户名 -p 库名 &lt; xxx.sql-- ----------以物理形式备份数据（导出的是表数据） -------------- 查看数据库导出数据的路径（如果没有则需在`my.ini/my.conf`中配置）show variables like &#x27;%secure_file_priv%&#x27;;-- 导出一张表的数据为txt文件（使用 select ... into outfile 语句）select * from 表名 into outfile &quot;备份文件名.txt&quot;;-- 导出一张表的数据为txt文件（使用 mysql 工具）mysql -u用户名 -p --execute=&quot;select ...;&quot; 库名 &gt; &quot;数据存放目录/xxx.txt&quot;-- 导出一张表的结构和数据为sql、txt文件（使用 mysqldump -T 的方式）mysqldump -u用户名 -p -T &quot;数据存放目录&quot; 库名 文件名-- 导出一张表的数据为txt文件，以竖排形式存储（使用 mysql –veritcal 的方式）mysql -u用户名 -p -veritcal --execute=&quot;select ...;&quot; 库名 &gt; &quot;数据存放目录/xxx.txt&quot;-- 导出一张表的数据为xml文件（使用 mysql -xml 的方式）mysql -u用户名 -p -xml --execute=&quot;select ...;&quot; 库名 &gt; &quot;数据存放目录/xxx.xml&quot;-- -----------通过物理数据文件恢复数据------------------ 使用load data infile 的方式导入.txt 物理数据load data infile &quot;数据目录/xxx.txt&quot; into table 库名.表名;-- 使用 mysqlimport 工具导入xxx.txt物理数据mysqlimport -u用户名 -p 库名 &#x27;数据存放目录/xxx.txt&#x27; --fields-terminatedby=&#x27;,&#x27; --fields-optionally-enclosed-by=&#x27;\\&quot;&#x27;-- 使用 mysqldump 工具迁移数据mysqldump –h 地址1 –u用户名 –p密码 –-all-databases | mysql –h地址2 –u用户名 –p密码 三、表分区相关命令-- 创建范围分区create table `表名`(\t`xxx` xxx not null, ....) partition by range(xxx)(\tpartition 分区名1 values less than (范围) data directory = &quot;/xxx/xxx/xxx&quot;,\tpartition 分区名2 values less than (范围) data directory = &quot;/xxx/xxx/xxx&quot;,\t......);-- 创建枚举分区create table `表名`(\t`xxx` xxx not null, ....)partition by list(xxx)(\tpartition 分区名1 values in (枚举值1,枚举值2...),\tpartition 分区名2 values in (枚举值),\t......);-- 创建常规哈希分区create table `表名`(\t`xxx` xxx not null, ....)partition by hash(xxx)partitions 分区数量;-- 创建线性哈希分区create table `表名`(\t`xxx` xxx not null, ....)partition by linear hash(xxx)partitions 分区数量;-- 创建Key键分区create table `表名`(\t`xxx` xxx not null, ....)partition by key(xxx)partitions 分区数量;-- 创建Sub子分区create table `表名`(\t`xxx` xxx not null, ....) partition by range(父分区键)subpartition by hash(子分区键)( partition 分区名1 values less than (范围1)( subpartition 子分区名1, subpartition 子分区名2, ...... ), partition 分区名2 values less than (范围2)( subpartition 子分区名1, subpartition 子分区名2, ...... ), ......);-- 查询一张表各个分区的数据量select partition_name as &quot;分区名称&quot;,table_rows as &quot;数据行数&quot;from information_schema.partitions where table_name = &#x27;表名&#x27;;-- 查询一张表父子分区的数据量select partition_name as &quot;父分区名称&quot;, subpartition_name as &quot;子分区名称&quot;, table_rows as &quot;子分区行数&quot;from information_schema.partitions where table_name = &#x27;表名&#x27;;-- 查询MySQL中所有表分区的信息select * from information_schema.partitions;-- 查询一张表某个分区中的所有数据select * from 表名 partition (分区名);-- 对于一张已存在的表添加分区alter table 表名 reorganize partition 分区名 into (\tpartition 分区名1 values less than (范围) data directory = &quot;/xxx/xxx/xxx&quot;,\tpartition 分区名2 values less than (范围) data directory = &quot;/xxx/xxx/xxx&quot;,\t......);-- 将多个分区合并成一个分区alter table 表明 reorganize partition 分区名1,分区名2... into ( partition 新分区名 values less than (范围));-- 清空一个分区中的所有数据alter table 表名 truncate partition 分区名;-- 删除一个表的指定分区alter table 表名 drop partition 分区名;-- 重建一张表的分区alter table 表名 rebuild partition 分区名;-- 分析一个表分区alter table 表名 analyze partition 分区名;-- 优化一个表分区alter table 表名 optimize partition 分区名;-- 检查一个表分区alter table 表名 check partition 分区名;-- 修复一个表分区alter table 表名 repair partition 分区名;-- 减少hash、key分区方式的 n 个分区alter table 表名 coalesce partition n;-- 将一张表的分区切换到另一张表alter table 表名1 exchange partition 分区名 with table 表名2;-- 移除一张表的所有分区alter table 表名 remove partitioning;","categories":["MYSQL"]},{"title":"Saas多租户数据库隔离","path":"/2023/11/27/Saas租户/SaaS多租户数据库隔离/","content":"SaaS 多租户【数据库隔离】本章节，讲解 SaaS 租户的 DATASOURCE 模式，实现数据库级别的隔离。 注意，需要前置阅读 《SaaS 多租户【字段隔离】》 文档。 #0. 极速体验① 克隆 https://gitee.com/zhijiantianya/ruoyi-vue-pro (opens new window)仓库，并切换到 feature/dev-yunai 分支。 ② 创建 ruoyi-vue-pro-master、ruoyi-vue-pro-tenant-a、ruoyi-vue-pro-tenant-b 三个数据库。 ③ 下载 多租户多db.zip 并解压，将 SQL 导入到对应的数据库中。 友情提示： 随着版本的迭代，SQL 脚本可能过期。如果碰到问题，可以在星球给我反馈下。 ④ 启动前端和后端项目，即可愉快的体验了。 #1. 实现原理DATASOURCE 模式，基于 dynamic-datasource (opens new window)进行拓展实现。 核心：每次对数据库操作时，动态切换到该租户所在的数据源，然后执行 SQL 语句。 #2. 功能演示我们来新增一个租户，使用 DATASOURCE 模式。 ① 点击 [基础设施 -&gt; 数据源配置] 菜单，点击 [新增] 按钮，新增一个名字为 tenant-a 数据源。 然后，手动将如下表拷贝到 ruoyi-vue-pro 主库中的如下表，拷贝到 ruoyi-vue-pro-tenant-a 库中。如下图所示： system_deptsystem_login_logsystem_noticesystem_notify_messagesystem_operate_logsystem_postsystem_rolesystem_role_menusystem_social_usersystem_social_user_bindsystem_user_postsystem_user_rolesystem_users 友情提示： 随着版本的迭代，可能需要拷贝更多的表。如果碰到问题，可以在星球给我反馈下。 ② 点击 [基础设施 -&gt; 租户管理] 菜单，点击 [新增] 按钮，新增一个名字为 土豆租户 的租户，并使用 tenant-a 数据源。如下图所示： 此时，在 ruoyi-vue-pro-tenant-a 库中，可以查询到对应的租户管理员、角色等信息。如下图所示： ③ 退出系统，登录刚创建的租户。 至此，我们已经完成了租户的创建。 补充说明： 后续在使用时，建议把拷贝到其它租户数据库的表，从 ruoyi-vue-pro 主库中进行删除。 目的是，主库只保留所有租户共享的全局表。例如说，菜单表、定时任表等等。 #3. 创建表在使用 DATASOURCE 模式时，数据库可以分为两种：主库、租户库。 #3.1 主库① 存放所有租户共享的表。例如说：菜单表、定时任务表等等。如下图所示： ② 对应 master 数据源，配置在 application-&#123;env&#125;.yaml 配置文件。如下图所示： ③ 每个主库对应的 Mapper，必须添加 @Master (opens new window)注解。例如说： #3.2 租户库① 存放每个租户的表。例如说：用户表、角色表等等。 ② 在 [基础设施 -&gt; 数据源配置] 菜单中，配置数据源。 ③ 每个主库对应的 Mapper，必须添加 @TenantDS 注解。例如说： #3.3 租户字段① 考虑到拓展性，在使用 DATASOURCE 模式时，默认会叠加 COLUMN 模式，即还有 tenant_id 租户字段： 在 INSERT 操作时，会自动记录租户编号到 tenant_id 字段。 在 SELECT 操作时，会自动添加 WHERE tenant_id = ? 查询条件。 如果你不需要，可以直接删除 TenantDatabaseInterceptor (opens new window)类，以及它的 Bean 自动配置。 拓展性，指的是部分【大】租户独立数据库，部分【小】租户共享数据。 ② 也因为叠加了 COLUMN 模式，主库的表需要根据情况添加 tenant_id 字段。 情况一：不需要添加 tenant_id 字段。例如说：菜单表、定时任务表等等。注意，需要把表名添加到 yudao.tenant.ignore-tables 配置项中。 情况二：需要 tenant_id 字段。例如说：访问日志表、异常日志表等等。目的，排查是哪个租户的系统级别的日志。 #4. 多数据源事务使用 DATASOURCE 模式后，可能一个操作涉及到多个数据源。例如说：创建租户时，即需要操作主库，也需要操作租户库。 考虑到多数据的数据一致性，我们会采用事务的方式，而使用 Spring 事务时，会存在多数据库无法切换的问题。不了解的胖友，可以阅读 《MyBatis Plus 的多数据源 @DS 切换不起作用了，谁的锅 》 (opens new window)文章。 多数据源的事务方案，是一个老生常谈的问题。比较主流的，有如下两种，都是相对重量级的方案： 使用 Atomikos (opens new window)实现 JTA 分布式事务，配置复杂，性能较差。 使用 Seata (opens new window)实现分布式事务，使用简单，性能不错，但是需要额外引入 Seata Server 服务。 #4.1 本地事务考虑到项目是单体架构，不适合采用重量级的事务，因此采用 dynamic-datasource (opens new window)提供的 “本地事务” 轻量级方案。 它的实现原理是：自定义 @DSTransactional (opens new window)事务注解，替代 Spring @Transactional 事务注解。 在逻辑执行成功时，循环提交每个数据源的事务。 在逻辑执行失败时，循环回滚每个数据源的事务。 但是它存在一个风险点，如果数据库发生异常（例如说宕机），那么本地事务就可能会存在数据不一致的问题。例如说： ① 主库的事务提交 ② 租户库发生异常，租户的事务提交失败 结果：主库的数据已经提交，而租户库的数据没有提交，就会导致数据不一致。 因此，如果你的系统对数据一致性要求很高，那么请使用 Seata 方案。 #4.2 使用示例在最外层的 Service 方法上，添加 @DSTransactional 注解。例如说，创建租户的 Service 方法： 注意，里面不能嵌套有 Spring 自带的事务，就是上图中【黄圈】的 Service 方法不能使用 Spring @Transactional 注解，否则会导致数据源无法切换。 如果【黄圈】的 Service 自身还需要事务，那么可以使用 @DSTransactional 注解。","categories":["Saas多租户"]},{"title":"匿名内部类","path":"/2023/11/27/Java/匿名内部类/","content":"匿名内部类一、定义1.本质还是一个类 2.是一个内部类 3.该类没有名字（但是系统会分配一个代号在内存中） 二、匿名内部类的使用场景假设我们有一个接口A内部有一个未被实现的方法eat如果我们想在main中直接调用eat方法则按照传统思路需要一个类B来实现接口A，同时再创建类B的对象来调用A代码如下： public class Interface01 &#123; public static void main(String[] args) &#123; B b = new B(); b.eat(); &#125;&#125;interface A&#123; public void eat();&#125;class B implements A&#123; @Override public void eat() &#123; System.out.println(&quot;正在调用eat方法&quot;); &#125;&#125;----------------------正在调用eat方法 如果我们只是想单纯的使用一次eat方法，不需要创建对象的话，则上面方法略显古板。此时，便用到了匿名内部类。 public class Interface01 &#123; public static void main(String[] args) &#123; new A()&#123; @Override public void eat() &#123; System.out.println(&quot;正在调用eat方法&quot;); &#125; &#125;.eat(); &#125;&#125;interface A&#123; public void eat();&#125;----------------------正在调用eat方法 上面这种写法就可以很好的简化代码的书写此时的匿名内部类相当于一个对象，所以它的后面可以直接调用eat方法，非常的简便快捷当A里面有多个方法时，如果想要同时调用，可以采用下面的写法： public class Interface01 &#123; public static void main(String[] args) &#123; A a = new A()&#123; @Override public void eat() &#123; System.out.println(&quot;正在调用eat方法&quot;); &#125; public void drink()&#123; System.out.println(&quot;正在调用drink方法&quot;); &#125; &#125;; a.eat(); a.drink(); &#125;&#125;interface A&#123; public void eat(); public void drink();&#125; 而此时我们并没有实例化这个接口的类，便实现了里面的方法，所以称之为匿名内部类 其实，这个类是被临时创建了，在内存中存在系统设定的名字， 我们可以使用下面方法来得到匿名内部类的（名字） public class Interface01 &#123; public static void main(String[] args) &#123; A a = new A()&#123; @Override public void eat() &#123; System.out.println(&quot;正在调用eat方法&quot;); &#125; public void drink()&#123; System.out.println(&quot;正在调用drink方法&quot;); &#125; &#125;; a.eat(); a.drink(); System.out.println(a.getClass()); //获取类名 &#125;&#125;interface A&#123; public void eat(); public void drink();&#125;-------------------正在调用eat方法正在调用drink方法class com.interface_.Interface01$1 由此可见，这个匿名内部类的名字为Interface01$1，当下一个匿名内部类时，就会变成$2，以此类推 三、匿名内部类的最常使用场景通过实参的形式来使用，大大简化了代码的书写 public class Interface01 &#123; public static void main(String[] args) &#123; f(new A() &#123; @Override public void eat() &#123; System.out.println(&quot;没有创建对象便成功的调用了f方法，不需要实现接口&quot;); &#125; &#125;); &#125; public static void f(A a)&#123; a.eat(); &#125;&#125;interface A&#123; public void eat();&#125;---------------------------没有创建对象便成功的调用了f方法，不需要实现接口 而按照传统方法则需要： public class Interface01 &#123; public static void main(String[] args) &#123; B b = new B(); f(b); &#125; public static void f(A a)&#123; a.eat(); &#125;&#125;interface A&#123; public void eat();&#125;class B implements A&#123; @Override public void eat() &#123; System.out.println(&quot;正在调用eat方法&quot;); &#125;&#125;--------------正在调用eat方法 四、Java匿名内部类的注意事项在使用匿名内部类的过程中，我们需要注意如下几点： 1、使用匿名内部类时，我们必须是继承一个类或者实现一个接口，但是两者不可兼得，同时也只能继承一个类或者实现一个接口。 2、匿名内部类中是不能定义构造函数的。 3、匿名内部类中不能存在任何的静态成员变量和静态方法。 4、匿名内部类为局部内部类，所以局部内部类的所有限制同样对匿名内部类生效。 5、匿名内部类不能是抽象的，它必须要实现继承的类或者实现的接口的所有抽象方法。","categories":["JAVA基础"]},{"title":"OAuth2.0","path":"/2023/11/27/Spring Security/OAuth2.0/","content":"OAuth2.0 （1）Third-party application：第三方应用程序，客户端 （2）HTTP Service：HTTP服务提供商，本文中简称”服务提供商” （3） Resource Owner：资源所有者，用户 （4）User Agent：用户代理，本文指浏览器 （5）Authorization Server：认证服务器，专门用来处理认证的服务器 （6）Resource Server：资源服务器，存放用户生成的资源的服务器 运行流程 （A）用户打开客户端以后，客户端要求用户给予授权。 （B）用户同意给予客户端授权。 （C）客户端使用上一步获得的授权，向认证服务器申请令牌。 （D）认证服务器对客户端进行认证以后，确认无误，同意发放令牌。 （E）客户端使用令牌，向资源服务器申请获取资源。 （F）资源服务器确认令牌无误，同意向客户端开放资源。 OAuth2.0四种授权方式 授权码模式（authorization code） 简化模式（implicit） 密码模式（resource owner password credentials） 客户端模式（client credentials） 授权码模式 （A）用户访问客户端，后者将前者导向认证服务器。 （B）用户选择是否给予客户端授权。 （C）假设用户给予授权，认证服务器将用户导向客户端事先指定的”重定向URI”（redirection URI），同时附上一个授权码。 （D）客户端收到授权码，附上早先的”重定向URI”，向认证服务器申请令牌。这一步是在客户端的后台的服务器上完成的，对用户不可见。 （E）认证服务器核对了授权码和重定向URI，确认无误后，向客户端发送访问令牌（access token）和更新令牌（refresh token）。 下面是上面这些步骤所需要的参数。 A步骤中，客户端申请认证的URI，包含以下参数： response_type：表示授权类型，必选项，此处的值固定为”code” client_id：表示客户端的ID，必选项 redirect_uri：表示重定向URI，可选项 scope：表示申请的权限范围，可选项 state：表示客户端的当前状态，可以指定任意值，认证服务器会原封不动地返回这个值。 例子： &gt; GET /authorize?response_type=code&amp;client_id=s6BhdRkqt3&amp;state=xyz&gt; &amp;redirect_uri=https%3A%2F%2Fclient%2Eexample%2Ecom%2Fcb HTTP/1.1&gt; Host: server.example.com&gt; C步骤中，服务器回应客户端的URI，包含以下参数： code：表示授权码，必选项。该码的有效期应该很短，通常设为10分钟，客户端只能使用该码一次，否则会被授权服务器拒绝。该码与客户端ID和重定向URI，是一一对应关系。 state：如果客户端的请求中包含这个参数，认证服务器的回应也必须一模一样包含这个参数。 下面是一个例子。 &gt; HTTP/1.1 302 Found&gt; Location: https://client.example.com/cb?code=SplxlOBeZQQYbYS6WxSbIA&gt; &amp;state=xyz&gt; D步骤中，客户端向认证服务器申请令牌的HTTP请求，包含以下参数： grant_type：表示使用的授权模式，必选项，此处的值固定为”authorization_code”。 code：表示上一步获得的授权码，必选项。 redirect_uri：表示重定向URI，必选项，且必须与A步骤中的该参数值保持一致。 client_id：表示客户端ID，必选项。 下面是一个例子。 &gt; POST /token HTTP/1.1&gt; Host: server.example.com&gt; Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW&gt; Content-Type: application/x-www-form-urlencoded&gt;&gt; grant_type=authorization_code&amp;code=SplxlOBeZQQYbYS6WxSbIA&gt; &amp;redirect_uri=https%3A%2F%2Fclient%2Eexample%2Ecom%2Fcb&gt; E步骤中，认证服务器发送的HTTP回复，包含以下参数： access_token：表示访问令牌，必选项。 token_type：表示令牌类型，该值大小写不敏感，必选项，可以是bearer类型或mac类型。 expires_in：表示过期时间，单位为秒。如果省略该参数，必须其他方式设置过期时间。 refresh_token：表示更新令牌，用来获取下一次的访问令牌，可选项。 scope：表示权限范围，如果与客户端申请的范围一致，此项可省略。 下面是一个例子。 &gt; HTTP/1.1 200 OK&gt; Content-Type: application/json;charset=UTF-8&gt; Cache-Control: no-store&gt; Pragma: no-cache&gt;&gt; &#123;&gt; &quot;access_token&quot;:&quot;2YotnFZFEjr1zCsicMWpAA&quot;,&gt; &quot;token_type&quot;:&quot;example&quot;,&gt; &quot;expires_in&quot;:3600,&gt; &quot;refresh_token&quot;:&quot;tGzv3JOkF0XG5Qx2TlKWIA&quot;,&gt; &quot;example_parameter&quot;:&quot;example_value&quot;&gt; &#125;&gt; 密码模式密码模式（Resource Owner Password Credentials Grant）中，用户向客户端提供自己的用户名和密码。客户端使用这些信息，向”服务商提供商”索要授权。 在这种模式中，用户必须把自己的密码给客户端，但是客户端不得储存密码。这通常用在用户对客户端高度信任的情况下，比如客户端是操作系统的一部分，或者由一个著名公司出品。而认证服务器只有在其他授权模式无法执行的情况下，才能考虑使用这种模式。 它的步骤如下： （A）用户向客户端提供用户名和密码。 （B）客户端将用户名和密码发给认证服务器，向后者请求令牌。 （C）认证服务器确认无误后，向客户端提供访问令牌。 B步骤中，客户端发出的HTTP请求，包含以下参数： grant_type：表示授权类型，此处的值固定为”password”，必选项。 username：表示用户名，必选项。 password：表示用户的密码，必选项。 scope：表示权限范围，可选项。 例子 &gt; POST /token HTTP/1.1&gt; Host: server.example.com&gt; Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW&gt; Content-Type: application/x-www-form-urlencoded&gt;&gt; grant_type=password&amp;username=johndoe&amp;password=A3ddj3w&gt; C步骤中，认证服务器向客户端发送访问令牌，下面是一个例子。 &gt; HTTP/1.1 200 OK&gt; Content-Type: application/json;charset=UTF-8&gt; Cache-Control: no-store&gt; Pragma: no-cache&gt;&gt; &#123;&gt; &quot;access_token&quot;:&quot;2YotnFZFEjr1zCsicMWpAA&quot;,&gt; &quot;token_type&quot;:&quot;example&quot;,&gt; &quot;expires_in&quot;:3600,&gt; &quot;refresh_token&quot;:&quot;tGzv3JOkF0XG5Qx2TlKWIA&quot;,&gt; &quot;example_parameter&quot;:&quot;example_value&quot;&gt; &#125;&gt; 授权方式的选择 问题一：什么场景下，使用客户端模式（Client Credentials）？ 如果令牌拥有者是机器的情况下，那就使用客户端模式。 例如说： 开发了一个开放平台，提供给其它外部服务调用 开发了一个 RPC 服务，提供给其它内部服务调用 实际的案例，我们接入微信公众号时，会使用 appid 和 secret 参数，获取 Access token (opens new window)访问令牌。 问题二：什么场景下，使用密码模式（Resource Owner Password Credentials）？ 接入的 Client 客户端，是属于自己的情况下，可以使用密码模式。 例如说： 客户端是你自己公司的 App 或网页，然后授权服务也是你公司的 不过，如果客户端是第三方的情况下，使用密码模式的话，该客户端是可以拿到用户的账号、密码，存在安全的风险，此时可以考虑使用授权码或简化模式。 问题三：什么场景下，使用授权码模式（Authorization Code）？ 接入的 Client 客户端，是属于第三方的情况下，可以使用授权码模式。例如说： 客户端是你自己公司的 App 或网页，作为第三方，接入 微信 (opens new window)、QQ (opens new window)、钉钉 (opens new window)等等进行 OAuth 2.0 登录 当然，如果客户端是自己的情况下，也可以采用授权码模式。例如说： 客户端是腾讯旗下的各种游戏，可使用微信、QQ，接入 微信 (opens new window)、QQ (opens new window)等等进行 OAuth 2.0 登录 客户端是公司内的各种管理后台（ERP、OA、CRM 等），跳转到统一的 SSO 单点登录，使用授权码模式进行授权 问题四：什么场景下，使用简化模式（Implicit）？ 简化模式，简化 的是授权码模式的流程的 第二步，差异在于： 授权码模式：授权完成后，获得的是 code 授权码，需要 Server Side 服务端使用该授权码，再向授权服务器获取 Access Token 访问令牌 简化模式：授权完成后，Client Side 客户端直接获得 Access Token 访问令牌 暂时没有特别好的案例，感兴趣可以看看如下文档，也可以不看： 《QQ OAuth 2.0 开发指定 —— 开发攻略_Client-side》(opens new window) 《百度 OAuth —— Implicit Grant 授权》","categories":["OAuth2.0"]},{"title":"队列","path":"/2023/11/27/数据结构/队列/","content":"队列1）队列是一个有序列表，可以用数组或链表实现 2）遵循先入先出的原则。即先存入队列的数据，要先取出。后存入的要后取出 3）示意图，使用数组模拟 数组模拟队列的思路1、队列本身是有序列表，若使用数组的结构来存储队列中的数据，其中maxSize是该队列的最大容量 2、因为队列的输入、输出是分别从前后端来处理，因此需要两个变量front和rear来记录队列前后端的下标，front会随着数据的输出而改变，rear会随着数据的输入而改变 当我们将数据存入队列时称为“addQueue”,addQueue的处理需要有两个步骤：思路分析 ​\t1）将尾指针往后移：rear+1，当front&#x3D;&#x3D;rear时，为空 ​\t2）若尾指针rear小于队列的最大下表MaxSize-1，则将数据存入rear所指的数组元素中，否则无法存入数据。rear&#x3D;&#x3D;MaxSize-1，队列满 /** * 使用数组模拟队列 * @author: dongnan * @date: 2022/11/7 16:55 * @param: **/class ArrayQueue&#123; private int maxSize;//表示数组的最大容量 private int front;//队列头 private int rear;//队列尾 private int[] arr;//该数组用于存放数据，模拟队列 //创建队列的构造器 public ArrayQueue(int arrMaxSize)&#123; maxSize = arrMaxSize; arr = new int[maxSize]; front = -1; rear = -1; &#125; //判断队列是否满 public boolean isFull()&#123; return rear == maxSize -1; &#125; //判断队列是否为空 public boolean isEmpty()&#123; return rear == front; &#125; //添加数据到队列 public void addQueue(int n)&#123; //判断队列是否满 if (isFull())&#123; System.out.println(&quot;队列满&quot;); return; &#125; rear++; arr[rear] = n; &#125; //获取队列的数据，出队列 public int getQueue()&#123; //判断队列是否为空 if (isEmpty())&#123; throw new RuntimeException(&quot;队列空，不能取数据&quot;); &#125; front++; return arr[front]; &#125; //显示队列的所有数据 public void showQueue()&#123; //遍历 if (isEmpty())&#123; throw new RuntimeException(&quot;队列空&quot;); &#125; for (int i = 0; i &lt; arr.length; i++) &#123; System.out.printf(&quot;arr[%d]=%d &quot;,i,arr[i]); &#125; &#125; //显示队列的头数据 public int headQueue()&#123; if (isEmpty())&#123; throw new RuntimeException(&quot;队列空&quot;); &#125; return arr[front+1]; &#125;&#125;","categories":["数据结构"]},{"title":"二分查找","path":"/2023/11/27/数据结构/查找/二分查找/","content":"二分查找二分查找又叫折半查找，顾名思义，折半查找的意思就是每次查询的时候对数组折半，原理如下所示： 我们此时进行比较，target &#x3D; 4与arr[mid]的值，如果比mid位置的数大，我们只需在mid的右边进行查找，因为当前数组为有序数组，mid左边的数都是比mid小的数，所以左边的数我们无需考虑，此时下一步如下： 当前计算的mid&#x3D;4,即arr[mid]&#x3D;5,此时的target&lt;mid位置的值，所以我们只需再比较mid左边的值，详细步骤如下： 此时计算的mid&#x3D;min,此时再进行比较，发现我们需要查找的target的值等于当前mid的值，所以我们直接返回当前mid位置。 public class BinarySearch &#123; public static void main(String[] args) &#123; int[] arr = &#123;1, 2, 3, 4, 5, 6, 7, 8, 9, 10&#125;; int target = 4; System.out.println(binarySearch(arr, target)); &#125; private static int binarySearch(int[] arr, int target) &#123; int min = 0; int max = arr.length - 1; int mid = min + (max - min) / 2; while (min &lt;= max) &#123; if (arr[mid] == target) &#123; return mid; &#125; if (arr[mid] &lt; target) &#123; min = mid + 1; mid = min + (max - min) / 2; &#125; if (arr[mid] &gt; target) &#123; max = mid - 1; mid = min + (max - min) / 2; &#125; &#125; return -1; &#125;&#125;","categories":["数据结构"]},{"title":"基础","path":"/2023/11/27/数据结构/基础知识/","content":"基础知识线性结构​\t常见的有：数组、队列、链表和栈 稀疏数组​\t当一个数组中大部分元素为0，或者同一个值的数组时，可以使用稀疏数组保存该数据 稀疏数组的处理方法是： ​\t1）记录数组一共有几行几列，有多少个不同的值 ​\t2）把具有不同值的元素的行列及值记录在一个小规模的数组中，从而缩小数组的规模 ​\t二维数组转稀疏数组的思路 1.遍历原始二维数组，读取有效数据个数sum 2.根据sum的值创建稀疏数组 int [sum +1] [3] 3.将二维数组的有效数据依次填到稀疏数组 ​\t稀疏数组转二维数组的思路 1.读取稀疏数组第一行数据，得到原二维数组的行和列，还原二维数组，比如上面的chessArr2 &#x3D; int[6] [7] 2.再读取稀疏数组后几行的数据，依次还原到对应的二维数组的位置 代码实现 public class SparseArray &#123; public static void main(String[] args) &#123; //创建一个原始的二维数组 11*11 //0：表示没有棋子 1：表示黑子 2：表示蓝子 int[][] chessArr1 = new int[11][11]; chessArr1[1][2] = 1; chessArr1[2][3] = 2; chessArr1[3][4] = 2; //输出原始二维数组 System.out.println(&quot;原始二维数组&quot;); for (int[] row : chessArr1) &#123; for (int data : row) &#123; System.out.printf(&quot;%d\\t&quot;,data); &#125; System.out.println(); &#125; //将二维数组转换为稀疏数组思路 //1.先遍历二维数组，得到非0数据的个数 int sum = 0; for (int i = 0; i &lt; 11; i++) &#123; for (int j = 0; j &lt; 11; j++) &#123; if (chessArr1[i][j] != 0)&#123; sum++; &#125; &#125; &#125; System.out.println(&quot;非0的个数：&quot; + sum); //2.创建对应的稀疏数组int[sum+1][3] int[][] sparseArr = new int[sum+1][3]; //给稀疏数组赋值 sparseArr[0][0] = 11; sparseArr[0][1] = 11; sparseArr[0][2] = sum; //遍历二维数组，将非0的值存放到sparseArr中 //count变量，count用来计二维数组中非0值的个数 int count = 0; for (int i = 0; i &lt; 11; i++) &#123; for (int j = 0; j &lt; 11; j++) &#123; if (chessArr1[i][j] != 0)&#123; count++; sparseArr[count][0] = i; sparseArr[count][1] = j; sparseArr[count][2] = chessArr1[i][j]; &#125; &#125; &#125; //输出稀疏数组的形式 System.out.println(); System.out.println(&quot;得到的稀疏数组为:&quot;); for (int i = 0; i &lt; sparseArr.length; i++) &#123; System.out.printf(&quot;%d\\t%d\\t%d\\t &quot;,sparseArr[i][0],sparseArr[i][1],sparseArr[i][2]); &#125; //稀疏数组恢复成原始数组 //1.读取稀疏数组第一行，创建原始二维数组 int[][] chessArr2 = new int[sparseArr[0][0]][sparseArr[0][1]]; //2.再读取稀疏数组后几行数据，填到原始二维数组对应位置 for (int i = 1; i &lt; sparseArr.length; i++) &#123; chessArr2[sparseArr[i][0]][sparseArr[i][1]] = sparseArr[i][2]; &#125; System.out.println(&quot;恢复后的二维数组&quot;); for (int[] row : chessArr2) &#123; for (int data : row) &#123; System.out.printf(&quot;%d\\t&quot;,data); &#125; System.out.println(); &#125; &#125;&#125; 非线性结构 常见的有：二维数组、多维数组、广义表、数结构、图结构","categories":["数据结构"]},{"title":"3、JVM运行时内存","path":"/2023/11/27/JVM/3、运行时内存/","content":"一、JVM运行时内存区JVM内存区域也被称为JVM运行时数据区，主要包含程序计数器、虚拟机栈、本地方法栈、堆空间、元数据空间（方法区）、运行时常量池、字符串常量池、直接内存（本地内存）等 1.1、线程私有区1.1.1 、程序计数器（Progran Counter Register）​ 程序计数器是JVM为每条线程开辟的一块较小的区域，每条线程都有且只有一个程序计数器，线程之间不相互干扰。生命周期与线程一致，随线程启动而生，线程销毁而亡。同时也是JVM所有内存区域中唯一不会发生OOM（OutOfMemoryError&#x2F;内存溢出）的区域，GC机制不会触及的区域。 ​ 主要是作为当前线程执行时的字节码行号指示器来使用的，当线程执行一个Java方法时，记录线程正在执行的字节码指令地址，当执行引擎处理完某个指令后，程序计数器需要进行对应更新，将指针改向下一条要执行的指令地址，执行引擎会根据PC计数器中记录的地址进行对应的指令执行。当线程在执行一些由C/C++编写的Native方法时，PC计数器中则为空（Undefined）。除此作用之外，也可以保证线程发生CPU时间片切换后能恢复到正确的位置执行。 1.1.2、虚拟机栈（Stack）​ 栈的作用是负责程序运行时具体如何执行、如何处理数据等工作。生命周期与线程一致，每个线程创建时都会为之创建一个虚拟机栈。 ​ 当线程在执行一个Java方法时，都会为执行的方法生产一个栈帧（StackFrame）。一个栈帧中主要包含局部变量表、操作数栈、动态链接、方法出口等信息。 栈不存在GC，但是可能会OOM 1.如何设置栈内存大小? ​\t-Xss size 一般默认为512k~1024k 1.1.2.1、局部变量表 定义为一个由槽（slot）数字数组，用于存放当前实例对象的引用信息、方法参数以及方法体内定义的基本数据类型变量、对象引用以及返回地址等信息 在Class文件的方法表的Code属性的max_locals指定了该方法所需局部变量表的最大容量。 局部变量表所容纳的大小是在编译期确定下来的 方法嵌套调用的次数是由栈的大小决定的 局部变量表的变量只在当前方法调用中有效。方法调用结束后，随着方法栈桢的销毁，局部变量表也销毁 槽（slot）：槽是局部变量表中的最小单位，规定大小为32bit，对于32bit的数据，比如int类型的变量，指针压缩后的对象信息等，都会使用一个槽来存储。而对于64bit等数据，如long、double类型的变量、未开启指针压缩的对象引用等数据，JVM会为其分配两个连续的槽空间进行存储。 ​ 局部变量表中存储的数据只对当前方法有效，虚拟机在执行时，依靠于操作数栈和局部变量表中存储的数据完成操作。方法执行结束后，局部变量表会随着栈桢的出栈\\销毁而销毁。 ​ 一般而言，如果当前方法属于构造方法或者实例方法，那么这些方法的局部变量表的下标为0的槽位存储的必然是this引用。 1.1.2.2、操作数栈（Operand Stack）​ 操作数栈是一个遵循FILO先进后出模式的栈结构。在class文件的Code属性的max_stacks定义了执行过程中最大的栈深度（在编译期就确定）。 ​ 在执行一个方法时，会先创建一个与该方法对应的栈桢，该栈桢中最初的操作数栈是空的，在执行过程中，会根据字节码指令往栈中写入或提取数据。操作数栈的主要目的是用于保存计算过程的中间结果，同时作为计算过程中变量的临时存储。 与局部变量表一样，操作数栈也是由一个32bit的字节数组组成的，操作数栈可以支持的类型：int、long、float、double、reference、returnType等类型，对于byte、short、char类型的数据会在入栈前被转化为int类型。与局部变量表不同的是，操作数栈是通过压栈、出栈方式完成数据访问。 1.1.2.3、动态链接（Dynamic Linking）​ 虚拟机栈中的每个栈桢都包含一个指向运行时常量池中该栈桢所属方法的引用，持有这个引用是为了支持方法调用过程中的动态链接。 ​ 在Java源文件被编译成Class文件时，类中所有的变量、方法调用都会化为符号引用，然后保存在class文件的常量池中，在class文件中描述一个方法调用另一个方法时，就使用常量池中指向方法的符号引用来表示的。动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用。 常量池：位于编译后生成的class字节码文件中 运行时常量池：位于运行期间的元数据空间&#x2F;方法区中 1.1.2.4、方法出口（Return Address）一个方法执行时，只有两种情况会导致方法退出，一种是在执行过程中遇到了正常返回的字节码指令，如：ireturn、lreturn、dreturn、areturn、return，释义如下： ireturn：返回值为int、byte、char、short、boolean类型时使用该指令返回 lreturn：返回值为long类型时使用该指令返回 dreturn：返回值为double类型时使用该指令返回 areturn：返回值为引用类型时使用该指令返回 return：无返回void、类或接口初始化方法时使用该指令返回 方法正常执行完成后退出的情况被称为正常完成出口，一般执行返回的字节码指令时，调用者的程序计数器会被作为返回的地址。 除开正常执行完成后退出的情况外，还有一种情况也会导致方法的退出，那就是方法执行过程中出现了异常，并且在方法体中没有处理该异常（没有try/catch），此时也会导致方法退出，这种情况下被称为异常完成出口，返回地址则需要通过异常处理器表来确定。 当一个方法执行结束退出时，会执行如下步骤： ①复原上层方法的局部变量表以及操作数栈。 ②如果当前方法有返回值的情况下，把返回值压入调用者方法栈帧的操作数栈中。 ③将PC计数器的地址指向改为方法下一条指令的位置，从而使得调用者正常工作。 PS：异常退出的情况下，是不会给上层调用者返回任何值的。 1.1.2.5、附加信息1.1.2.6、虚拟机栈的特点和运行原理采用数组这种快捷有效的存储方式，同时在运行时也被放在内存中，并且也会将操作数栈的栈顶数据放入高速缓存或寄存器中，所以从访问速度上来看， 仅次于PC寄存器。 虚拟机栈这块内存区域不存在垃圾回收，但是存在OOM，在《Java虚拟机规范》中，对这个区域规定了两种异常： StackOverflowError：当前线程请求的栈深度大于虚拟机栈所允许的深度时抛出该异常。 OutOfMemoryError：如果扩展时无法申请到足够的内存空间会抛出OOM异常。 对于每条线程的虚拟机栈大小可以通过-Xss参数进行调整，默认单位为字节，默认大小为1MB/1024KB/1048576字节。 public void a()&#123; int b_result = b();&#125;public int b()&#123; c(); return 9;&#125;public void c()&#123; // ....&#125; 对于这条线程而言，栈中的所有栈帧在同一时刻时，只会存在一个活动栈帧，也就是位于栈顶的栈帧，也就是我们前面所说的当前栈帧。执行引擎执行时，只会执行当前栈帧的字节码指令，如果执行当前方法时，在其中调用了其他方法，那么另外一个方法对应的栈帧会被创建出来，放在顶端，从而成为新的当前帧，接着执行引擎会去执行新帧，当该帧执行结束时，会传回此方法的执行结果给前一个栈帧，也就是上层调用者，比如上述案例中a()就是b()的上层调用者，接着虚拟机会丢弃当前栈帧，使得前一个栈帧重新成为栈顶的当前帧。这个过程会不断重复，直至一条方法调用链结束或因为异常中断，才会停止。 1.1.3、本地方法栈（Native Method Stack）​ 本地方法栈和虚拟机栈差不多，区别是虚拟机栈是执行java方法的，本地方法栈是执行C编写的Native本地方法。 在HotSpot虚拟机中，二者合二为一了 1.2、线程共享区​ 线程共享的含义是：在运行时，这些区域对程序中所有线程都是可见的，伴随着JVM的生命周期同生共死。 ​ 运行时数据区主要包含：堆空间、元数据空间（方法区）、直接内存三块 1.2.1、Java堆空间（heap）​ 大部分的JVM调优手段都是基于堆空间而进行展开的。堆主要解决的问题是数据存储问题，重点是数据怎么存，放哪里，怎么放等。 “-Xms”用于表示堆内存的起始内存，等价于-XX:InitialHeapSize “-Xmx”用于表示对内存的最大内存，等价于-XX:MaxHeapSize ​ 通常会将-Xms 和 -Xmx两个参数配置成同样的值，其目的是为了能够在Java垃圾回收机制清理完堆区后不需要重新分割计算堆区的大小，从而提高性能。 ​ 堆空间会在JVM启动时创建，对于JVM来说，堆空间是唯一的，每个JVM都只会存在一个堆空间。我们可以通过参数-Xms和-Xmx指定堆的起始内存大小和最大内存大小，当超过-Xmx指定的大小时则会抛出OOM 默认情况下，如果不指定堆内存大小，起始大小默认为当前物理机内存的1&#x2F;64，最大默认为当前物理机内存的1&#x2F;4。 ​ 在Java程序运行时，系统运行过程中产生的大部分实例对象及数组对象都会放到堆中存储。 堆空间在物理上可以是不连续的，只需要逻辑上视为连续即可。所以一个JVM的堆空间在实际的机器内存上，可能是由机器内存中多个不同位置的空间组成的。 在不同Java版本中，堆空间也发生了不同的改变。 JDK7及之前：堆空间包含新生代、年老代、永久代 JDK8：堆空间包含新生代、年老代，永久代被改为元数据空间，位于堆之外 JDK9：堆空间从逻辑上保留了分代的概念，但物理上本身不分代 JDK11：堆空间从此以后逻辑上和物理上都不分代 1.2.1.1、分代堆空间​\t堆空间会被分为不同的区域用于存储不同的生命周期的对象实例。JDK8之前，也就是一个Eden区、两个Survivor区（From&#x2F;To区）以及一个old区，从物理上来说都是连续的内存，每个区域存储不同周期的对象的实例。 1.2.1.2、不分代堆空间​ 到了JDK1.9时，G1成为了JVM内嵌的默认GC器，Java堆空间从此出现了不分代的概念，但不分代也分为两种情况，一种是逻辑分代，物理不分代，另一种则是逻辑+物理都不分代。 逻辑分代，物理不分代（G1）：对象分配的逻辑上还是存在分代的思想，但是物理内存上不会再分为几块完整的分代空间。 逻辑+物理都不分代（ZGC、ShenandoahGC）：无论从对象分配的逻辑上还是物理内存上，都不存在分代的概念。 1.2.1.3、JDK7及之前的堆空间内存划分​ 一般堆空间会被划分为三个区域：新生代、年老代、永久代 新生代：一个Eden区，两个Survivor区（From&#x2F;To区）以及一个old区，比例时8:1:1 (几乎所有的对象都是在Eden区被new出来的，大对象除外) 年老代：一个old区 永久代：方法区 ​ 新生代主要用于存储未达到年老代分配条件的对象，其中Eden区是专门用来存储刚创建出来的对象实例，两个Survivor区主要用于垃圾回收时给存活对象“避难”。​ 年老代主要用于存储达到符合分配条件的对象实例，比如达到“年龄”的对象以及过大“体积”的大对象等。​ 方法区&#x2F;永久代主要用于存储类的元数据信息，如类描述信息、字段信息、方法信息、静态变量信息、异常表、方法表等。 1.2.1.4、JDK8堆空间内存划分 ​ 最大区别在于移除了方法区，在本地内存中加入了元数据空间来存储之前方法区中的大部分数据（原方法区中的数据并不是所有都被迁移到了元空间存储，有些数据被分散到了JVM各个区域）。除此之外，常量池在1.8的时候也被移到了堆外。 1.2.1.5、JDK9堆空间内存划分​ 在此之前，堆空间的内存布局都是分代存储的方式。到了JDK9，内存区域被划分为一个个的Region区。 ​ 在JDK1.9时，G1将Java堆划分为多个大小相等的独立的Region区域，不过在HotSpot的源码TARGET_REGION_NUMBER定义了Region区的数量限制为2048个（实际上允许超过这个值，但是超过这个数量后，堆空间会变的难以管理）。 一般Region区的大小等于堆空间的总大小除以2048，比如目前的堆空间总大小为8GB，就是8192MB/2048=4MB，那么最终每个Region区的大小为4MB，当然也可以用参数-XX:G1HeapRegionSize强制指定每个Region区的大小，但是不推荐，毕竟默认的计算方式计算出的大小是最适合管理堆空间的。 G1保留了年轻代和老年代的概念，但不再是物理隔阂了，它们都是可以不连续物理内存来组成的Region的集合。 在G1中，判定一个对象是否为大对象的方式为：对象大小是否超过单个普通Region区的50%，如果超过则代表当前对象为大对象，那么该对象会被直接放入Humongous区 1.2.1.6、JDK11堆空间内存划分​ JDK11推出了一款新的垃圾回收器ZGC。 ​ ZGC也会把堆空间划分为一个个的Regin区，不同的是，ZGC中不存在分代，只是简单的把Region区分为大、中、小三个等级： 小型Region区（small）：固定大小为2MB，用于分配小于256kb大对象 中型Region区（Medium）：固定大小为32MB，用于分配大于256kb，小于4MB大对象 大型Region区（Large）：没有固定大小，但是大小必须为2MB的整数倍，用于存放大于4MB的对象，每个Large区只能存放一个大对象 1.2.2、本地内存​ 运行时数据区中的本地内存主要分为两块，一部分是元数据空间（原方法区），另一部分则为直接内存。 1.2.2.1 元数据空间（MetaSpace）&#x2F;方法区(MethodArea)​ 所有的方法区在逻辑上属于堆的一部分，但是一些简单的实现可能不会选择去进行垃圾回收或压缩，对于HotSpot来说，方法区还有一个别名，叫做非堆（Non-heap），目的就是为了和堆分开。 ​ 元数据空间大小设置 -XX:MetaSpaceSize -XX:MaxMetaSpaceSize 方法区也就是所谓的永久代、持久代，方法区主要存储了可以通过反射机制拿到的所有数据，如Class类信息、Method方法信息、Field字段信息 方法区主要存储的数据：类的元数据、VM的内部类、类的层级信息&#x2F;方法信息&#x2F;字段信息、方法的编译信息和字节码数据、静态变量、常量池以及方法引用。 而之前方法区运行时常量池中的字符串常量池则被放置在了堆中，因为在程序运行过程中会随着运行时间的增加，字符串常量池中的字符串会越来越多，所占空间会越来越大，所以将其放在堆中的好处在于：使得字符串常量池在GC机制的范围之内，字符串也会存在回收操作。 同时除开字符串常量池被挪动到了堆内之外，类的静态变量的存储也被放在了堆中。对比如下 1.2.2.2、直接内存​ 直接内存这块区域不是虚拟机的内存区域，在创建时会直接向操作系统申请内存空间，属于直接使用物理内存的一块区域，也被称为“堆外空间”。 ​ 对比堆空间而言，访问直接内存的速度会超出堆内存，也就是读写性能优于Java堆，来源于Java的NIO库，Java的NIO可以允许Java程序直接使用本地的直接内存存储数据缓冲，因为如果把一些文件数据转为对象存储在堆中时，很容易导致堆空间负载过重而OOM。所以出于性能和稳定性两方面的考虑，一般对于一些读写频繁的场景或读取&#x2F;写出大文件时的场景都可以使用直接内存进行操作 如果程序中需要用到直接内存时可以通过java.nio.ByteBuffer来创建，调用allocateDirect方法申请即可，同时可以通过存在堆中的DirectByteBuffer操作直接内存。 ​ 直接内存的最大空间值可以通过-XX:MaxDirectMemorySize设置，如果不指定则默认与-Xmx参数设置的空间大小一致 ​ 一般在使用直接内存的时候，不能将希望寄托给GC机制的全局GC来管理内存，因此我们可以和C语言一样，尝试自己写一个回收直接内存的方法，然后使用完成后自己手动回收申请的内存 import java.nio.ByteBuffer;import sun.nio.ch.DirectBuffer;public class NonHeapGC &#123; public static void clean(final ByteBuffer byteBuffer) &#123; if (byteBuffer.isDirect()) &#123; ((DirectBuffer)byteBuffer).cleaner().clean(); &#125; &#125; public static void sleep(long i) &#123; try &#123; Thread.sleep(i); &#125;catch(Exception e) &#123; /*skip*/ &#125; &#125; public static void main(String []args) throws Exception &#123; ByteBuffer buffer = ByteBuffer.allocateDirect(1024 * 1024 * 200); System.out.println(&quot;start&quot;); sleep(5000); clean(buffer);//执行垃圾回收// System.gc(); //执行Full gc进行垃圾回收 System.out.println(&quot;end&quot;); sleep(5000); &#125; &#125; 当使用完成申请的内存空间后，可以手动调用clean()方法进行内存的回收释放。 二、内存溢出OOM（OutOfMemory）​ 除程序计数器外，其他区域都存在OOM 2.1、Java堆空间OOM java.lang.OutOfMemoryError: Java heap space ​ Java堆空间是用于存储对象实例和数组数据的内存区域，同时JVM的GC机制也会重点对于这块区域进行内存管理。但是如果内存不足发生GC时，堆中的对象都还存活，此时又没有足够的内存分配新的对象实例，最终堆空间就会出现OOM 2.2、虚拟机栈和本地方法栈OOM StackOverflowError：当前线程请求的栈深度大于虚拟机栈所允许的深度时抛出该异常。 OutOfMemoryError：如果扩展时无法申请到足够的内存空间会抛出OOM异常。 栈OOM的条件为：如果栈空间扩展时无法申请到足够的内存空间会抛出OOM异常。 2.3、元数据空间和运行时常量池OOM​ 元数据空间主要存储类名、访问修饰符、常量池、字段描述、方法描述等信息，对于测试元数据空间的内存溢出基本思路是：在运行时产生大量类字节码，从而使得元数据空间内存被耗尽，从而抛出OOM。 三、内存泄漏（MemoryLeak）​ 内存泄露是指程序分配的内存由于某些原因未释放或无法释放，造成系统内存的浪费。针对于Java而言，是指申请的内存空间没有被正确释放，存储在该区域的数据使用完后没有被回收，而指向这块区域的直接指针却不存在了，但还有其他引用可以关联到该区域，造成数据已经失效，引用链依旧保持，GC无法回收的情况出现，最终导致后续程序里这块内存被永远占用（不可达），内存空间就这么一点点被蚕食，最后导致程序运行缓慢、内存耗尽的问题出现。","categories":["JVM"]},{"title":"Saas多租户字段隔离","path":"/2023/11/27/Saas租户/SaaS多租户字段隔离/","content":"SaaS 多租户【字段隔离】本章节，将介绍多租户的基础知识、以及怎样使用多租户的功能。 相关的视频教程： 01、如何实现多租户的 DB 封装？(opens new window) 02、如何实现多租户的 Redis 封装？(opens new window) 03、如何实现多租户的 Web 与 Security 封装？(opens new window) 04、如何实现多租户的 Job 封装？(opens new window) 05、如何实现多租户的 MQ 与 Async 封装？(opens new window) 06、如何实现多租户的 AOP 与 Util 封装？(opens new window) 07、如何实现多租户的管理？(opens new window) 08、如何实现多租户的套餐？(opens new window) #1. 多租户是什么？多租户，简单来说是指一个业务系统，可以为多个组织服务，并且组织之间的数据是隔离的。 例如说，在服务上部署了一个 ruoyi-vue-pro (opens new window)系统，可以支持多个不同的公司使用。这里的一个公司就是一个租户，每个用户必然属于某个租户。因此，用户也只能看见自己租户下面的内容，其它租户的内容对他是不可见的。 #2. 数据隔离方案多租户的数据隔离方案，可以分成分成三种： DATASOURCE 模式：独立数据库 SCHEMA 模式：共享数据库，独立 Schema COLUMN 模式：共享数据库，共享 Schema，共享数据表 #2.1 DATASOURCE 模式一个租户一个数据库，这种方案的用户数据隔离级别最高，安全性最好，但成本也高。 优点：为不同的租户提供独立的数据库，有助于简化数据模型的扩展设计，满足不同租户的独特需求；如果出现故障，恢复数据比较简单。 缺点：增大了数据库的安装数量，随之带来维护成本和购置成本的增加。 #2.2 SCHEMA 模式多个或所有租户共享数据库，但一个租户一个表。 优点：为安全性要求较高的租户提供了一定程度的逻辑数据隔离，并不是完全隔离；每个数据库可以支持更多的租户数量。 缺点：如果出现故障，数据恢复比较困难，因为恢复数据库将牵扯到其他租户的数据； 如果需要跨租户统计数据，存在一定困难。 #2.3 COLUMN 模式共享数据库，共享数据架构。租户共享同一个数据库、同一个表，但在表中通过 tenant_id 字段区分租户的数据。这是共享程度最高、隔离级别最低的模式。 优点：维护和购置成本最低，允许每个数据库支持的租户数量最多。 缺点：隔离级别最低，安全性最低，需要在设计开发时加大对安全的开发量；数据备份和恢复最困难，需要逐表逐条备份和还原。 #2.4 方案选择 一般情况下，可以考虑采用 COLUMN 模式，开发、运维简单，以最少的服务器为最多的租户提供服务。 租户规模比较大，或者一些租户对安全性要求较高，可以考虑采用 DATASOURCE 模式，当然它也相对复杂的多。 不推荐采用 SCHEMA 模式，因为它的优点并不明显，而且它的缺点也很明显，同时对复杂 SQL 支持一般。 提问：项目支持哪些模式？ 目前支持最主流的 DATASOURCE 和 COLUMN 两种模式。而 SCHEMA 模式不推荐使用，所以暂时不考虑实现。 考虑到让大家更好的理解 DATASOURCE 和 COLUMN 模式，拆成了两篇文章： 《SaaS 多租户【字段隔离】》：讲解 COLUMN 模式 《SaaS 多租户【数据库隔离】》：讲解 DATASOURCE 模式 #3. 多租户的开关系统有两个配置项，设置为 true 时开启多租户，设置为 false 时关闭多租户。 注意，两者需要保持一致，否则会报错！ 配置项 说明 配置文件 yudao.server.tenant 后端开关 VUE_APP_TENANT_ENABLE 前端开关 疑问：为什么要设置两个配置项？ 前端登录界面需要使用到多租户的配置项，从后端加载配置项的话，体验会比较差。 #4. 多租户的业务功能多租户主要有两个业务功能： 业务功能 说明 界面 代码 租户管理 配置系统租户，创建对应的租户管理员 后端 (opens new window)前端(opens new window) 租户套餐 配置租户套餐，自定每个租户的菜单、操作、按钮的权限 后端 (opens new window)前端(opens new window) 下面，我们来新增一个租户，它使用 COLUMN 模式。 ① 点击 [租户套餐] 菜单，点击 [新增] 按钮，填写租户的信息。 ② 点击 [确认] 按钮，完成租户的创建，它会自动创建对应的租户管理员、角色等信息。 ③ 退出系统，登录刚创建的租户。 至此，我们已经完成了租户的创建。 #5. 多租户的技术组件技术组件 yudao-spring-boot-starter-biz-tenant (opens new window)，实现透明化的多租户能力，针对 Web、Security、DB、Redis、AOP、Job、MQ、Async 等多个层面进行封装。 #5.1 租户上下文TenantContextHolder (opens new window)是租户上下文，通过 ThreadLocal 实现租户编号的共享与传递。 通过调用 TenantContextHolder 的 #getTenantId() 静态方法，获得当前的租户编号。绝绝绝大多数情况下，并不需要。 #5.2 Web 层【重要】 实现可见 web (opens new window)包。 默认情况下，前端的每个请求 Header 必须带上 tenant-id，值为租户编号，即 system_tenant 表的主键编号。 如果不带该请求头，会报“租户的请求未传递，请进行排查”错误提示。 😜 通过 yudao.tenant.ignore-urls 配置项，可以设置哪些 URL 无需带该请求头。例如说： #5.3 Security 层 实现可见 security (opens new window)包。 主要是校验登录的用户，校验是否有权限访问该租户，避免越权问题。 #5.4 DB 层【重要】 实现可见 db (opens new window)包。 COLUMN 模式，基于 MyBatis Plus 自带的多租户 (opens new window)功能实现。 核心：每次对数据库操作时，它会自动拼接 WHERE tenant_id = ? 条件来进行租户的过滤，并且基本支持所有的 SQL 场景。 如下是具体方式： ① 需要开启多租户的表，必须添加 tenant_id 字段。例如说 system_users、system_role 等表。 CREATE TABLE `system_role` ( `id` bigint NOT NULL AUTO_INCREMENT COMMENT &#x27;角色ID&#x27;, `name` varchar(30) CHARACTER NOT NULL COMMENT &#x27;角色名称&#x27;, `tenant_id` bigint NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;租户编号&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=1 COMMENT=&#x27;角色信息表&#x27;; 并且该表对应的 DO 需要使用到 tenantId 属性时，建议继承 TenantBaseDO (opens new window)类。 ② 无需开启多租户的表，需要添加表名到 yudao.tenant.ignore-tables 配置项目。例如说： 如果不配置的话，MyBatis Plus 会自动拼接 WHERE tenant_id = ? 条件，导致报 tenant_id 字段不存在的错误。 #5.5 Redis 层【重要】 实现可见 redis (opens new window)包。 由于 Redis 不同于 DB 有 tenant_id 字段，无法通过类似 WHERE tenant_id &#x3D; ? 的方式过滤，所以需要通过在 Redis Key 上增加 :t&#123;tenantId&#125; 后缀的方式，进行租户之间的隔离。 例如说，假设 Redis Key 是 user:%d，示例是 user:1024；对应到多租户 1 的 Redis Key 是 user:t1:1024。 为什么 Redis Key 要多租户隔离呢？ ① 在使用 DATASOURCE 模式时，不同库的相同表的 id 可能相同，例如说 A 库的用户，和 B 库的用户都是 1024，直接缓存会存在 Redis Key 的冲突。 ② 在所有模式下，跨租户可能存在相同的需要唯一的数据，例如说用户的手机号，直接缓存会存在 Redis Key 的冲突。 #使用方式一：基于 Spring Cache + Redis【推荐】只需要一步，在方法上添加 Spring Cache 注解，例如说 @Cachable、@CachePut、@CacheEvict。 具体的实现原理，可见 TenantRedisCacheManager (opens new window)的源码。 注意！！！默认配置下，Spring Cache 都开启 Redis Key 的多租户隔离。如果不需要，可以将 Key 添加到 yudao.tenant.ignore-cache 配置项中。如下图所示： #使用方式二：基于 RedisTemplate + TenantRedisKeyDefine暂时没有合适的封装，需要在自己 format Redis Key 的时候，手动将 :t&#123;tenantId&#125; 后缀拼接上。 这也是为什么，我推荐你使用 Spring Cache + Redis 的原因！ #5.6 AOP【重要】 实现可见 aop (opens new window)包。 ① 声明 @TenantIgnore (opens new window)注解在方法上，标记指定方法不进行租户的自动过滤，避免自动拼接 WHERE tenant_id = ? 条件等等。 例如说：RoleServiceImpl (opens new window)的 #initLocalCache() (opens new window)方法，加载所有租户的角色到内存进行缓存，如果不声明 @TenantIgnore 注解，会导致租户的自动过滤，只加载了某个租户的角色。 // RoleServiceImpl.javapublic class RoleServiceImpl implements RoleService &#123; @Resource @Lazy // 注入自己，所以延迟加载 private RoleService self; @Override @PostConstruct @TenantIgnore // 忽略自动多租户，全局初始化缓存 public void initLocalCache() &#123; // ... 从数据库中，加载角色 &#125; @Scheduled(fixedDelay = SCHEDULER_PERIOD, initialDelay = SCHEDULER_PERIOD) public void schedulePeriodicRefresh() &#123; self.initLocalCache(); // &lt;x&gt; 通过 self 引用到 Spring 代理对象 &#125;&#125; 有一点要格外注意，由于 @TenantIgnore 注解是基于 Spring AOP 实现，如果是方法内部的调用，避免使用 this 导致不生效，可以采用上述示例的 &lt;x&gt; 处的 self 方式。 ② 使用 TenantUtils (opens new window)的 #execute(Long tenantId, Runnable runnable) 方法，模拟指定租户( tenantId )，执行某段业务逻辑( runnable )。 例如说：在 TenantServiceImpl (opens new window)的 #createTenant(...) 方法，在创建完租户时，需要模拟该租户，进行用户和角色的创建。如下图所示： #5.7 Job【重要】 实现可见 job (opens new window)包。 声明 @TenantJob (opens new window)注解在 Job 类上，实现并行遍历每个租户，执行定时任务的逻辑。 #5.8 MQ 实现可见 mq (opens new window)包。 通过租户对 MQ 层面的封装，实现租户上下文，可以继续传递到 MQ 消费的逻辑中，避免丢失的问题。实现原理是： 发送消息时，MQ 会将租户上下文的租户编号，记录到 Message 消息头 tenant-id 上。 消费消息时，MQ 会将 Message 消息头 tenant-id，设置到租户上下文的租户编号。 #5.9 Async 实现可见 YudaoAsyncAutoConfiguration (opens new window)类。 通过使用阿里开源的 TransmittableThreadLocal (opens new window)组件，实现 Spring Async 执行异步逻辑时，租户上下文可以继续传递，避免丢失的问题。","categories":["Saas多租户"]},{"title":"会签或签","path":"/2023/11/27/activiti/会签或签/","content":"工作流（Flowable）会签、或签项目基于 Flowable 实现了工作流的功能。本章节，我们将介绍工作流的相关功能。 以请假流程为例，讲解系统支持的两种表单方式的工作流： 流程表单：在线配置动态表单，无需建表与开发 业务表单：业务需建立独立的数据库表，并开发对应的表单、详情界面 整个过程包括： 定义流程：【管理员】新建流程、设计流程模型、并设置用户任务的审批人，最终发布流程 发起流程：【员工】选择流程，并发起流程实例 审批流程：【审批人】接收到流程任务，审批结果为通过或不通过 01、如何集成 Flowable 框架？(opens new window) 02、如何实现动态的流程表单？(opens new window) 03、如何实现流程表单的保存？(opens new window) 04、如何实现流程表单的展示？(opens new window) 05、如何实现流程模型的新建？(opens new window) 06、如何实现流程模型的流程图的设计？(opens new window) 07、如何实现流程模型的流程图的预览？(opens new window) 08、如何实现流程模型的分配规则？(opens new window) 09、如何实现流程模型的发布？(opens new window) 10、如何实现流程定义的查询？(opens new window) 11、如何实现流程的发起？(opens new window) 12、如何实现我的流程列表？(opens new window) 13、如何实现流程的取消？(opens new window) 14、如何实现流程的任务分配？(opens new window) 15、如何实现会签、或签任务？(opens new window) 16、如何实现我的待办任务列表？(opens new window) 17、如何实现我的已办任务列表？(opens new window) 18、如何实现任务的审批通过？(opens new window) 19、如何实现任务的审批不通过？(opens new window) 20、如何实现流程的审批记录？(opens new window) 21、如何实现流程的流程图的高亮？(opens new window) 22、如何实现工作流的短信通知？(opens new window) 23、如何实现 OA 请假的发起？(opens new window) 24、如何实现 OA 请假的审批？(opens new window) #0. 如何开启 bpm 模块？yudao-module-bpm 模块默认未启用，需要手动开启。步骤如下： ① 第一步，修改根目录的 pom.xml (opens new window)文件，取消 yudao-module-bpm 模块的注释。 ② 第二步，修改 yudao-server 的 pom.xml (opens new window)文件，取消 yudao-module-bpm-biz 依赖的注释，并进行 IDEA 的 Maven 刷新。 ③ 第三步，点击 bpm.sql 下载，然后导入到数据库中。 友情提示：↑↑↑ bpm.sql 是可以点击下载的！ ↑↑↑ ④ 第四步，重启项目，看到 Property Source flowable-liquibase-override refreshed 说明开启成功。 另外，启动过程中，Flowable 会自动创建 ACT_ 和 FLW_ 开头的表。 如果启动中报 MySQL “Specified key was too long; max key length is 1000 bytes” (opens new window)错误，可以将 MySQL 的缺省存储引擎设置为 innodb，即 default-storage-engine=innodb 配置项。 #1. 请假流程【流程表单】#1.1 第一步：定义流程登录账号 admin、密码 admin123 的用户，扮演【管理员】的角色，进行流程的定义。 ① 访问 [工作流程 -&gt; 流程管理 -&gt; 流程模型] 菜单，点击 [新建流程] 按钮，填写流程标识、流程名称。如下图所示： 流程标识：对应 BPMN 流程文件 XML 的 id 属性，不能重复，新建后不可修改。 流程名称：对应 BPMN 流程文件 XML 的 name 属性。 &lt;!-- 这是一个 BPMN XML 的示例，主要看 id 和 name 属性 --&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;bpmn2:definitions xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:bpmn2=&quot;http://www.omg.org/spec/BPMN/20100524/MODEL&quot; xmlns:bpmndi=&quot;http://www.omg.org/spec/BPMN/20100524/DI&quot; id=&quot;diagram_Process_1647305370393&quot; targetNamespace=&quot;http://activiti.org/bpmn&quot;&gt; &lt;bpmn2:process id=&quot;common-form&quot; name=&quot;通用表单流程&quot; isExecutable=&quot;true&quot; /&gt; &lt;bpmndi:BPMNDiagram id=&quot;BPMNDiagram_1&quot;&gt; &lt;bpmndi:BPMNPlane id=&quot;common-form_di&quot; bpmnElement=&quot;common-form&quot; /&gt; &lt;/bpmndi:BPMNDiagram&gt;&lt;/bpmn2:definitions&gt; ② 访问 [工作流程 -&gt; 流程管理 -&gt; 流程表单] 菜单，点击 [新增] 按钮，新增一个名字为 leave-form 的表单。如下图所示： 流程表单的实现？ 基于 https://github.com/JakHuang/form-generator (opens new window)项目实现的动态表单。 回到 [工作流程 -&gt; 流程管理 -&gt; 流程模型] 菜单，点击 [修改流程] 按钮，配置表单类型为流程表单，选择名字为 leave-form 的流程表单。如下图所示： ③ 点击 [设计流程] 按钮，在线设计请假流程模型，包含两个用户任务：领导审批、HR 审批。如下图所示： 设计流程的实现？ 基于 https://github.com/miyuesc/bpmn-process-designer (opens new window)项目实现，它的底层是 bpmn-js (opens new window)。 ④ 点击 [分配规则] 按钮，设置用户任务的审批人。其中，规则类型用于分配用户任务的审批人，目前有 7 种规则：角色、部门成员、部门负责人、岗位、用户、用户组、自定义脚本，基本可以满足绝大多数场景，是不是非常良心。 设置【领导审批】的规则类型为自定义脚本 + 流程发起人的一级领导，如下图所示： 设置【HR 审批】的规则类型为岗位 + 人力资源，如下图所示： 规则类型的实现？ 可见 BpmUserTaskActivityBehavior (opens new window)代码，目前暂时支持分配一个审批人。 ⑤ 点击 [发布流程] 按钮，把定义的流程模型部署出去。部署成功后，就可以发起该流程了。如下图所示： 修改流程后，需要重新发布流程吗？ 需要，必须重新发布才能生效。每次流程发布后，会生成一个新的流程定义，版本号从 v1 开始递增。 发布成功后，会部署新版本的流程定义，旧版本的流程定义将被挂起。当然，已经发起的流程不会受到影响，还是走老的流程定义。 #1.2 第二步：发起流程登录账号 admin、密码 admin123 的用户，扮演【员工】的角色，进行流程的发起。 ① 访问 [工作流程 -&gt; 任务管理 -&gt; 我的流程] 菜单，点击 [发起流程] 按钮，可以看到可以选择的流程定义的列表。 ② 选择名字为通用表单的流程定义，发起请假流程。填写请假表单信息如下： ③ 点击提交成功后，可在我的流程中，可看到该流程的状态、结果。 ④ 点击 [详情] 按钮，可以查看申请的表单信息、审批记录、流程跟踪图。 #1.2 第三步：审批流程（领导审批）登录账号 test、密码 test123 的用户，扮演【审批人】的角色，进行请假流程的【领导审批】任务。 ① 访问 [工作流程 -&gt; 任务管理 -&gt; 待办任务] 菜单，可以查询到需要审批的任务。 ② 点击 [审批] 按钮，填写审批建议，并点击 [通过] 按钮，这样任务的审批就完成了。 ③ 访问 [工作流程 -&gt; 任务管理 -&gt; 已办任务] 菜单，可以查询到已经审批的任务。 此时，使用【员工】的角色，访问 [工作流程 -&gt; 任务管理 -&gt; 我的流程] 菜单，可以看到流程流转到了【HR 审批】任务。 #1.3 第三步：审批流程（HR 审批）登录账号 hrmgr、密码 hr123 的用户，扮演【审批人】的角色，进行请假流程的【HR 审批】任务。 ① 访问 [工作流程 -&gt; 任务管理 -&gt; 待办任务] 菜单，点击 [审批] 按钮，填写审批建议，并点击 [通过] 按钮。 此时，使用【员工】的角色，访问 [工作流程 -&gt; 任务管理 -&gt; 我的流程] 菜单，可以看到流程处理结束，最终审批通过。 #2. 请假流程【业务表单】根据业务需要，业务通过建立独立的数据库表（业务表）记录申请信息，而流程引擎只负责推动流程的前进或者结束。两者需要进行双向的关联： 每一条业务表记录，通过它的流程实例的编号( process_instance_id )指向对应的流程实例 每一个流程实例，通过它的业务键( BUSINESS_KEY_ ) 指向对应的业务表记录。 以项目中提供的 OALeave (opens new window)请假举例子，它的业务表 bpm_oa_leave 和流程引擎的流程实例的关系如下图： 也因为业务建立了独立的业务表，所以必须开发业务表对应的列表、表单、详情页面。不过，审核相关的功能是无需重新开发的，原因是业务表已经关联对应的流程实例，流程引擎审批流程实例即可。 下面，我们以项目中的 OALeave (opens new window)为例子，详细讲解下业务表单的开发与使用的过程。 #2.0 第零步：业务开发① 新建业务表 bpm_oa_leave，建表语句如下： CREATE TABLE `bpm_oa_leave` ( `id` bigint NOT NULL AUTO_INCREMENT COMMENT &#x27;请假表单主键&#x27;, `user_id` bigint NOT NULL COMMENT &#x27;申请人的用户编号&#x27;, `type` tinyint NOT NULL COMMENT &#x27;请假类型&#x27;, `reason` varchar(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#x27;请假原因&#x27;, `start_time` datetime NOT NULL COMMENT &#x27;开始时间&#x27;, `end_time` datetime NOT NULL COMMENT &#x27;结束时间&#x27;, `day` tinyint NOT NULL COMMENT &#x27;请假天数&#x27;, `result` tinyint NOT NULL COMMENT &#x27;请假结果&#x27;, `process_instance_id` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;流程实例的编号&#x27;, `creator` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT &#x27;&#x27; COMMENT &#x27;创建者&#x27;, `create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `updater` varchar(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT &#x27;&#x27; COMMENT &#x27;更新者&#x27;, `update_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;更新时间&#x27;, `deleted` bit(1) NOT NULL DEFAULT b&#x27;0&#x27; COMMENT &#x27;是否删除&#x27;, `tenant_id` bigint NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;租户编号&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=26 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#x27;OA 请假申请表&#x27;; process_instance_id 字段，关联流程引擎的流程实例对应的 ACT_HI_PROCINST 表的 PROC_INST_ID_ 字段 result 字段，请假结果，需要通过 Listener 监听回调结果，稍后来看看 ② 实现业务表的【后端】业务逻辑，具体代码可以看看如下两个类： BpmOALeaveController(opens new window) BpmOALeaveServiceImpl(opens new window) 重点是看流程发起的逻辑，它定义了 /bpm/oa/leave/create 给业务的表单界面调用，UML 时序图如下： 具体的实现代码比较简单，如下图所示： PROCESS_KEY 静态变量：是业务对应的流程模型的编号，稍后会进行创建编号为 oa_leave 的流程模型。 BpmProcessInstanceApi (opens new window)定义了 #createProcessInstance(...) 方法，用于创建流程实例，业务无需关心底层是 Activiti 还是 Flowable 引擎，甚至未来可能的 Camunda 引擎。 ③ 实现业务表的【前端】业务逻辑，具体代码可以看看如下三个页面： leave/create.vue(opens new window) leave/detail.vue(opens new window) leave/index.vue(opens new window) 另外，在 router/index.js (opens new window)中定义 create.vue 和 detail.vue 的路由，配置如下： &#123; path: &#x27;/bpm&#x27;, component: Layout, hidden: true, redirect: &#x27;noredirect&#x27;, children: [&#123; path: &#x27;oa/leave/create&#x27;, component: (resolve) =&gt; require([&#x27;@/views/bpm/oa/leave/create&#x27;], resolve), name: &#x27;发起 OA 请假&#x27;, meta: &#123;title: &#x27;发起 OA 请假&#x27;, icon: &#x27;form&#x27;, activeMenu: &#x27;/bpm/oa/leave&#x27;&#125; &#125;, &#123; path: &#x27;oa/leave/detail&#x27;, component: (resolve) =&gt; require([&#x27;@/views/bpm/oa/leave/detail&#x27;], resolve), name: &#x27;查看 OA 请假&#x27;, meta: &#123;title: &#x27;查看 OA 请假&#x27;, icon: &#x27;view&#x27;, activeMenu: &#x27;/bpm/oa/leave&#x27;&#125; &#125; ]&#125; 为什么要做独立的 create.vue 和 index.vue 页面？ 创建流程时，需要跳转到 create.vue 页面，填写业务表的信息，才能提交流程。 审批流程时，需要跳转到 detail.vue 页面，查看业务表的信息。 ④ 实现业务表的【后端】监听逻辑，具体可见 BpmOALeaveResultListener (opens new window)监听器。它实现流程引擎定义的 BpmProcessInstanceResultEventListener (opens new window)抽象类，在流程实例结束时，回调通知它最终的结果是通过还是不通过。代码如下图： 至此，我们了解了 OALeave 使用业务表单所涉及到的开发，下面我们来定义对应的流程、发起该流程、并审批该流程。 #2.1 第一步：定义流程登录账号 admin、密码 admin123 的用户，扮演【管理员】的角色，进行流程的定义。 ① 访问 [工作流程 -&gt; 流程管理 -&gt; 流程模型] 菜单，点击 [新建流程] 按钮，填写流程标识、流程名称。如下图所示： 注意，流程标识需要填 oa_leave。因为在 BpmOALeaveServiceImpl 类中，规定了对应的流程标识为 oa_leave。 ② 点击 [修改流程] 按钮，配置表单类型为业务表单，填写表单提交路由为 /bpm/oa/leave/create（用于发起流程时，跳转的业务表单的路由）、表单查看路由为 /bpm/oa/leave/detail（用于在流程详情中，点击查看表单的路由）。如下图所示： ③ 点击 [设计流程] 按钮，在线设计请假流程模型，包含两个用户任务：领导审批、HR 审批。如下图所示： 可以点击 oa_leave_bpmn.XML 进行下载，然后点击 [打开文件] 按钮，进行导入。 ④ 点击 [分配规则] 按钮，设置用户任务的审批人。 设置【领导审批】的规则类型为自定义脚本 + 流程发起人的一级领导，如下图所示： 设置【HR 审批】的规则类型为岗位 + 人力资源，如下图所示： ⑤ 点击 [发布流程] 按钮，把定义的流程模型部署出去。部署成功后，就可以发起该流程了。 #2.1 第二步：发起流程登录账号 admin、密码 admin123 的用户，扮演【员工】的角色，进行流程的发起。 ① 发起业务表单请假流程，两种路径： 访问 [工作流程 -&gt; 任务管理 -&gt; 我的流程] 菜单，点击 [发起流程] 按钮，会跳转到流程模型 oa_leave 配置的表单提交路由。 &lt;img src&#x3D;”会签或签&#x2F;25.png” alt&#x3D;”路径一：我的流程 - style&#x3D;”zoom:67%;” &gt; 发起流程” &#x2F;&gt; 访问 [工作流程 -&gt; 请假查询] 菜单，点击 [发起请假] 按钮。 &lt;img src&#x3D;”会签或签&#x2F;26.png” alt&#x3D;”路径二：请假查询 - style&#x3D;”zoom:67%;” &gt; 发起请假” &#x2F;&gt; ② 填写一个小于等于 3 天的请假，只会走【领导审批】任务；填写一个大于 3 天的请假，在走完【领导审批】任务后，会额外走【HR 审批】任务。 后续的流程，和「1. 请假流程【流程表单】」是基本一致的，这里就不重复赘述，当然你还是要试着跑一跑，了解整个的过程。 #2.3 第三步：审批流程（领导审批）略~自己跑 #2.4 第三步：审批流程（HR 审批）略~自己跑 #2. 流程通知流程在发生变化时，会发送通知给相关的人。目前有三个场景会有通知，通过短信的方式。 #3. 流程图示例#3.1 会签定义：指同一个审批节点设置多个人，如 ABC 三人，三人会同时收到审批，需全部同意之后，审批才可到下一审批节点。 配置方式如下图所示： 重点是【完成条件】为 $&#123; nrOfCompletedInstances== nrOfInstances &#125;。 #3.2 或签定义：指同一个审批节点设置多个人，如ABC三人，三人会同时收到审批，只要其中任意一人审批即可到下一审批节点。 配置方式如下图所示： 重点是【完成条件】为 $&#123; nrOfCompletedInstances== 1 &#125;。 #4. 如何使用 Activiti？Activiti 和 Flowable 提供的 Java API 是基本一致的，例如说 Flowable 的 org.flowable.engine.RepositoryService 对应 Activiti 的 org.activiti.engine .RepositoryService。所以，我们可以修改 import 的包路径来替换。 另外，在项目的老版本，我们也提供了 Activiti 实现，你可以具体参考下： yudao-spring-boot-starter-activiti(opens new window) yudao-module-bpm-biz-activiti(opens new window) #4. 迭代计划工作流的基本功能已经开发完成，当然还是有很多功能需要进行建设。已经整理在 https://gitee.com/zhijiantianya/ruoyi-vue-pro/issues/I4UPEU (opens new window)链接中，你也可以提一些功能的想法。 如果您有参与工作流开发的想法，可以添加我的微信 wangwenbin10 ！ 艿艿会带着你做技术方案，Code Review 你的每一行代码的实现。相信在这个过程中，你会收获不错的技术成长！","categories":["工作流"]},{"title":"MYSQL","path":"/2023/11/27/MySQL/MySQL/","content":"MySQL 从上往下看，依次会分为网络连接层、系统服务层、存储引擎层、以及文件系统层，往往编写SQL后，都会遵守着MySQL的这个架构往下走。 连接层：主要是指数据库连接池，会负责处理所有客户端接入的工作。 服务层：主要包含SQL接口、解析器、优化器以及缓存缓冲区四块区域。 存储引擎层：这里是指MySQL支持的各大存储引擎，如InnoDB、MyISAM等。 文件系统层：涵盖了所有的日志，以及数据、索引文件，位于系统硬盘上。 show processlist;命令查询所有正在运行的线程 一、数据库连接池(Connection Pool)​ 连接池的最大线程数可以通过参数max-connections来控制，如果到来的客户端连接超出该值时，新到来的连接都会被拒绝，关于最大连接数的一些命令主要有两条： show variables like &#39;%max_connections%&#39;;：查询目前DB的最大连接数。 set GLOBAL max_connections = 200;：修改数据库的最大连接数为指定值。 1、系统服务层 在SQL中会分为五大类： DML：数据库操作语句，比如update、delete、insert等都属于这个分类。 DDL：数据库定义语句，比如create、alter、drop等都属于这个分类。 DQL：数据库查询语句，比如最常见的select就属于这个分类。 DCL：数据库控制语句，比如grant、revoke控制权限的语句都属于这个分类。 TCL：事务控制语句，例如commit、rollback、setpoint等语句属于这个分类。 2、缓存&amp;缓冲​ 缓冲区的设计主要是：为了通过内存的速度来弥补磁盘速度较慢对数据库造成的性能影响。在数据库中读取某页数据操作时，会先将从磁盘读到的页存放在缓冲区中，后续操作相同页的时候，可以基于内存操作。 ​ 一般来说，当你对数据库进行写操作时，都会先从缓冲区中查询是否有你要操作的页，如果有，则直接对内存中的数据页进行操作（例如修改、删除等），对缓冲区中的数据操作完成后，会直接给客户端返回成功的信息，然后MySQL会在后台利用一种名为Checkpoint的机制，将内存中更新的数据刷写到磁盘。 ​ 同时缓冲区是与存储引擎有关的，不同的存储引擎实现也不同，比如InnoDB的缓冲区叫做innodb_buffer_pool，而MyISAM则叫做key_buffer。 3、日志模块 在MySQL中主要存在七种常用的日志类型，如下： binlog二进制日志，主要记录MySQL数据库的所有写操作（增删改）。 redo-log重做&#x2F;重写日志，MySQL崩溃时，对于未落盘的操作会记录在这里面，用于重启时重新落盘（InnoDB专有的）。 undo-logs撤销&#x2F;回滚日志：记录事务开始前[修改数据]的备份，用于回滚事务。 error-log：错误日志：记录MySQL启动、运行、停止时的错误信息。 general-log常规日志，主要记录MySQL收到的每一个查询或SQL命令。 slow-log：慢查询日志，主要记录执行时间较长的SQL。 relay-log：中继日志，主要用于主从复制做数据拷贝。 二、索引1、创建索引CREATE INDEX indexName ON tableName (columnName(length) [ASC**|**DESC]); indexName：当前创建的索引，创建成功后叫啥名字。 tableName：要在哪张表上创建一个索引，这里指定表名。 columnName：要为表中的哪个字段创建索引，这里指定字段名。 length：如果字段存储的值过长，选用值的前多少个字符创建索引。 ASC|DESC：指定索引的排序方式，ASC是升序，DESC是降序，默认ASC。 ALTER TABLE tableName ADD INDEX indexName(columnName(length) [ASC|DESC]); 2、删除索引DROP INDEX indexName ON tableName; 当然，当建立了一条索引后，也可以强制性的为SELECT语句指定索引，如下： SELECT * FROM table_name FORCE INDEX(index_name) WHERE .....; 这条命令查询一个表中拥有的索引 SHOW INDEX FROM tableName; ①Table：当前索引属于那张表。 ②Non_unique：目前索引是否属于唯一索引，0代表是的，1代表不是。 ③Key_name：当前索引的名字。 ④Seq_in_index：如果当前是联合索引，目前字段在联合索引中排第几个。 ⑤Column_name：当前索引是位于哪个字段上建立的。 ⑥Collation：字段值以什么方式存储在索引中，A表示有序存储，NULL表无序。 ⑦Cardinality：当前索引的散列程度，也就是索引中存储了多少个不同的值。 ⑧Sub_part：当前索引使用了字段值的多少个字符建立，NULL表示全部。 ⑨Packed：表示索引在存储字段值时，以什么方式压缩，NULL表示未压缩， ⑩Null：当前作为索引字段的值中，是否存在NULL值，YES表示存在。 ⑪Index_type：当前索引的结构（BTREE, FULLTEXT, HASH, RTREE）。 ⑫Comment：创建索引时，是否对索引有备注信息。 3、多列索引​ 当建立多列索引后，一条SELECT语句，只有当查询条件中了包含了多列索引的第一个字段时，才能使用多列索引，下面举个栗子。 ​ 比如在用户表中，通过id、name、age三个字段建立一个多列索引 -- 无法使用多列索引的SQL语句SELECT * FROM `zz_user` WHERE name = &quot;竹子&quot; AND age = &quot;18&quot;;-- 能命中多列索引的SQL语句SELECT * FROM `zz_user` WHERE name = &quot;竹子&quot; AND id = 6; 4、全文索引​ 全文索引类似于ES、Solr搜索中间件中的分词器，或者说和之前常用的like+%模糊查询很类似，它只能创建在CHAR、VARCHAR、TEXT等这些文本类型字段上，而且使用全文索引查询时，条件字符数量必须大于3才生效。当然，还是举个栗子才有感觉： MYSQL各个索引的优劣势1、主键索引 数据表的主键，最好选用带顺序性的值，否则有可能掉入主键索引的“陷阱”中。 2、前缀索引 由于其索引节点中，未存储一个字段的完整值，所以MySQL也无法通过前缀索引来完成ORDER BY、GROUP BY等分组排序工作，同时也无法完成覆盖扫描等操作。 3、唯一索引 如果COLUMN_XX字段上建立的是唯一索引，当找到一条数据后就会立马停下检索，因此本身建立唯一索引的字段值就具备唯一性。 但插入数据时就不同了，因为要确保数据不重复，所以插入前会检查一遍表中是否存在相同的数据。但普通索引则不需要考虑这个问题，因此普通索引的数据插入会快一些。 4、哈希索引 但哈希结构的致命问题在于无序，也就是无法基于哈希索引的字段做排序、分组等工作。 建立索引时遵循的原则建立索引时，需要遵守的一些原则： ①经常频繁用作查询条件的字段应酌情考虑为其创建索引。 ②表的主外键或连表字段，必须建立索引，因为能很大程度提升连表查询的性能。 ③建立索引的字段，一般值的区分性要足够高，这样才能提高索引的检索效率。 ④建立索引的字段，值不应该过长，如果较长的字段要建立索引，可以选择前缀索引。 ⑤建立联合索引，应当遵循最左前缀原则，将多个字段之间按优先级顺序组合。 ⑥经常根据范围取值、排序、分组的字段应建立索引，因为索引有序，能加快排序时间。 ⑦对于唯一索引，如果确认不会利用该字段排序，那可以将结构改为Hash结构。 ⑧尽量使用联合索引代替单值索引，联合索引比多个单值索引查询效率要高。 同时，除开上述一些建立索引的原则外，在建立索引时还需有些注意点： ❶值经常会增删改的字段，不合适建立索引，因为每次改变后需维护索引结构。 ❷一个字段存在大量的重复值时，不适合建立索引，比如之前举例的性别字段。 ❸索引不能参与计算，因此经常带函数查询的字段，并不适合建立索引。 ❹一张表中的索引数量并不是越多越好，一般控制在3，最多不能超过5。 ❺建立联合索引时，一定要考虑优先级，查询频率最高的字段应当放首位。 ❻当表的数据较少，不应当建立索引，因为数据量不大时，维护索引反而开销更大。 ❼索引的字段值无序时，不推荐建立索引，因为会造成页分裂，尤其是主键索引。 联合索引的最左前缀原则​ 联合索引的最左前缀原则，道理很简单的，就是组成联合索引的多个列，越靠左边优先级越高，同时也只有SQL查询条件中，包含了最左的字段，才能使用联合索引，例如： -- 基于上面的哪个X、Y、Z联合索引SELECT * FROM tb WHERE Y = &quot;...&quot; AND Z = &quot;...&quot;; 不会使用联合索引 执行分析工具 - ExplainEXPLAIN SELECT * FROM `zz_users`;+----+-------------+----------+------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+----------+------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | zz_users | ALL | NULL | NULL | NULL | NULL | 3 | |+----+-------------+----------+------+---------------+------+---------+------+------+-------+ id：这是执行计划的ID值，这个值越大，表示执行的优先级越高。 select_type：当前查询语句的类型，有如下几个值： simple：简单查询。 primary：复杂查询的外层查询。 subquery：包含在查询语句中的子查询。 derived：包含在FROM中的子查询。 table：表示当前这个执行计划是基于那张表执行的。 type：当前执行计划查询的类型，有几种情况： all：表示走了全表查询，未命中索引或索引失效。 system：表示要查询的表中仅有一条数据。 const：表示当前SQL语句的查询条件中，可以命中索引查询。 range：表示当前查询操作是查某个区间。 eq_ref：表示目前在做多表关联查询。 ref：表示目前使用了普通索引查询。 index：表示目前SQL使用了辅助索引查询。 possible_keys：执行SQL时，优化器可能会选择的索引（最后执行不一定用）。 key：查询语句执行时，用到的索引名字。 key_len：这里表示索引字段使用的字节数。 ref：这里显示使用了那种查询的类型。 rows：当前查询语句可能会扫描多少行数据才能检索出结果。 Extra：这里是记录着额外的一些索引使用信息，有几种状态： using index：表示目前使用了覆盖索引查询（稍后讲）。 using where：表示使用了where子句查询，通常表示没使用索引。 using index condition：表示查询条件使用到了联合索引的前面几个字段。 using temporary：表示使用了临时表处理查询结果。 using filesort：表示以索引字段之外的方式进行排序，效率较低。 select tables optimized away：表示在索引字段上使用了聚合函数 索引失效 查询中带有OR会导致索引失效 模糊查询中like以%开头导致索引失效 字符类型查询时不带引号导致索引失效 -- 先插入一条user_name = 1111 的数据INSERT INTO `zz_users` VALUES(4,&quot;1111&quot;,&quot;男&quot;,&quot;4321&quot;,&quot;2022-09-17 23:48:29&quot;);EXPLAIN SELECT * FROM `zz_users` WHERE user_name = 111; 索引字段参与计算导致索引失效 字段被用于函数计算导致索引失效 违背最左前缀原则导致索引失效 不同字段值比对导致索引失效 EXPLAIN SELECT * FROM `zz_users` WHERE user_name = user_sex; 反向范围操作 一般来说，如果SQL属于正向范围查询，例如&gt;、&lt;、between、like、in...等操作时，索引是可以正常生效的，但如果SQL执行的是反向范围操作，例如NOT IN、NOT LIKE、IS NOT NULL、!=、&lt;&gt;...等操作时，就会出现问题，例如： EXPLAIN SELECT * FROM `zz_users` WHERE user_id NOT IN(1,2,3); MyISAM引擎的表，在磁盘中有三个文件： zz_myisam_index.frm：该文件中存储表的结构信息。 zz_myisam_index.MYD：该文件中存储表的行数据。 zz_myisam_index.MYI：该文件中存储表的索引数据。 也就是说，MyISAM引擎的表数据和索引数据，是分别放在两个不同的磁盘文件中存储的，这也意味着MyISAM引擎并不支持聚簇索引，因为聚簇索引要求表数据和索引数据一起存储在同一块空间，而MyISAM的.MYI索引文件中，存储的是表数据所在的地址指针。 InnoDB引擎的表，在磁盘中仅有两个文件： zz_innodb_index.frm：该文件中存储表的结构信息。 zz_innodb_index.ibd：该文件中存储表的行数据和索引数据。 因为InnoDB引擎中，表数据和索引数据都一起放在.ibd文件中，也就代表着索引数据和表数据是处于同一块空间存储的，这符合聚簇索引的定义，因此InnoDB支持聚簇索引。 三、事务概念Atomicity原子性指组成一个事务的一组SQL要么全部执行成功，要么全部执行失败，事务中的一组SQL会被看成一个不可分割的整体，当成一个操作看待。 Consistency一致性一致性也比较好理解，也就是不管事务发生的前后，MySQL中原本的数据变化都是一致的，也就是DB中的数据只允许从一个一致性状态变化为另一个一致性状态。这句话似乎听起来有些绕，不太好理解对嘛？简单解释一下就是：一个事务中的所有操作，要么一起改变数据库中的数据，要么都不改变，对于其他事务而言，数据的变化是一致的。 Isolation独立性&#x2F;隔离性指多个事务之间都是独立的，相当于每个事务都被装在一个箱子中，每个箱子之间都是隔开的，相互之间并不影响。 Durability持久性持久性是指一个事务一旦被提交，它会保持永久性，所更改的数据都会被写入到磁盘做持久化处理，就算MySQL宕机也不会影响数据改变，因为宕机后也可以通过日志恢复数据。 手动管理事务 start transaction | begin | begin work：开启一个事务 commit：提交一个事务 rollback：回滚一个事务 -- 开启一个事务start transaction;-- 第一条SQL语句-- 第二条SQL语句-- 第三条SQL语句-- 提交或回滚事务commit || rollback; 事务回滚点在MySQL中提供了两个关于事务回滚点的命令： savepoint point_name：添加一个事务回滚点 rollback to point_name：回滚到指定的事务回滚点 -- 先查询一次用户表SELECT * FROM `zz_users`;-- 开启事务start transaction;-- 修改 ID=4 的姓名为：黑熊update `zz_users` set `user_name` = &quot;黑熊&quot; where `user_id` = 4;-- 添加一个事务回滚点：update_namesavepoint update_name;-- 删除 ID=1 的行数据delete from `zz_users` where `user_id` = 1;-- 回滚到 update_name 这个事务点rollback to update_name;-- 再次查询一次数据SELECT * FROM `zz_users`;-- 提交事务COMMIT; MySQL事务的隔离机制在MySQL中，事务隔离机制分为了四个级别： ①Read uncommitted/RU：读未提交 ②Read committed/RC：读已提交 ③Repeatable read/RR：可重复读(MySQL默认) ④Serializable：序列化&#x2F;串行化 脏读、幻读、不可重复读问题脏读问题脏读的意思是指一个事务读到了其他事务还未提交的数据，也就是当前事务读到的数据，由于还未提交，因此有可能会回滚。 不可重复读问题不可重复读问题是指在一个事务中，多次读取同一数据，先后读取到的数据不一致。 幻读问题幻读：指同一个事务内多次查询返回的结果集不一样。比如同一个事务A，在第一次查询表的数据行数时，发现表中有n条行记录，但是第二次以同等条件查询时，却发现有n+1条记录，这就好像产生了幻觉。 发生幻读问题的原因是在于：另外一个事务在第一个事务要处理的目标数据范围之内新增了数据，然后先于第一个事务提交造成的问题。 ①读未提交 ​ 处于该隔离级别的数据库，脏读、不可重复读、幻读问题都有可能发生。 ​ 这种隔离级别是基于「写互斥锁」实现的，当一个事务开始写某一个数据时，另外一个事务也来操作同一个数据，此时为了防止出现问题则需要先获取锁资源，只有获取到锁的事务，才允许对数据进行写操作，同时获取到锁的事务具备排他性&#x2F;互斥性，也就是其他线程无法再操作这个数据。 但虽然这个级别中，写同一数据时会互斥，但读操作却并不是互斥的，也就是当一个事务在写某个数据时，就算没有提交事务，其他事务来读取该数据时，也可以读到未提交的数据，因此就会导致脏读、不可重复读、幻读一系列问题出现。 ②读已提交 ​ 处于该隔离级别的数据库，解决了脏读问题，不可重复读、幻读问题依旧存在。 ​ 在这个隔离级别中，对于写操作同样会使用「写互斥锁」，也就是两个事务操作同一事务时，会出现排他性，而对于读操作则使用了一种名为MVCC多版本并发控制的技术处理，也就是有事务中的SQL需要读取当前事务正在操作的数据时，MVCC机制不会让另一个事务读取正在修改的数据，而是读取上一次提交的数据（也就是读原本的老数据）。 ③可重复读 ​ 处于该隔离级别的数据库，解决了脏读、不可重复读问题，幻读问题依旧存在。 在可重复读级别中，则不会每次查询时都创建新的ReadView，而是在一个事务中，只有第一次执行查询会创建一个ReadView，在这个事务的生命周期内，所有的查询都会从这一个ReadView中读取数据，从而确保了一个事务中多次读取相同数据是一致的，也就是解决了不可重复读问题。 ④序列化&#x2F;串行化 ​ 处于该隔离级别的数据库，解决了脏读、不可重复读、幻读问题都不存在。 ​ 序列化意思是将所有的事务按序排队后串行化处理，也就是操作同一张表的事务只能一个一个执行，事务在执行前需要先获取表级别的锁资源，拿到锁资源的事务才能执行，其余事务则陷入阻塞，等待当前事务释放锁。 -- 方式①：查询当前数据库的隔离级别SELECT @@tx_isolation;-- 方式②：查询当前数据库的隔离级别show variables like &#x27;%tx_isolation%&#x27;;-- 设置隔离级别为RU级别（当前连接生效）set transaction isolation level read uncommitted;-- 设置隔离级别为RC级别（全局生效）set global transaction isolation level read committed;-- 设置隔离级别为RR级别（当前连接生效）-- 这里和上述的那条命令作用相同，是第二种设置的方式set tx_isolation = &#x27;repeatable-read&#x27;;-- 设置隔离级别为最高的serializable级别（全局生效）set global.tx_isolation = &#x27;serializable&#x27;; MySQL事务实现原理单条SQL的事务机制任意一条写SQL的执行都会记录三个日志：undo-log、redo-log、bin-log。 undo-log：主要记录SQL的撤销日志，比如目前是insert语句，就记录一条delete日志。 redo-log：记录当前SQL归属事务的状态，以及记录修改内容和修改页的位置。 bin-log：记录每条SQL操作日志，只要是用于数据的主从复制与数据恢复&#x2F;备份。 redo-log是一种WAL(Write-ahead logging)预写式日志，在数据发生更改之前会先记录日志，也就是在SQL执行前会先记录一条prepare状态的日志，然后再执行数据的写操作。 ​ MySQL是基于磁盘的，但磁盘的写入速度相较内存而言会较慢，因此MySQL-InnoDB引擎中不会直接将数据写入到磁盘文件中，而是会先写到BufferPool缓冲区中，当SQL被成功写入到缓冲区后，紧接着会将redo-log日志中相应的记录改为commit状态，然后再由MySQL刷盘机制去做具体的落盘操作。 -- 先记录一条状态为 prepare 的日志-- 然后执行SQL，在缓冲区中更改对应的数据INSERT INTO `zz_users` VALUES(5,&quot;黑竹&quot;,&quot;男&quot;,&quot;9999&quot;,&quot;2022-09-24 23:48:29&quot;);-- 写入缓冲区成功后，将日志记录改为 commit状态-- 返回 [Affected rows: 1]，MySQL后台线程执行刷盘动作 四、MySQL锁机制MySQL的锁机制与索引机制类似，都是由存储引擎负责实现的 以锁粒度的维度划分： ①表锁： 全局锁：加上全局锁之后，整个数据库只能允许读，不允许做任何写操作。 元数据锁 &#x2F; MDL锁：基于表的元数据加锁，加锁后整张表不允许其他事务操作。 意向锁：这个是InnoDB中为了支持多粒度的锁，为了兼容行锁、表锁而设计的。 自增锁 &#x2F; AUTO-INC锁：这个是为了提升自增ID的并发插入性能而设计的。 ②页面锁 ③行锁： 记录锁 &#x2F; Record锁：也就是行锁，一条记录和一行数据是同一个意思。 间隙锁 &#x2F; Gap锁：InnoDB中解决幻读问题的一种锁机制。 临建锁 &#x2F; Next-Key锁：间隙锁的升级版，同时具备记录锁+间隙锁的功能。 以互斥性的维度划分： 共享锁 &#x2F; S锁 Shared Lock：不同事务之间不会相互排斥、可以同时获取的锁。 排他锁 &#x2F; X锁 Exclusive Lock：不同事务之间会相互排斥、同时只能允许一个事务获取的锁。 共享排他锁 &#x2F; SX锁：MySQL5.7版本中新引入的锁，主要是解决SMO带来的问题。 以操作类型的维度划分： 读锁：查询数据时使用的锁。 写锁：执行插入、删除、修改、DDL语句时使用的锁。 以加锁方式的维度划分： 显示锁：编写SQL语句时，手动指定加锁的粒度。 隐式锁：执行SQL语句时，根据隔离级别自动为SQL操作加锁。 以思想的维度划分： 乐观锁：每次执行前认为自己会成功，因此先尝试执行，失败时再获取锁。 悲观锁：每次执行前都认为自己无法成功，因此会先获取锁，然后再执行。 共享锁 SELECT ... LOCK IN SHARE MODE;-- MySQL8.0之后也优化了写法，如下：SELECT ... FOR SHARE; 示例 -- 窗口1：-- 开启一个事务begin;-- 获取共享锁并查询 ID=1 的数据select * from `zz_users` where user_id = 1 lock in share mode; 排他锁 ​ 排他锁并不是只能用于写操作，对于一个读操作，咱们也可以手动的指定为获取排他锁，当一个事务在读数据时，获取了排他锁，那当其他事务来读、写同一数据时，都会被排斥，比如事务T1对ID=88的这条数据加了一个排他锁，此时T2来加排他锁读取这条数据，T3来修改这条数据，都会被T1排斥。 begin;SELECT ... FOR UPTATE; 表锁 一张表只能存在一个同一类型的表锁。 InnoDB是一个支持多粒度锁的存储引擎，它的锁机制是基于聚簇索引实现的，当SQL执行时，如果能在聚簇索引命中数据，则加的是行锁，如无法命中聚簇索引的数据则加的是表锁，比如： select * from `zz_users` for update; 这条SQL就无法命中聚簇索引，此时自然加的就是表级别的排他锁，但是这个表级锁，并不是真正意义上的表锁，是一个“伪表锁”，但作用是相同的，锁了整张表。 元数据锁(Meta Data Lock) 更改表结构时使用 意向锁（Intention Lock） ​ 当事务T2尝试获取一个表级锁时，就会先看一下表上是否有意向锁，如果有的话再判断一下与自身是否冲突，比如表上存在一个意向共享锁，目前T2要获取的是表级别的读锁，那自然不冲突可以获取。但反之，如果T2要获取一个表记的写锁时，就会出现冲突，T2事务则会陷入阻塞，直至T1释放了锁（事务结束）为止。 自增锁（AUTO-INC Lock） 自增锁主要负责维护并发事务下自增列的顺序 innodb_autoinc_lock_mode = 0：传统模式。 innodb_autoinc_lock_mode = 1：连续模式（MySQL8.0以前的默认模式）。 innodb_autoinc_lock_mode = 2：交错模式（MySQL8.0之后的默认模式）。 MySQL中可能出现的三种插入类型： 普通插入：指通过INSERT INTO table_name(...) VALUES(...)这种方式插入。 批量插入：指通过INSERT ... SELECT ...这种方式批量插入查询出的数据。 混合插入：指通过INSERT INTO table_name(id,...) VALUES(1,...),(NULL,...),(3,...)这种方式插入，其中一部分指定ID，一部分不指定。 行锁记录锁（Record Lock） 实际上就是行锁 -- 获取行级别的 共享锁select * from `zz_users` where user_id = 1 lock in share mode;-- 获取行级别的 排他锁select * from `zz_users` where user_id = 1 for update; 间隙锁（Gap Lock） 主要是用来解决幻读问题的 乐观锁 每次执行都认为只会有自身一条线程操作，因此无需拿锁直接执行。 在MySQL中则可以通过version版本号+CAS的形式实现乐观锁，也就是在表中多设计一个version字段，然后在SQL修改时以如下形式操作： UPDATE ... SET version = version + 1 ... WHERE ... AND version = version; 乐观锁更加适用于读大于写的业务场景，频繁写库的业务则并不适合加乐观锁 悲观锁 在每次执行前必须获取到锁，然后才能继续往下执行，而数据库中的排他锁，就是一种典型的悲观锁类型。 五、MySQL-MVCC机制 MVCC机制的全称为Multi-Version Concurrency Control，即&#x3D;&#x3D;多版本并发控制技术&#x3D;&#x3D;，主要是为了提升数据库并发性能而设计的，其中采用更好的方式处理了读-写并发冲突，做到即使有读写冲突时，也可以不加锁解决，从而确保了任何时刻的读操作都是非阻塞的。&#x3D;&#x3D;只有InnoDB实现了MVCC机制&#x3D;&#x3D; 1、MySQL-MVCC多版本并发控制 MySQL中的多版本并发控制，也和上面给出的例子类似，毕竟回想一下，脏读、不可重复读、幻读问题都是由于多个事务并发读写导致的，但这些问题都是基于最新版本的数据并发操作才会出现，那如果读、写的事务操作的不是同一个版本呢？比如写操作走新版本，读操作走老版本，这样是不是无论执行写操作的事务干了啥，都不会影响读的事务？答案是Yes。 不过要稍微记住，MySQL中仅在RC读已提交级别、RR可重复读级别才会使用MVCC机制，Why？ 因为如果是RU读未提交级别，既然都允许存在脏读问题、允许一个事务读取另一个事务未提交的数据，那自然可以直接读最新版本的数据，因此无需MVCC介入。 同时如若是Serializable串行化级别，因为会将所有的并发事务串行化处理，也就是不论事务是读操作，亦或是写操作，都会被排好队一个个执行，这都不存在所谓的多线程并发问题了，自然也无需MVCC介入。 2、MySQL-MVCC机制实现原理MVCC机制主要通过隐藏字段、Undo-log日志、ReadView这三个东西实现的。 隐藏主键-ROW_ID(6bytes) 对于InnoDB引擎的表而言，由于其表数据是按照聚簇索引的格式存储，因此通常都会选择主键作为聚簇索引列，然后基于主键字段构建索引树，但如若表中未定义主键，则会选择一个具备唯一非空属性的字段，作为聚簇索引的字段来构建树。 当两者都不存在时，InnoDB就会隐式定义一个顺序递增的列ROW_ID来作为聚簇索引列。 六、MySQL死锁​\t1、锁超时机制 show variables like &#x27;innodb_lock_wait_timeout&#x27;;+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| innodb_lock_wait_timeout | 50 |+--------------------------+-------+ 2、死锁检测算法 wait-for graph wait-for graph算法被启用后，会要求MySQL收集两个信息： 锁的信息链表：目前持有每个锁的事务是谁。 事务等待链表：阻塞的事务要等待的锁是谁。 但出现死锁问题时，MySQL会选择哪个事务回滚呢？之前分析过，当一个事务在执行SQL更改数据时，都会记录在Undo-log日志中，Undo量越小的事务，代表它对数据的更改越少，同时回滚的代价最低，因此会选择Undo量最小的事务回滚（如若两个事务的Undo量相同，会选择回滚触发死锁的事务）。 3、如何避免死锁产生？ 因为死锁的检测过程较为耗时，所以尽量不要等死锁出现后再去解除，而是尽量调整业务避免死锁的产生，一般来说可以从如下方面考虑： 合理的设计索引结构，使业务SQL在执行时能通过索引定位到具体的几行数据，减小锁的粒度。 业务允许的情况下，也可以将隔离级别调低，因为级别越低，锁的限制会越小。 调整业务SQL的逻辑顺序，较大、耗时较长的事务尽量放在特定时间去执行（如凌晨对账…）。 尽可能的拆分业务的粒度，一个业务组成的大事务，尽量拆成多个小事务，缩短一个事务持有锁的时间。 如果没有强制性要求，就尽量不要手动在事务中获取排他锁，否则会造成一些不必要的锁出现，增大产生死锁的几率。 …….. 2.1、锁的内存结构在Java中，Synchronized锁是基于Monitor实现的，而ReetrantLock又是基于AQS实现的，那MySQL的锁是基于啥实现的呢？想要搞清楚这点，得先弄明白锁的内存结构，先看图： InnoDB引擎中，每个锁对象在内存中的结构如上，其中记录的信息也比较多，先全部理清楚后再聊聊锁的实现。 2.1.1、锁的事务信息其中记录着当前的锁结构是由哪个事务生成的，记录的是指针，指向一个具体的事务。 2.1.2、索引的信息这个是行锁的特有信息，对于行锁来说，需要记录一下加锁的行数据属于哪个索引、哪个节点，记录的也是指针。 2.1.3、锁粒度信息这个略微有些复杂，对于不同粒度的锁，其中存储的信息也并不同，如果是表锁，其中就记录了一下是对哪张表加的锁，以及表的一些其他信息。 但如果锁粒度是行锁，其中记录的信息更多，有三个较为重要的： Space ID：加锁的行数据，所在的表空间ID。 Page Number：加锁的行数据，所在的页号。 n_bits：使用的比特位，对于一页数据中，加了多少个锁（后续结合讲）。 2.1.4、锁类型信息对于锁结构的类型，在内部实现了复用，采用一个32bit的type_mode来表示，这个32bit的值可以拆为lock_mode、lock_type、rec_lock_type三部分，如下： lock_mode：表示锁的模式，使用低四位。 0000/0：表示当前锁结构是共享意向锁，即IS锁。 0001/1：表示当前锁结构是排他意向锁，即IX锁。 0010/2：表示当前锁结构是共享锁，即S锁。 0011/3：表示当前锁结构是排他锁，即X锁。 0100/4：表示当前锁结构是自增锁，即AUTO-INC锁。 lock_type：表示锁的类型，使用低位中的5~8位。 LOCK_TABLE：当第5个比特位是1时，表示目前是表级锁。 LOCK_REC：当第6个比特位是1时，表示目前是行级锁。 rec_lock_type：表示行锁的具体类型，使用其余位。 LOCK_ORDINARY：当高23位全零时，表示目前是临键锁。 LOCK_GAP：当第10位是1时，表示目前是间隙锁。 LOCK_REC_NOT_GAP：当第11位是1时，表示目前是记录锁。 LOCK_INSERT_INTENTION：当第12位是1时，表示目前是插入意向锁。 .....：内部还有一些其他的锁类型，会使用其他位。 is_waiting：表示目前锁处于等待状态还是持有状态，使用低位中的第9位。 0：表示is_waiting=false，即当前锁无需阻塞等待，是持有状态。 1：表示is_waiting=true，即当前锁需要阻塞，是等待状态。","categories":["MYSQL"]},{"title":"工作流介绍","path":"/2023/11/27/activiti/activiti/","content":"概念工作流。通过计算机对业务流程自动化执行管理，主要解决的是“使在多个参与者之间按照某种预定义的规则自动进行传递文档、信息或任务的过程，从而实现某个预期的业务目标，或者促使此目标的实现”。 Activiti7介绍Activiti是一个工作流引擎，Activiti可以将业务系统中复杂的业务流程抽取出来，使用专门的建模语言BPMN2.0进行定义，业务流程按照预先定义的流程进行执行，实现了系统的流程由Activiti进行管理，减少业务系统由于流程变更进行系统升级改造的工作量，从而提高系统的健壮性，同时也减少了系统开发维护成本。 在使用activiti之前，首先需要编写activiti.cfg.xml配置文件。并且引入相关依赖。 &lt;dependencies&gt; &lt;!--activiti的核心包--&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-engine&lt;/artifactId&gt; &lt;version&gt;6.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-spring&lt;/artifactId&gt; &lt;version&gt;6.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-bpmn-model&lt;/artifactId&gt; &lt;version&gt;6.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-bpmn-converter&lt;/artifactId&gt; &lt;version&gt;6.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-json-converter&lt;/artifactId&gt; &lt;version&gt;6.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti&lt;/groupId&gt; &lt;artifactId&gt;activiti-bpmn-layout&lt;/artifactId&gt; &lt;version&gt;6.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.activiti.cloud&lt;/groupId&gt; &lt;artifactId&gt;activiti-cloud-services-api&lt;/artifactId&gt; &lt;version&gt;7-201710-EA&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.5.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.40&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.0.7.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;4.1.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-dbcp&lt;/groupId&gt; &lt;artifactId&gt;commons-dbcp&lt;/artifactId&gt; &lt;version&gt;1.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; activiti.cfg.xmlactiviti的引擎配置文件，包括：ProcessEngineConfiguration的定义、数据源定义、事务管理器等。其实就是一个Spring配置文件。 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!--dbcp连接池--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/activiti&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;maxActive&quot; value=&quot;3&quot;/&gt; &lt;property name=&quot;maxIdle&quot; value=&quot;1&quot;/&gt; &lt;/bean&gt; &lt;!--在默认方式下,bean的id固定为processEngineConfiguration--&gt; &lt;bean id=&quot;processEngineConfiguration&quot; class=&quot;org.activiti.engine.impl.cfg.StandaloneProcessEngineConfiguration&quot;&gt; &lt;!--配置数据库相关信息--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;!-- activiti数据库表处理策略 false(默认值)：检查数据库的版本和依赖库的版本，如果不匹配就抛出异常 true：构建流程引擎时，执行检查，如果需要就执行更新。如果表不存在，就创建。 create-drop：构建流程引擎时创建数据库报表，关闭流程引擎时就删除这些表。 drop-create：先删除表再创建表。 create：构建流程引擎时创建数据库表，关闭流程引擎时不删除这些表 --&gt; &lt;property name=&quot;databaseSchemaUpdate&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;asyncExecutorActivate&quot; value=&quot;false&quot;/&gt; &lt;property name=&quot;mailServerHost&quot; value=&quot;mail.my-corp.com&quot;/&gt; &lt;property name=&quot;mailServerPort&quot; value=&quot;5025&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; Activiti流程框架，在前期主要需要了解的就是数据库表的创建、流程的部署、流程的启动和各个阶段任务的完成。 流程引擎配置类流程引擎配置类（ProcessEngineConfiguration），通过 ProcessEngineConfiguration可以创建工作流引擎 ProceccEngine。 工作流引擎的创建工作流引擎的创建主要有两种方式：默认创建方式和一般创建方式 默认创建方式ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();System.out.println(processEngine); 一般创建方式//使用自定义方式创建ProcessEngineConfiguration processEngineConfiguration = ProcessEngineConfiguration.createProcessEngineConfigurationFromResource(&quot;activiti.cfg.xml&quot;);//获取流程引擎对象:通过 ProcessEngineConfiguration 创建 ProcessEngine,此时会创建数据库ProcessEngine processEngine = processEngineConfiguration.buildProcessEngine(); 当创建好工作流引擎后，对应的数据库中会自动生成25张数据库表。 ACT_GE_PROPERTY中会先展示下一次流程的ID（next.dbid），并且在下一次流程部署的时候，对下一次流程的ID进行赋值。 Activiti表说明这里以表名的前缀进行说明： Service服务接口Activiti中还有许多的Service服务接口。这些Service 是工作流引擎提供用于进行工作流部署、执行、管理的服务接口，我们可以使用这些接口操作服务对应的数据表。扩展：最全的java面试题库 Service创建方式通过ProcessEngine创建Service方式： Runtimeservice runtimeService = processEngine.getRuntimeService();RepositoryService repositoryService = processEngine.getRepositoryService();TaskService taskService = processEngine.getTaskService(); Service总览 RepositoryService Activiti 的资源管理类，提供了管理和控制流程发布包和流程定义的操作。使用工作流建模工具设计的业务流程图需要使用此service将流程定义文件的内容部署到计算机。除了部署流程定义以外，还可以查询引擎中的发布包和流程定义。 暂停或激活发布包，对应全部和特定流程定义。暂停意味着它们不能再执行任何操作了，激活是对应的反向操作。获得多种资源，像是包含在发布包里的文件，或引擎自动生成的流程图。获得流程定义的pojo版本，可以用来通过java解析流程，而不必通过xml。 Runtimeservice Activiti的流程运行管理类。可以从这个服务类中获取很多关于流程执行相关的信息 Taskservice Activiti的任务管理类。可以从这个类中获取任务的信息。 Historyservice Activiti的历史管理类，可以查询历史信息，执行流程时，引擎会保存很多数据（根据配置)，比如流程实例启动时间，任务的参与者，完成任务的时间，每个流程实例的执行路径，等等。这个服务主要通过查询功能来获得这些数据。 ManagementService Activiti的引擎管理类，提供了对Activiti流程引擎的管理和维护功能，这些功能不在工作流驱动的应用程序中使用，主要用于Activiti 系统的日常维护。 流程图符号说明 BPMN插件使用IDEA进行开发，建议下载一个插件。actiBPM插件，直接搜索下载。 流程符号、画流程图流程符号：事件Event，活动Activity，网关Gateway，流向 使用流程设计器画出流程图 创建bpmn文件，在流程设计器使用流程符号来表达流程，指定流程的key，指定任务负责人 生成png文件 创建的bpmn文件要放在resourse下的bpmn文件夹下。 注意： 当前任务流程的ID不能是数字开头。 找到本地的文件，选择notepad打开 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;&lt;definitions xmlns=&quot;http://www.omg.org/spec/BPMN/20100524/MODEL&quot; xmlns:activiti=&quot;http://activiti.org/bpmn&quot; xmlns:bpmndi=&quot;http://www.omg.org/spec/BPMN/20100524/DI&quot; xmlns:omgdc=&quot;http://www.omg.org/spec/DD/20100524/DC&quot; xmlns:omgdi=&quot;http://www.omg.org/spec/DD/20100524/DI&quot; xmlns:tns=&quot;http://www.activiti.org/test&quot; xmlns:xsd=&quot;http://www.w3.org/2001/XMLSchema&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; expressionLanguage=&quot;http://www.w3.org/1999/XPath&quot; id=&quot;m1611283406582&quot; name=&quot;&quot; targetNamespace=&quot;http://www.activiti.org/test&quot; typeLanguage=&quot;http://www.w3.org/2001/XMLSchema&quot;&gt; &lt;process id=&quot;myEvection&quot; isClosed=&quot;false&quot; isExecutable=&quot;true&quot; name=&quot;出差申请&quot; processType=&quot;None&quot;&gt; &lt;startEvent id=&quot;_2&quot; name=&quot;StartEvent&quot;/&gt; &lt;userTask activiti:assignee=&quot;zhangsan&quot; activiti:exclusive=&quot;true&quot; id=&quot;_3&quot; name=&quot;创建出差申请&quot;/&gt; &lt;userTask activiti:assignee=&quot;jerry&quot; activiti:exclusive=&quot;true&quot; id=&quot;_4&quot; name=&quot;经理审批&quot;/&gt; &lt;userTask activiti:assignee=&quot;jack&quot; activiti:exclusive=&quot;true&quot; id=&quot;_5&quot; name=&quot;总经理审批&quot;/&gt; &lt;userTask activiti:assignee=&quot;rose&quot; activiti:exclusive=&quot;true&quot; id=&quot;_6&quot; name=&quot;财务审批&quot;/&gt; &lt;endEvent id=&quot;_7&quot; name=&quot;EndEvent&quot;/&gt; &lt;sequenceFlow id=&quot;_8&quot; sourceRef=&quot;_2&quot; targetRef=&quot;_3&quot;/&gt; &lt;sequenceFlow id=&quot;_9&quot; sourceRef=&quot;_3&quot; targetRef=&quot;_4&quot;/&gt; &lt;sequenceFlow id=&quot;_10&quot; sourceRef=&quot;_4&quot; targetRef=&quot;_5&quot;/&gt; &lt;sequenceFlow id=&quot;_11&quot; sourceRef=&quot;_5&quot; targetRef=&quot;_6&quot;/&gt; &lt;sequenceFlow id=&quot;_12&quot; sourceRef=&quot;_6&quot; targetRef=&quot;_7&quot;/&gt; &lt;/process&gt; &lt;bpmndi:BPMNDiagram documentation=&quot;background=#FFFFFF;count=1;horizontalcount=1;orientation=0;width=842.4;height=1195.2;imageableWidth=832.4;imageableHeight=1185.2;imageableX=5.0;imageableY=5.0&quot; id=&quot;Diagram-_1&quot; name=&quot;New Diagram&quot;&gt; &lt;bpmndi:BPMNPlane bpmnElement=&quot;myEvection&quot;&gt; &lt;bpmndi:BPMNShape bpmnElement=&quot;_2&quot; id=&quot;Shape-_2&quot;&gt; &lt;omgdc:Bounds height=&quot;32.0&quot; width=&quot;32.0&quot; x=&quot;185.0&quot; y=&quot;0.0&quot;/&gt; &lt;bpmndi:BPMNLabel&gt; &lt;omgdc:Bounds height=&quot;32.0&quot; width=&quot;32.0&quot; x=&quot;0.0&quot; y=&quot;0.0&quot;/&gt; &lt;/bpmndi:BPMNLabel&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement=&quot;_3&quot; id=&quot;Shape-_3&quot;&gt; &lt;omgdc:Bounds height=&quot;55.0&quot; width=&quot;85.0&quot; x=&quot;160.0&quot; y=&quot;85.0&quot;/&gt; &lt;bpmndi:BPMNLabel&gt; &lt;omgdc:Bounds height=&quot;55.0&quot; width=&quot;85.0&quot; x=&quot;0.0&quot; y=&quot;0.0&quot;/&gt; &lt;/bpmndi:BPMNLabel&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement=&quot;_4&quot; id=&quot;Shape-_4&quot;&gt; &lt;omgdc:Bounds height=&quot;55.0&quot; width=&quot;85.0&quot; x=&quot;160.0&quot; y=&quot;185.0&quot;/&gt; &lt;bpmndi:BPMNLabel&gt; &lt;omgdc:Bounds height=&quot;55.0&quot; width=&quot;85.0&quot; x=&quot;0.0&quot; y=&quot;0.0&quot;/&gt; &lt;/bpmndi:BPMNLabel&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement=&quot;_5&quot; id=&quot;Shape-_5&quot;&gt; &lt;omgdc:Bounds height=&quot;55.0&quot; width=&quot;85.0&quot; x=&quot;160.0&quot; y=&quot;285.0&quot;/&gt; &lt;bpmndi:BPMNLabel&gt; &lt;omgdc:Bounds height=&quot;55.0&quot; width=&quot;85.0&quot; x=&quot;0.0&quot; y=&quot;0.0&quot;/&gt; &lt;/bpmndi:BPMNLabel&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement=&quot;_6&quot; id=&quot;Shape-_6&quot;&gt; &lt;omgdc:Bounds height=&quot;55.0&quot; width=&quot;85.0&quot; x=&quot;160.0&quot; y=&quot;390.0&quot;/&gt; &lt;bpmndi:BPMNLabel&gt; &lt;omgdc:Bounds height=&quot;55.0&quot; width=&quot;85.0&quot; x=&quot;0.0&quot; y=&quot;0.0&quot;/&gt; &lt;/bpmndi:BPMNLabel&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNShape bpmnElement=&quot;_7&quot; id=&quot;Shape-_7&quot;&gt; &lt;omgdc:Bounds height=&quot;32.0&quot; width=&quot;32.0&quot; x=&quot;185.0&quot; y=&quot;475.0&quot;/&gt; &lt;bpmndi:BPMNLabel&gt; &lt;omgdc:Bounds height=&quot;32.0&quot; width=&quot;32.0&quot; x=&quot;0.0&quot; y=&quot;0.0&quot;/&gt; &lt;/bpmndi:BPMNLabel&gt; &lt;/bpmndi:BPMNShape&gt; &lt;bpmndi:BPMNEdge bpmnElement=&quot;_12&quot; id=&quot;BPMNEdge__12&quot; sourceElement=&quot;_6&quot; targetElement=&quot;_7&quot;&gt; &lt;omgdi:waypoint x=&quot;201.0&quot; y=&quot;445.0&quot;/&gt; &lt;omgdi:waypoint x=&quot;201.0&quot; y=&quot;475.0&quot;/&gt; &lt;bpmndi:BPMNLabel&gt; &lt;omgdc:Bounds height=&quot;0.0&quot; width=&quot;0.0&quot; x=&quot;0.0&quot; y=&quot;0.0&quot;/&gt; &lt;/bpmndi:BPMNLabel&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement=&quot;_8&quot; id=&quot;BPMNEdge__8&quot; sourceElement=&quot;_2&quot; targetElement=&quot;_3&quot;&gt; &lt;omgdi:waypoint x=&quot;201.0&quot; y=&quot;32.0&quot;/&gt; &lt;omgdi:waypoint x=&quot;201.0&quot; y=&quot;85.0&quot;/&gt; &lt;bpmndi:BPMNLabel&gt; &lt;omgdc:Bounds height=&quot;0.0&quot; width=&quot;0.0&quot; x=&quot;0.0&quot; y=&quot;0.0&quot;/&gt; &lt;/bpmndi:BPMNLabel&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement=&quot;_9&quot; id=&quot;BPMNEdge__9&quot; sourceElement=&quot;_3&quot; targetElement=&quot;_4&quot;&gt; &lt;omgdi:waypoint x=&quot;202.5&quot; y=&quot;140.0&quot;/&gt; &lt;omgdi:waypoint x=&quot;202.5&quot; y=&quot;185.0&quot;/&gt; &lt;bpmndi:BPMNLabel&gt; &lt;omgdc:Bounds height=&quot;0.0&quot; width=&quot;0.0&quot; x=&quot;0.0&quot; y=&quot;0.0&quot;/&gt; &lt;/bpmndi:BPMNLabel&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement=&quot;_11&quot; id=&quot;BPMNEdge__11&quot; sourceElement=&quot;_5&quot; targetElement=&quot;_6&quot;&gt; &lt;omgdi:waypoint x=&quot;202.5&quot; y=&quot;340.0&quot;/&gt; &lt;omgdi:waypoint x=&quot;202.5&quot; y=&quot;390.0&quot;/&gt; &lt;bpmndi:BPMNLabel&gt; &lt;omgdc:Bounds height=&quot;0.0&quot; width=&quot;0.0&quot; x=&quot;0.0&quot; y=&quot;0.0&quot;/&gt; &lt;/bpmndi:BPMNLabel&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;bpmndi:BPMNEdge bpmnElement=&quot;_10&quot; id=&quot;BPMNEdge__10&quot; sourceElement=&quot;_4&quot; targetElement=&quot;_5&quot;&gt; &lt;omgdi:waypoint x=&quot;202.5&quot; y=&quot;240.0&quot;/&gt; &lt;omgdi:waypoint x=&quot;202.5&quot; y=&quot;285.0&quot;/&gt; &lt;bpmndi:BPMNLabel&gt; &lt;omgdc:Bounds height=&quot;0.0&quot; width=&quot;0.0&quot; x=&quot;0.0&quot; y=&quot;0.0&quot;/&gt; &lt;/bpmndi:BPMNLabel&gt; &lt;/bpmndi:BPMNEdge&gt; &lt;/bpmndi:BPMNPlane&gt; &lt;/bpmndi:BPMNDiagram&gt;&lt;/definitions&gt; 流程的操作部署流程使用 Activiti 提供的 API 把流程图的内容写入到数据库中 属于资源操作类，使用 RepositoryService 单文件部署：把bpmn文件和png文件逐个处理 压缩包部署：把bpmn文件和png文件打成压缩包来处理 部署操作表：act_re_deployment、act_re_procdef、act_ge_bytearray /** * 流程部署 */public void deployment() &#123; // 创建 ProcessEngine ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 获取 RepositoryService RepositoryService repositoryService = processEngine.getRepositoryService(); // 使用 service 进行流程的部署,定义一个流程的名字,把bpmn和png部署到数据中 Deployment deployment = repositoryService.createDeployment() .name(&quot;出差申请流程&quot;) //流程图标的名字 .addClasspathResource(&quot;bpmn/evection.bpmn&quot;) //bpmn文件 .addClasspathResource(&quot;bpmn/evection.png&quot;) //bpmn文件生成的图片 .deploy(); // 输出部署信息 System.out.println(&quot;流程部署ID:&quot; + deployment.getId()); System.out.println(&quot;流程部署名字:&quot; + deployment.getName());&#125; 有时候我们会有多个流程，需要创建多个bpmn流程文件，这个时候想要同时部署，我们可以对bpmn文件进行打包压缩，使用Zip包进行批量的部署 /** * 使用Zip包进行批量的部署 */@Testpublic void deployProcessByZip() &#123; // 获取流程引擎 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 获取 RepositoryService RepositoryService repositoryService = processEngine.getRepositoryService(); // 流程部署 // 读取资源包文件，构造成 InputStream InputStream inputStream = this.getClass().getClassLoader().getResourceAsStream(&quot;bpmn/evection.zip&quot;); // 使用 InputStream 构造 ZipInputStream ZipInputStream zipInputStream = new ZipInputStream(inputStream); // 使用压缩包的流，进行流程的部署 Deployment deploy = repositoryService.createDeployment() .addZipInputStream(zipInputStream) .deploy(); // 输出 System.out.println(&quot;流程部署的ID：&quot; + deploy.getId()); System.out.println(&quot;流程部署的名称：&quot; + deploy.getName());&#125; 操作的数据库表： act_ge_bytearray act_ge_property act_re_deployment act_re_procdef 启动流程实例流程部署完成以后，需要启动流程实例。使用 RuntimeService 根据流程定义的 key进行启动。 核心代码： /** * 启动流程 */public void starProcess() &#123; // 创建 ProcessEngine ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 获取 RunTimeService RuntimeService runtimeService = processEngine.getRuntimeService(); // 根据流程定义的ID启动流程 ProcessInstance instance = runtimeService.startProcessInstanceByKey(&quot;myEvection&quot;); // 输出内容 System.out.println(&quot;流程定义ID:&quot; + instance.getProcessDefinitionId()); System.out.println(&quot;流程实例的ID:&quot; + instance.getId()); System.out.println(&quot;当前活动的ID:&quot; + instance.getActivityId());&#125; 任务查询使用 TaskService ，根据流程定义的 key ，任务负责人来进行查询 核心代码： /** * 查询个人待执行的任务 */@Testpublic void findPersonalTaskList() &#123; // 获取流程引擎 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 获取TaskService TaskService taskService = processEngine.getTaskService(); // 根据流程的key和任务的负责人去查询任务 List&lt;Task&gt; taskList = taskService.createTaskQuery() .processDefinitionKey(&quot;myEvection&quot;) // 流程的key .includeProcessVariables() .taskAssignee(&quot;zhangsan&quot;) // 要查询的负责人 .list(); // 输出 for (Task task : taskList) &#123; System.out.println(&quot;流程实例的ID：&quot; + task.getProcessInstanceId()); System.out.println(&quot;任务的ID：&quot; + task.getId()); System.out.println(&quot;任务的负责人：&quot; + task.getAssignee()); System.out.println(&quot;任务的名称：&quot; + task.getName()); &#125;&#125; 任务完成使用 TaskService ，用任务 ID 直接完成任务。 核心代码： /** * 完成个人任务 */@Testpublic void completTask() &#123; String key = &quot;testCandidiate&quot;; String assignee = &quot;张三1&quot;; //任务的负责人 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .processDefinitionKey(key) .taskAssignee(assignee) .singleResult(); if (task != null) &#123; taskService.complete(task.getId()); &#125;&#125; 关于流程实例的挂起和激活全部流程实例的挂起和激活 /** * 全部流程实例的挂起和激活 */@Testpublic void suspendAllProcessInstance() &#123; // 1.获取流程引擎 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 2.获取 RepositoryService RepositoryService repositoryService = processEngine.getRepositoryService(); // 3.查询流程定义 ProcessDefinition processDefinition = repositoryService.createProcessDefinitionQuery() .processDefinitionKey(&quot;myEvection&quot;) .singleResult(); // 4.获取当前流程定义的实例是否都是挂起状态 boolean flag = processDefinition.isSuspended(); // 5.获取流程定义的ID String id = processDefinition.getId(); // 6.判断是否挂起状态。是:改为激活;否:改为挂起 if (flag) &#123; // 改为激活. 参数1:流程定义的ID,参数2:是否激活,参数3:激活时间 repositoryService.activateProcessDefinitionById(id, true, null); System.out.println(&quot;流程定义ID：&quot; + id + &quot;已激活&quot;); &#125; else &#123; // 改为挂起. 参数1:流程定义的ID;参数2:是否挂起;参数3:挂起时间 repositoryService.suspendProcessDefinitionById(id, true, null); System.out.println(&quot;流程定义ID：&quot; + id + &quot;已挂起&quot;); &#125;&#125; 单个流程实例的挂起和激活 /** * 单个流程实例的挂起和激活 */@Testpublic void suspendSingleProcessInstance() &#123; // 1.获取流程引擎 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); // 2.获取 RuntimeService RuntimeService runtimeService = processEngine.getRuntimeService(); // 3.通过 RuntimeService 获取流程实例对象 ProcessInstance instance = runtimeService.createProcessInstanceQuery() .processInstanceId(&quot;17501&quot;) .singleResult(); // 4.得到当前流程实例的暂停状态 boolean flag = instance.isSuspended(); // 5.获取流程实例的ID String instanceId = instance.getId(); // 6.判断是否暂停。是:改为激活;否:改为暂停 if (flag) &#123; runtimeService.activateProcessInstanceById(instanceId); System.out.println(&quot;流程实例ID：&quot; + instanceId + &quot;已激活&quot;); &#125; else &#123; runtimeService.suspendProcessInstanceById(instanceId); System.out.println(&quot;流程实例ID：&quot; + instanceId + &quot;已暂停&quot;); &#125;&#125; 注意： 流程实例在挂起的状态下是无法进行下一步操作的。扩展：最全的java面试题库 流程变量我们在使用流程变量的时候。如果我们将一个对象存储到一个流程变量中，那么这个对象需要实现Serializable接口。 /** * 出差申请中的流程变量对象 */@NoArgsConstructor@AllArgsConstructor@Datapublic class Evection implements Serializable &#123; private Long id; //主键ID private Integer days; //出差天数 private String evectionName; //出差单名字 private Date startTime; //出差开始时间 private Date endTime; //出差结束时间 private String address; //目的地 private String reason; //出差原因&#125; 流程变量的作用域 整个流程实例、任务、执行实例。 默认：整个流程实例。 使用方法在属性上使用UEL表达式 $&#123;assignee&#125;，assignee就是一个流程变量的名称。 在连线上使用UEL表达式 $&#123;days&lt;=3&#125;，days就是一个流程变量名称，返回结果为true或者false。 Activiti有很多种方式设置流程变量，这里简单介绍两种： 启动流程时设置流程变量 /** * 启动流程 */@Testpublic void startProcess() &#123; ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); RuntimeService runtimeService = processEngine.getRuntimeService(); // 流程变量map Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); // 设置流程变量 Evection evection = new Evection(); evection.setDays(2); // 把流程变量的pojo放入map map.put(&quot;evection&quot;, evection); map.put(&quot;assignee0&quot;, &quot;张三&quot;); map.put(&quot;assignee1&quot;, &quot;李经理&quot;); map.put(&quot;assignee2&quot;, &quot;王财务&quot;); map.put(&quot;assignee3&quot;, &quot;赵总经理&quot;); runtimeService.startProcessInstanceByKey(&quot;myProcess_1&quot;, map);&#125; 任务办理时设置 /** * 完成任务 */@Testpublic void completTask() &#123; ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Evection evection = new Evection(); evection.setDays(2); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;evection&quot;, evection); Task task = taskService.createTaskQuery() .processDefinitionKey(&quot;myProcess_2&quot;) .taskAssignee(&quot;王财务0&quot;) .singleResult(); if (task != null) &#123; String taskId = task.getId(); // 完成任务 taskService.complete(taskId, map); &#125;&#125; 网关用来控制流程的走向 排他网关——ExclusiveGateway用来在流程中实现决策，当流程执行到这个网关，所有的分支都会判断条件是否为true，如果为true，则执行该分支。 注意： 排他网关只会选择一个作为true的分支执行，如果有两个分支都为true，排他网关会选择ID值比较小的一条分支去执行。 如果从排他网关出去的流程所有的条件都不满足，则会抛出异常。扩展：最全的java面试题库 并行网关——ParallelGateway并行网关，允许流程分成多条分支，也可以把多分支汇聚到一起，并行网关的功能是基于进入和外出顺序流的： fork分支：并行后的所有外出顺序流，为每个顺序流都创建一个并发分支 join汇聚：所有到达并行网关，在此等待的分支，直到所有进入顺序流的分支都到达以后，流程就会通过汇聚网关。 注意： 如果同一个并行网关有多个进入和多个外出顺序流，它就同时具有分支和汇聚功能，这时，网关会先汇聚所有进入的顺序流，然后再切分成多个并行分支。 与其他网关的主要区别是：并行网关不会解析条件，即使顺序流中定义了条件，也会被忽略。 并行网关需要所有分支的全部运行完了，才会汇聚，继续向下执行。 包含网关——InclusiveGateway包含网关可以看成是排他网关和并行网关的结合体，和排他网关一样，可以在外出顺序流上定义条件，包含网关会解析它们，但是主要的区别是：包含网关可以选择多于一条顺序流，这和并行网关一样。 包含网关的功能是基于进入和外出顺序流的。 分支：所有外出顺序流的条件都会被解析，结果为true的顺序流会以并行方式继续执行，会为每一个顺序流创建一个分支。 汇聚：所有并行分支到达包含网关，会进入等待状态，直到每个包含流程token的进入顺序流的分支都到达。这是和并行网关最大的不同。 事件网关——EventGatewayActiviti和Spring的整合开发配置文件： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt; &lt;!--工作流引擎配置对象--&gt; &lt;bean id=&quot;processEngineConfiguration&quot; class=&quot;org.activiti.spring.SpringProcessEngineConfiguration&quot;&gt; &lt;!--数据源--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;!--使用Spring的事务管理器--&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot;/&gt; &lt;!-- 数据库策略: false:默认值。activiti在启动时，会对比数据库表中保存的版本。如果没有表或者版本不匹配，将抛出 异常。 true:activiti会对数据库中所有表进行更新操作，如果表不存在，则会自动创建。 create_drop:在activiti启动时创建表，在关闭时删除表（必须手动关闭引擎，才能删除表）。 drop-create:在activiti启动时删除原来的旧表，然后再创建新表（不需要手动关闭引擎）。 --&gt; &lt;property name=&quot;databaseSchemaUpdate&quot; value=&quot;true&quot;/&gt; &lt;/bean&gt; &lt;!--配置数据源--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/actspring&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;maxActive&quot; value=&quot;3&quot;/&gt; &lt;property name=&quot;maxIdle&quot; value=&quot;1&quot;/&gt; &lt;/bean&gt; &lt;!-- 流程引擎对象 --&gt; &lt;bean id=&quot;processEngine&quot; class=&quot;org.activiti.spring.ProcessEngineFactoryBean&quot;&gt; &lt;property name=&quot;processEngineConfiguration&quot; ref=&quot;processEngineConfiguration&quot;/&gt; &lt;/bean&gt; &lt;!--资源服务--&gt; &lt;bean id=&quot;repositoryService&quot; factory-bean=&quot;processEngine&quot; factory-method=&quot;getRepositoryService&quot;/&gt; &lt;!--流程管理--&gt; &lt;bean id=&quot;runtimeService&quot; factory-bean=&quot;processEngine&quot; factory-method=&quot;getRuntimeService&quot;/&gt; &lt;!--任务管理--&gt; &lt;bean id=&quot;taskService&quot; factory-bean=&quot;processEngine&quot; factory-method=&quot;getTaskService&quot;/&gt; &lt;!--历史管理--&gt; &lt;bean id=&quot;historyService&quot; factory-bean=&quot;processEngine&quot; factory-method=&quot;getHistoryService&quot;/&gt; &lt;!--事务管理器--&gt; &lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!--通知--&gt; &lt;tx:advice id=&quot;txAdvice&quot; transaction-manager=&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;!--传播行为--&gt; &lt;tx:method name=&quot;save*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;insert*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;delete*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;update*&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;tx:method name=&quot;find*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot;/&gt; &lt;tx:method name=&quot;get*&quot; propagation=&quot;SUPPORTS&quot; read-only=&quot;true&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt;&lt;/beans&gt; Activiti和SpringBoot的整合开发配置文件： spring: application: name: actspringboot datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/actspring?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&amp;autoReconnect=true&amp;serverTimezone=UTC username: root password: root activiti: # false:默认值。activiti在启动时，会对比数据库表中保存的版本。如果没有表或者版本不匹配，将抛出异常 # true:activiti会对数据库中所有表进行更新操作，如果表不存在，则会自动创建 # create_drop:在activiti启动时创建表，在关闭时删除表（必须手动关闭引擎，才能删除表） # drop-create:在activiti启动时删除原来的旧表，然后再创建新表（不需要手动关闭引擎） # 线上一般使用false,开发中使用true database-schema-update: true # 自动部署验证设置:true-开启（默认）、false-关闭 check-process-definitions: false # 开启历史表 db-history-used: true # 历史记录存储等级 history-level: fullserver: port: 8082","categories":["工作流"]},{"title":"一、IoC 之 Spring 统一资源加载策略","path":"/2023/11/27/Spring/一、IoC 之 Spring 统一资源加载策略/","content":"一、IoC 之 Spring 统一资源加载策略本文主要基于 Spring 5.0.6.RELEASE 摘要: 原创出处 http://cmsblogs.com/?p=2656 「小明哥」，谢谢！ 在学 Java SE 的时候，我们学习了一个标准类 java.net.URL，该类在 Java SE 中的定位为统一资源定位器（Uniform Resource Locator），但是我们知道它的实现基本只限于网络形式发布的资源的查找和定位。然而，实际上资源的定义比较广泛，除了网络形式的资源，还有以二进制形式存在的、以文件形式存在的、以字节流形式存在的等等。而且它可以存在于任何场所，比如网络、文件系统、应用程序中。所以 java.net.URL 的局限性迫使 Spring 必须实现自己的资源加载策略，该资源加载策略需要满足如下要求： 职能划分清楚。资源的定义和资源的加载应该要有一个清晰的界限； 统一的抽象。统一的资源定义和资源加载策略。资源加载后要返回统一的抽象给客户端，客户端要对资源进行怎样的处理，应该由抽象资源接口来界定。 1. 统一资源：Resourceorg.springframework.core.io.Resource 为 Spring 框架所有资源的抽象和访问接口，它继承 org.springframework.core.io.InputStreamSource接口。作为所有资源的统一抽象，Resource 定义了一些通用的方法，由子类 AbstractResource 提供统一的默认实现。定义如下： public interface Resource extends InputStreamSource &#123;\t/** * 资源是否存在 */\tboolean exists();\t/** * 资源是否可读 */\tdefault boolean isReadable() &#123; return true;\t&#125;\t/** * 资源所代表的句柄是否被一个 stream 打开了 */\tdefault boolean isOpen() &#123; return false;\t&#125;\t/** * 是否为 File */\tdefault boolean isFile() &#123; return false;\t&#125;\t/** * 返回资源的 URL 的句柄 */\tURL getURL() throws IOException;\t/** * 返回资源的 URI 的句柄 */\tURI getURI() throws IOException;\t/** * 返回资源的 File 的句柄 */\tFile getFile() throws IOException;\t/** * 返回 ReadableByteChannel */\tdefault ReadableByteChannel readableChannel() throws IOException &#123; return java.nio.channels.Channels.newChannel(getInputStream());\t&#125;\t/** * 资源内容的长度 */\tlong contentLength() throws IOException;\t/** * 资源最后的修改时间 */\tlong lastModified() throws IOException;\t/** * 根据资源的相对路径创建新资源 */\tResource createRelative(String relativePath) throws IOException;\t/** * 资源的文件名 */\t@Nullable\tString getFilename();\t/** * 资源的描述 */\tString getDescription();&#125; 1.1 子类结构类结构图如下： 从上图可以看到，Resource 根据资源的不同类型提供不同的具体实现，如下： FileSystemResource ：对 java.io.File 类型资源的封装，只要是跟 File 打交道的，基本上与 FileSystemResource 也可以打交道。支持文件和 URL 的形式，实现 WritableResource 接口，且从 Spring Framework 5.0 开始，FileSystemResource 使用 NIO2 API进行读&#x2F;写交互。 ByteArrayResource ：对字节数组提供的数据的封装。如果通过 InputStream 形式访问该类型的资源，该实现会根据字节数组的数据构造一个相应的 ByteArrayInputStream。 UrlResource ：对 java.net.URL类型资源的封装。内部委派 URL 进行具体的资源操作。 ClassPathResource ：class path 类型资源的实现。使用给定的 ClassLoader 或者给定的 Class 来加载资源。 InputStreamResource ：将给定的 InputStream 作为一种资源的 Resource 的实现类。 1.2 AbstractResourceorg.springframework.core.io.AbstractResource ，为 Resource 接口的默认抽象实现。它实现了 Resource 接口的大部分的公共实现，作为 Resource 接口中的重中之重，其定义如下： public abstract class AbstractResource implements Resource &#123;\t/** * 判断文件是否存在，若判断过程产生异常（因为会调用SecurityManager来判断），就关闭对应的流 */\t@Override\tpublic boolean exists() &#123; try &#123; // 基于 File 进行判断 return getFile().exists(); &#125; catch (IOException ex) &#123; // Fall back to stream existence: can we open the stream? // 基于 InputStream 进行判断 try &#123; InputStream is = getInputStream(); is.close(); return true; &#125; catch (Throwable isEx) &#123; return false; &#125; &#125;\t&#125;\t/** * 直接返回true，表示可读 */\t@Override\tpublic boolean isReadable() &#123; return true;\t&#125;\t/** * 直接返回 false，表示未被打开 */\t@Override\tpublic boolean isOpen() &#123; return false;\t&#125;\t/** * 直接返回false，表示不为 File */\t@Override\tpublic boolean isFile() &#123; return false;\t&#125;\t/** * 抛出 FileNotFoundException 异常，交给子类实现 */\t@Override\tpublic URL getURL() throws IOException &#123; throw new FileNotFoundException(getDescription() + &quot; cannot be resolved to URL&quot;);\t&#125;\t/** * 基于 getURL() 返回的 URL 构建 URI */\t@Override\tpublic URI getURI() throws IOException &#123; URL url = getURL(); try &#123; return ResourceUtils.toURI(url); &#125; catch (URISyntaxException ex) &#123; throw new NestedIOException(&quot;Invalid URI [&quot; + url + &quot;]&quot;, ex); &#125;\t&#125;\t/** * 抛出 FileNotFoundException 异常，交给子类实现 */\t@Override\tpublic File getFile() throws IOException &#123; throw new FileNotFoundException(getDescription() + &quot; cannot be resolved to absolute file path&quot;);\t&#125;\t/** * 根据 getInputStream() 的返回结果构建 ReadableByteChannel */\t@Override\tpublic ReadableByteChannel readableChannel() throws IOException &#123; return Channels.newChannel(getInputStream());\t&#125;\t/** * 获取资源的长度 * * 这个资源内容长度实际就是资源的字节长度，通过全部读取一遍来判断 */\t@Override\tpublic long contentLength() throws IOException &#123; InputStream is = getInputStream(); try &#123; long size = 0; byte[] buf = new byte[255]; // 每次最多读取 255 字节 int read; while ((read = is.read(buf)) != -1) &#123; size += read; &#125; return size; &#125; finally &#123; try &#123; is.close(); &#125; catch (IOException ex) &#123; &#125; &#125;\t&#125;\t/** * 返回资源最后的修改时间 */\t@Override\tpublic long lastModified() throws IOException &#123; long lastModified = getFileForLastModifiedCheck().lastModified(); if (lastModified == 0L) &#123; throw new FileNotFoundException(getDescription() + &quot; cannot be resolved in the file system for resolving its last-modified timestamp&quot;); &#125; return lastModified;\t&#125;\tprotected File getFileForLastModifiedCheck() throws IOException &#123; return getFile();\t&#125;\t/** * 抛出 FileNotFoundException 异常，交给子类实现 */\t@Override\tpublic Resource createRelative(String relativePath) throws IOException &#123; throw new FileNotFoundException(&quot;Cannot create a relative resource for &quot; + getDescription());\t&#125;\t/** * 获取资源名称，默认返回 null ，交给子类实现 */\t@Override\t@Nullable\tpublic String getFilename() &#123; return null;\t&#125;\t/** * 返回资源的描述 */\t@Override\tpublic String toString() &#123; return getDescription();\t&#125;\t@Override\tpublic boolean equals(Object obj) &#123; return (obj == this || (obj instanceof Resource &amp;&amp; ((Resource) obj).getDescription().equals(getDescription())));\t&#125;\t@Override\tpublic int hashCode() &#123; return getDescription().hashCode();\t&#125;&#125; 如果我们想要实现自定义的 Resource ，记住不要实现 Resource 接口，而应该继承 AbstractResource 抽象类，然后根据当前的具体资源特性覆盖相应的方法即可。 1.3 其他子类Resource 的子类，例如 FileSystemResource、ByteArrayResource 等等的代码非常简单。感兴趣的胖友，自己去研究。 2. 统一资源定位：ResourceLoader一开始就说了 Spring 将资源的定义和资源的加载区分开了，Resource 定义了统一的资源，那资源的加载则由 ResourceLoader 来统一定义。 org.springframework.core.io.ResourceLoader 为 Spring 资源加载的统一抽象，具体的资源加载则由相应的实现类来完成，所以我们可以将 ResourceLoader 称作为统一资源定位器。其定义如下： FROM 《Spring 源码深度解析》P16 页 ResourceLoader，定义资源加载器，主要应用于根据给定的资源文件地址，返回对应的 Resource 。 public interface ResourceLoader &#123;\tString CLASSPATH_URL_PREFIX = ResourceUtils.CLASSPATH_URL_PREFIX; // CLASSPATH URL 前缀。默认为：&quot;classpath:&quot;\tResource getResource(String location);\tClassLoader getClassLoader();&#125; #getResource(String location) 方法，根据所提供资源的路径 location 返回 Resource 实例，但是它不确保该 Resource 一定存在，需要调用**Resource#exist()** 方法来判断。 - 该方法支持以下模式的资源加载： - URL位置资源，如 `&quot;file:C:/test.dat&quot;` 。 - ClassPath位置资源，如 `&quot;classpath:test.dat` 。 - 相对路径资源，如 `&quot;WEB-INF/test.dat&quot;` ，此时返回的Resource 实例，根据实现不同而不同。 - 该方法的主要实现是在其子类 DefaultResourceLoader 中实现，具体过程我们在分析 DefaultResourceLoader 时做详细说明。- `#getClassLoader()` 方法，返回 ClassLoader 实例，对于想要获取 ResourceLoader 使用的 ClassLoader 用户来说，可以直接调用该方法来获取。在分析 Resource 时，提到了一个类 ClassPathResource ，这个类是可以根据指定的 ClassLoader 来加载资源的。## 2.1 子类结构作为 Spring 统一的资源加载器，它提供了统一的抽象，具体的实现则由相应的子类来负责实现，其类的类结构图如下：&lt;img src=&quot;IoC 之 Spring 统一资源加载策略/2446cc9fba90605b691ea250cf340ebb.png&quot; alt=&quot;ResourceLoader 类图&quot; style=&quot;zoom:67%;&quot; /&gt;## 2.1 DefaultResourceLoader与 AbstractResource 相似，`org.springframework.core.io.DefaultResourceLoader` 是 ResourceLoader 的默认实现。### 2.1.1 构造函数它接收 ClassLoader 作为构造函数的参数，或者使用不带参数的构造函数。- 在使用**不带**参数的构造函数时，使用的 ClassLoader 为默认的 ClassLoader（一般 `Thread.currentThread()#getContextClassLoader()` ）。- 在使用**带**参数的构造函数时，可以通过 `ClassUtils#getDefaultClassLoader()`获取。代码如下：```java@Nullableprivate ClassLoader classLoader;public DefaultResourceLoader() &#123; // 无参构造函数\tthis.classLoader = ClassUtils.getDefaultClassLoader();&#125;public DefaultResourceLoader(@Nullable ClassLoader classLoader) &#123; // 带 ClassLoader 参数的构造函数\tthis.classLoader = classLoader;&#125;public void setClassLoader(@Nullable ClassLoader classLoader) &#123;\tthis.classLoader = classLoader;&#125;@Override@Nullablepublic ClassLoader getClassLoader() &#123;\treturn (this.classLoader != null ? this.classLoader : ClassUtils.getDefaultClassLoader());&#125; 另外，也可以调用 #setClassLoader() 方法进行后续设置。 2.1.2 getResource 方法ResourceLoader 中最核心的方法为 #getResource(String location) ，它根据提供的 location 返回相应的 Resource 。而 DefaultResourceLoader 对该方法提供了核心实现（因为，它的两个子类都没有提供覆盖该方法，所以可以断定 ResourceLoader 的资源加载策略就封装在 DefaultResourceLoader 中)，代码如下： // DefaultResourceLoader.java@Overridepublic Resource getResource(String location) &#123; Assert.notNull(location, &quot;Location must not be null&quot;); // 首先，通过 ProtocolResolver 来加载资源 for (ProtocolResolver protocolResolver : this.protocolResolvers) &#123; Resource resource = protocolResolver.resolve(location, this); if (resource != null) &#123; return resource; &#125; &#125; // 其次，以 / 开头，返回 ClassPathContextResource 类型的资源 if (location.startsWith(&quot;/&quot;)) &#123; return getResourceByPath(location); // 再次，以 classpath: 开头，返回 ClassPathResource 类型的资源 &#125; else if (location.startsWith(CLASSPATH_URL_PREFIX)) &#123; return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()), getClassLoader()); // 然后，根据是否为文件 URL ，是则返回 FileUrlResource 类型的资源，否则返回 UrlResource 类型的资源 &#125; else &#123; try &#123; // Try to parse the location as a URL... URL url = new URL(location); return (ResourceUtils.isFileURL(url) ? new FileUrlResource(url) : new UrlResource(url)); &#125; catch (MalformedURLException ex) &#123; // 最后，返回 ClassPathContextResource 类型的资源 // No URL -&gt; resolve as resource path. return getResourceByPath(location); &#125; &#125;&#125; 首先，通过 ProtocolResolver 来加载资源，成功返回 Resource 。 其次，若 location 以 &quot;/&quot; 开头，则调用 #getResourceByPath() 方法，构造 ClassPathContextResource 类型资源并返回。代码如下： protected Resource getResourceByPath(String path) &#123;\treturn new ClassPathContextResource(path, getClassLoader());&#125; 再次，若 location 以 &quot;classpath:&quot; 开头，则构造 ClassPathResource 类型资源并返回。在构造该资源时，通过 #getClassLoader() 获取当前的 ClassLoader。 然后，构造 URL ，尝试通过它进行资源定位，若没有抛出 MalformedURLException 异常，则判断是否为 FileURL , 如果是则构造 FileUrlResource 类型的资源，否则构造 UrlResource 类型的资源。 最后，若在加载过程中抛出 MalformedURLException 异常，则委派 #getResourceByPath() 方法，实现资源定位加载。😈 实际上，和【其次】相同落。 2.1.3 ProtocolResolverorg.springframework.core.io.ProtocolResolver ，用户自定义协议资源解决策略，作为 DefaultResourceLoader 的 SPI：它允许用户自定义资源加载协议，而不需要继承 ResourceLoader 的子类。在介绍 Resource 时，提到如果要实现自定义 Resource，我们只需要继承 AbstractResource 即可，但是有了 ProtocolResolver 后，我们不需要直接继承 DefaultResourceLoader，改为实现 ProtocolResolver 接口也可以实现自定义的 ResourceLoader。 ProtocolResolver 接口，仅有一个方法 Resource resolve(String location, ResourceLoader resourceLoader) 。代码如下： /** * 使用指定的 ResourceLoader ，解析指定的 location 。 * 若成功，则返回对应的 Resource 。 * * Resolve the given location against the given resource loader * if this implementation&#x27;s protocol matches. * @param location the user-specified resource location 资源路径 * @param resourceLoader the associated resource loader 指定的加载器 ResourceLoader * @return a corresponding &#123;@code Resource&#125; handle if the given location * matches this resolver&#x27;s protocol, or &#123;@code null&#125; otherwise 返回为相应的 Resource */@NullableResource resolve(String location, ResourceLoader resourceLoader); 在 Spring 中你会发现该接口并没有实现类，它需要用户自定义，自定义的 Resolver 如何加入 Spring 体系呢？调用 DefaultResourceLoader#addProtocolResolver(ProtocolResolver) 方法即可。代码如下： /** * ProtocolResolver 集合 */private final Set&lt;ProtocolResolver&gt; protocolResolvers = new LinkedHashSet&lt;&gt;(4);public void addProtocolResolver(ProtocolResolver resolver) &#123;\tAssert.notNull(resolver, &quot;ProtocolResolver must not be null&quot;);\tthis.protocolResolvers.add(resolver);&#125; 2.1.4 示例下面示例是演示 DefaultResourceLoader 加载资源的具体策略，代码如下（该示例参考《Spring 揭秘》 P89）： ResourceLoader resourceLoader = new DefaultResourceLoader();Resource fileResource1 = resourceLoader.getResource(&quot;D:/Users/chenming673/Documents/spark.txt&quot;);System.out.println(&quot;fileResource1 is FileSystemResource:&quot; + (fileResource1 instanceof FileSystemResource));Resource fileResource2 = resourceLoader.getResource(&quot;/Users/chenming673/Documents/spark.txt&quot;);System.out.println(&quot;fileResource2 is ClassPathResource:&quot; + (fileResource2 instanceof ClassPathResource));Resource urlResource1 = resourceLoader.getResource(&quot;file:/Users/chenming673/Documents/spark.txt&quot;);System.out.println(&quot;urlResource1 is UrlResource:&quot; + (urlResource1 instanceof UrlResource));Resource urlResource2 = resourceLoader.getResource(&quot;http://www.baidu.com&quot;);System.out.println(&quot;urlResource1 is urlResource:&quot; + (urlResource2 instanceof UrlResource)); 运行结果： fileResource1 is FileSystemResource:falsefileResource2 is ClassPathResource:trueurlResource1 is UrlResource:trueurlResource1 is urlResource:true 其实对于 fileResource1 ，我们更加希望是 FileSystemResource 资源类型。但是，事与愿违，它是 ClassPathResource 类型。为什么呢？在 DefaultResourceLoader#getResource() 方法的资源加载策略中，我们知道 &quot;D:/Users/chenming673/Documents/spark.txt&quot; 地址，其实在该方法中没有相应的资源类型，那么它就会在抛出 MalformedURLException 异常时，通过 DefaultResourceLoader#getResourceByPath(...) 方法，构造一个 ClassPathResource 类型的资源。 而 urlResource1 和 urlResource2 ，指定有协议前缀的资源路径，则通过 URL 就可以定义，所以返回的都是 UrlResource 类型。 2.2 FileSystemResourceLoader从上面的示例，我们看到，其实 DefaultResourceLoader 对#getResourceByPath(String) 方法处理其实不是很恰当，这个时候我们可以使用 org.springframework.core.io.FileSystemResourceLoader 。它继承 DefaultResourceLoader ，且覆写了 #getResourceByPath(String) 方法，使之从文件系统加载资源并以 FileSystemResource 类型返回，这样我们就可以得到想要的资源类型。代码如下： @Overrideprotected Resource getResourceByPath(String path) &#123;\t// 截取首 /\tif (path.startsWith(&quot;/&quot;)) &#123; path = path.substring(1);\t&#125;\t// 创建 FileSystemContextResource 类型的资源\treturn new FileSystemContextResource(path);&#125; 2.2.1 FileSystemContextResourceFileSystemContextResource ，为 FileSystemResourceLoader 的内部类，它继承 FileSystemResource 类，实现 ContextResource 接口。代码如下： /** * FileSystemResource that explicitly expresses a context-relative path * through implementing the ContextResource interface. */private static class FileSystemContextResource extends FileSystemResource implements ContextResource &#123;\tpublic FileSystemContextResource(String path) &#123; super(path);\t&#125;\t@Override\tpublic String getPathWithinContext() &#123; return getPath();\t&#125;&#125; 在构造器中，也是调用 FileSystemResource 的构造函数来构造 FileSystemResource 的。 为什么要有 FileSystemContextResource 类的原因是，实现 ContextResource 接口，并实现对应的 #getPathWithinContext() 接口方法。 2.2.2 示例😈 在回过头看 「2.1.4 示例」 ，如果将 DefaultResourceLoader 改为 FileSystemResourceLoader ，则 fileResource1 则为 FileSystemResource 类型的资源。 2.3 ClassRelativeResourceLoaderorg.springframework.core.io.ClassRelativeResourceLoader ，是 DefaultResourceLoader 的另一个子类的实现。和 FileSystemResourceLoader 类似，在实现代码的结构上类似，也是覆写 #getResourceByPath(String path) 方法，并返回其对应的 ClassRelativeContextResource 的资源类型。 感兴趣的胖友，可以看看 《Spring5：就这一次，搞定资源加载器之ClassRelativeResourceLoader》 文章。 ClassRelativeResourceLoader 扩展的功能是，可以根据给定的class 所在包或者所在包的子包下加载资源。 2.4 ResourcePatternResolverResourceLoader 的 Resource getResource(String location) 方法，每次只能根据 location 返回一个 Resource 。当需要加载多个资源时，我们除了多次调用 #getResource(String location) 方法外，别无他法。org.springframework.core.io.support.ResourcePatternResolver 是 ResourceLoader 的扩展，它支持根据指定的资源路径匹配模式每次返回多个 Resource 实例，其定义如下： public interface ResourcePatternResolver extends ResourceLoader &#123;\tString CLASSPATH_ALL_URL_PREFIX = &quot;classpath*:&quot;;\tResource[] getResources(String locationPattern) throws IOException;&#125; ResourcePatternResolver 在 ResourceLoader 的基础上增加了 #getResources(String locationPattern) 方法，以支持根据路径匹配模式返回多个 Resource 实例。 同时，也新增了一种新的协议前缀 &quot;classpath*:&quot;，该协议前缀由其子类负责实现。 2.5 PathMatchingResourcePatternResolverorg.springframework.core.io.support.PathMatchingResourcePatternResolver ，为 ResourcePatternResolver 最常用的子类，它除了支持 ResourceLoader 和 ResourcePatternResolver 新增的 &quot;classpath*:&quot; 前缀外，还支持 Ant 风格的路径匹配模式（类似于 &quot;**/*.xml&quot;）。 2.5.1 构造函数PathMatchingResourcePatternResolver 提供了三个构造函数，如下： /** * 内置的 ResourceLoader 资源定位器 */private final ResourceLoader resourceLoader;/** * Ant 路径匹配器 */private PathMatcher pathMatcher = new AntPathMatcher();public PathMatchingResourcePatternResolver() &#123;\tthis.resourceLoader = new DefaultResourceLoader();&#125;public PathMatchingResourcePatternResolver(ResourceLoader resourceLoader) &#123;\tAssert.notNull(resourceLoader, &quot;ResourceLoader must not be null&quot;);\tthis.resourceLoader = resourceLoader;&#125;public PathMatchingResourcePatternResolver(@Nullable ClassLoader classLoader) &#123;\tthis.resourceLoader = new DefaultResourceLoader(classLoader);&#125; PathMatchingResourcePatternResolver 在实例化的时候，可以指定一个 ResourceLoader，如果不指定的话，它会在内部构造一个 DefaultResourceLoader 。 pathMatcher 属性，默认为 AntPathMatcher 对象，用于支持 Ant 类型的路径匹配。 2.5.2 getResource@Overridepublic Resource getResource(String location) &#123;\treturn getResourceLoader().getResource(location);&#125;public ResourceLoader getResourceLoader() &#123;\treturn this.resourceLoader;&#125; 该方法，直接委托给相应的 ResourceLoader 来实现。所以，如果我们在实例化的 PathMatchingResourcePatternResolver 的时候，如果未指定 ResourceLoader 参数的情况下，那么在加载资源时，其实就是 DefaultResourceLoader 的过程。 其实在下面介绍的 Resource[] getResources(String locationPattern) 方法也相同，只不过返回的资源是多个而已。 2.5.3 getResources@Overridepublic Resource[] getResources(String locationPattern) throws IOException &#123; Assert.notNull(locationPattern, &quot;Location pattern must not be null&quot;); // 以 &quot;classpath*:&quot; 开头 if (locationPattern.startsWith(CLASSPATH_ALL_URL_PREFIX)) &#123; // 路径包含通配符 // a class path resource (multiple resources for same name possible) if (getPathMatcher().isPattern(locationPattern.substring(CLASSPATH_ALL_URL_PREFIX.length()))) &#123; // a class path resource pattern return findPathMatchingResources(locationPattern); // 路径不包含通配符 &#125; else &#123; // all class path resources with the given name return findAllClassPathResources(locationPattern.substring(CLASSPATH_ALL_URL_PREFIX.length())); &#125; // 不以 &quot;classpath*:&quot; 开头 &#125; else &#123; // Generally only look for a pattern after a prefix here, // 通常只在这里的前缀后面查找模式 // and on Tomcat only after the &quot;*/&quot; separator for its &quot;war:&quot; protocol. 而在 Tomcat 上只有在 “*/ ”分隔符之后才为其 “war:” 协议 int prefixEnd = (locationPattern.startsWith(&quot;war:&quot;) ? locationPattern.indexOf(&quot;*/&quot;) + 1 : locationPattern.indexOf(&#x27;:&#x27;) + 1); // 路径包含通配符 if (getPathMatcher().isPattern(locationPattern.substring(prefixEnd))) &#123; // a file pattern return findPathMatchingResources(locationPattern); // 路径不包含通配符 &#125; else &#123; // a single resource with the given name return new Resource[] &#123;getResourceLoader().getResource(locationPattern)&#125;; &#125; &#125;&#125; 非 &quot;classpath*:&quot; 开头，且路径不包含通配符，直接委托给相应的 ResourceLoader 来实现。 其他情况，调用 #findAllClassPathResources(...)、或 #findPathMatchingResources(...) 方法，返回多个 Resource 。下面，我们来详细分析。 2.5.4 findAllClassPathResources当 locationPattern 以 &quot;classpath*:&quot; 开头但是不包含通配符，则调用 #findAllClassPathResources(...) 方法加载资源。该方法返回 classes 路径下和所有 jar 包中的所有相匹配的资源。 protected Resource[] findAllClassPathResources(String location) throws IOException &#123;\tString path = location;\t// 去除首个 /\tif (path.startsWith(&quot;/&quot;)) &#123; path = path.substring(1);\t&#125;\t// 真正执行加载所有 classpath 资源\tSet&lt;Resource&gt; result = doFindAllClassPathResources(path);\tif (logger.isTraceEnabled()) &#123; logger.trace(&quot;Resolved classpath location [&quot; + location + &quot;] to resources &quot; + result);\t&#125;\t// 转换成 Resource 数组返回\treturn result.toArray(new Resource[0]);&#125; 真正执行加载的是在 #doFindAllClassPathResources(...) 方法，代码如下： protected Set&lt;Resource&gt; doFindAllClassPathResources(String path) throws IOException &#123;\tSet&lt;Resource&gt; result = new LinkedHashSet&lt;&gt;(16);\tClassLoader cl = getClassLoader();\t// &lt;1&gt; 根据 ClassLoader 加载路径下的所有资源\tEnumeration&lt;URL&gt; resourceUrls = (cl != null ? cl.getResources(path) : ClassLoader.getSystemResources(path));\t// &lt;2&gt;\twhile (resourceUrls.hasMoreElements()) &#123; URL url = resourceUrls.nextElement(); // 将 URL 转换成 UrlResource result.add(convertClassLoaderURL(url));\t&#125;\t// &lt;3&gt; 加载路径下得所有 jar 包\tif (&quot;&quot;.equals(path)) &#123; // The above result is likely to be incomplete, i.e. only containing file system references. // We need to have pointers to each of the jar files on the classpath as well... addAllClassLoaderJarRoots(cl, result);\t&#125;\treturn result;&#125; &lt;1&gt; 处，根据 ClassLoader 加载路径下的所有资源。在加载资源过程时，如果在构造 PathMatchingResourcePatternResolver 实例的时候如果传入了 ClassLoader，则调用该 ClassLoader 的 #getResources() 方法，否则调用 ClassLoader#getSystemResources(path) 方法。另外，ClassLoader#getResources() 方法，代码如下: // java.lang.ClassLoader.javapublic Enumeration&lt;URL&gt; getResources(String name) throws IOException &#123; @SuppressWarnings(&quot;unchecked&quot;) Enumeration&lt;URL&gt;[] tmp = (Enumeration&lt;URL&gt;[]) new Enumeration&lt;?&gt;[2]; if (parent != null) &#123; tmp[0] = parent.getResources(name); &#125; else &#123; tmp[0] = getBootstrapResources(name); &#125; tmp[1] = findResources(name); return new CompoundEnumeration&lt;&gt;(tmp);&#125; 看到这里是不是就已经一目了然了？如果当前父类加载器不为 null ，则通过父类向上迭代获取资源，否则调用 #getBootstrapResources() 。这里是不是特别熟悉，(^▽^)。 &lt;2&gt; 处，遍历 URL 集合，调用 #convertClassLoaderURL(URL url) 方法，将 URL 转换成 UrlResource 对象。代码如下： protected Resource convertClassLoaderURL(URL url) &#123;\treturn new UrlResource(url);&#125; &lt;3&gt; 处，若 path 为空（“”）时，则调用 #addAllClassLoaderJarRoots(...)方法。该方法主要是加载路径下得所有 jar 包，方法较长也没有什么实际意义就不贴出来了。感兴趣的胖友，自己可以去看看。😈 当然，可能代码也比较长哈。 通过上面的分析，我们知道 #findAllClassPathResources(...) 方法，其实就是利用 ClassLoader 来加载指定路径下的资源，不论它是在 class 路径下还是在 jar 包中。如果我们传入的路径为空或者 /，则会调用 #addAllClassLoaderJarRoots(...) 方法，加载所有的 jar 包。 2.5.5 findPathMatchingResources当 locationPattern 中包含了通配符，则调用该方法进行资源加载。代码如下： protected Resource[] findPathMatchingResources(String locationPattern) throws IOException &#123; // 确定根路径、子路径 String rootDirPath = determineRootDir(locationPattern); String subPattern = locationPattern.substring(rootDirPath.length()); // 获取根据路径下的资源 Resource[] rootDirResources = getResources(rootDirPath); // 遍历，迭代 Set&lt;Resource&gt; result = new LinkedHashSet&lt;&gt;(16); for (Resource rootDirResource : rootDirResources) &#123; rootDirResource = resolveRootDirResource(rootDirResource); URL rootDirUrl = rootDirResource.getURL(); // bundle 资源类型 if (equinoxResolveMethod != null &amp;&amp; rootDirUrl.getProtocol().startsWith(&quot;bundle&quot;)) &#123; URL resolvedUrl = (URL) ReflectionUtils.invokeMethod(equinoxResolveMethod, null, rootDirUrl); if (resolvedUrl != null) &#123; rootDirUrl = resolvedUrl; &#125; rootDirResource = new UrlResource(rootDirUrl); &#125; // vfs 资源类型 if (rootDirUrl.getProtocol().startsWith(ResourceUtils.URL_PROTOCOL_VFS)) &#123; result.addAll(VfsResourceMatchingDelegate.findMatchingResources(rootDirUrl, subPattern, getPathMatcher())); // jar 资源类型 &#125; else if (ResourceUtils.isJarURL(rootDirUrl) || isJarResource(rootDirResource)) &#123; result.addAll(doFindPathMatchingJarResources(rootDirResource, rootDirUrl, subPattern)); // 其它资源类型 &#125; else &#123; result.addAll(doFindPathMatchingFileResources(rootDirResource, subPattern)); &#125; &#125; if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Resolved location pattern [&quot; + locationPattern + &quot;] to resources &quot; + result); &#125; // 转换成 Resource 数组返回 return result.toArray(new Resource[0]);&#125; 方法有点儿长，但是思路还是很清晰的，主要分两步： 确定目录，获取该目录下得所有资源。 在所获得的所有资源后，进行迭代匹配获取我们想要的资源。 在这个方法里面，我们要关注两个方法，一个是 #determineRootDir(String location) 方法，一个是 #doFindPathMatchingXXXResources(...) 等方法。 2.5.5.1 determineRootDirdetermineRootDir(String location) 方法，主要是用于确定根路径。代码如下： /** * Determine the root directory for the given location. * &lt;p&gt;Used for determining the starting point for file matching, * resolving the root directory location to a &#123;@code java.io.File&#125; * and passing it into &#123;@code retrieveMatchingFiles&#125;, with the * remainder of the location as pattern. * &lt;p&gt;Will return &quot;/WEB-INF/&quot; for the pattern &quot;/WEB-INF/*.xml&quot;, * for example. * @param location the location to check * @return the part of the location that denotes the root directory * @see #retrieveMatchingFiles */protected String determineRootDir(String location) &#123;\t// 找到冒号的后一位\tint prefixEnd = location.indexOf(&#x27;:&#x27;) + 1;\t// 根目录结束位置\tint rootDirEnd = location.length();\t// 在从冒号开始到最后的字符串中，循环判断是否包含通配符，如果包含，则截断最后一个由”/”分割的部分。\t// 例如：在我们路径中，就是最后的ap?-context.xml这一段。再循环判断剩下的部分，直到剩下的路径中都不包含通配符。\twhile (rootDirEnd &gt; prefixEnd &amp;&amp; getPathMatcher().isPattern(location.substring(prefixEnd, rootDirEnd))) &#123; rootDirEnd = location.lastIndexOf(&#x27;/&#x27;, rootDirEnd - 2) + 1;\t&#125;\t// 如果查找完成后，rootDirEnd = 0 了，则将之前赋值的 prefixEnd 的值赋给 rootDirEnd ，也就是冒号的后一位\tif (rootDirEnd == 0) &#123; rootDirEnd = prefixEnd;\t&#125;\t// 截取根目录\treturn location.substring(0, rootDirEnd);&#125; 方法比较绕，效果如下示例： 原路径 确定根路径 classpath*:test/cc*/spring-*.xml classpath*:test/ classpath*:test/aa/spring-*.xml classpath*:test/aa/ 2.5.5.2 doFindPathMatchingXXXResources 来自艿艿 #doFindPathMatchingXXXResources(...) 方法，是个泛指，一共对应三个方法： #doFindPathMatchingJarResources(rootDirResource, rootDirUrl, subPatter) 方法 #doFindPathMatchingFileResources(rootDirResource, subPattern) 方法 VfsResourceMatchingDelegate#findMatchingResources(rootDirUrl, subPattern, pathMatcher) 方法 因为本文重在分析 Spring 统一资源加载策略的整体流程。相对来说，上面几个方法的代码量会比较多。所以本文不再追溯，感兴趣的胖友，推荐阅读如下文章： 《Spring源码情操陶冶-PathMatchingResourcePatternResolver路径资源匹配溶解器》 ，主要针对 #doFindPathMatchingJarResources(rootDirResource, rootDirUrl, subPatter) 方法。 《深入 Spring IoC 源码之 ResourceLoader》 ，主要针对 #doFindPathMatchingFileResources(rootDirResource, subPattern) 方法。 《Spring 源码学习 —— 含有通配符路径解析（上）》 😈 貌似没有下 3. 小结至此 Spring 整个资源记载过程已经分析完毕。下面简要总结下： Spring 提供了 Resource 和 ResourceLoader 来统一抽象整个资源及其定位。使得资源与资源的定位有了一个更加清晰的界限，并且提供了合适的 Default 类，使得自定义实现更加方便和清晰。 AbstractResource 为 Resource 的默认抽象实现，它对 Resource 接口做了一个统一的实现，子类继承该类后只需要覆盖相应的方法即可，同时对于自定义的 Resource 我们也是继承该类。 DefaultResourceLoader 同样也是 ResourceLoader 的默认实现，在自定 ResourceLoader 的时候我们除了可以继承该类外还可以实现 ProtocolResolver 接口来实现自定资源加载协议。 DefaultResourceLoader 每次只能返回单一的资源，所以 Spring 针对这个提供了另外一个接口 ResourcePatternResolver ，该接口提供了根据指定的 locationPattern 返回多个资源的策略。其子类 PathMatchingResourcePatternResolver 是一个集大成者的 ResourceLoader ，因为它即实现了 Resource getResource(String location) 方法，也实现了 Resource[] getResources(String locationPattern) 方法。 另外，如果胖友认真的看了本文的包结构，我们可以发现，Resource 和 ResourceLoader 核心是在，spring-core 项目中。 如果想要调试本小节的相关内容，可以直接使用 Resource 和 ResourceLoader 相关的 API ，进行操作调试。","categories":["Spring"]},{"title":"RocketMQ","path":"/2023/11/27/RocketMQ/RocketMQ/","content":"一、基本概念1、消息(Message) 消息是指，消息系统所传输的物理载体，生产和消费数据的最小单位，每个消息必须属于一个主题。 2、主题(Topic) Topic表示一类消息的集合，每个主题包含若干条消息，是RocketMQ进行消息订阅的基本单位。 一个生产者可以同时发送多种Topic的消息，而一个消费者只能订阅和消费一种Topic的消息 3、标签（TAG） 4、队列（Queue） 存储消息的物理实体。一个Topic中可以包含多个Queue，每个Queue中存放的就是该Topic的消息。一个Topic的Queue也被称为一个Topic中消息的分区。 一个Topic中Queue中的消息只能被一个消费者组中的一个消费者消费。一个Queue中的消息不允许同一个消费者组中的多个消费者同时消费。 5、消息标识（MessageId&#x2F;Key） RocketMQ中每个消息都拥有唯一的MessageId，且可以携带具有业务标识的Key，以方便对消息的查询。不过需要注意的是，MessageId有两个：在生产者send()消息时会自动生成一个MessageId(msgId)，当消息达到Broker之后，Broker也会自动生成一个MessageId(offsetMsgId)。msgId、offsetMsgId与key都称为消息标识。 msgId：由producer生成，其生成规则为： producerIp + 进程pid + messageClientIDSetter的ClassLoader的hashCode + 当前时间 + AutomicInteger自增计数器 offsetMsgId：由broker端生成，其生成规则为：brokerIp + 物理分区的offset（也就是queue中的偏移量） key: 由用户指定的业务相关的唯一标识 二、系统架构 RocketMQ架构上主要分为四部分构成： 1 Producer消息生产者，负责生产消息。Producer通过MQ的负载均衡模块选择相应的Broker集群队列进行消息投递，投递的过程支持快速失败并且低延迟。 RocketMQ中的消息生产者都是以生产者组（Producer Group）的形式出现的。生产者组是同一类生产者的集合，这类Producer发送相同Topic类型的消息。一个生产者组可以同时发送多个主题的消息。 2 Consumer消息消费者，负责消费消息。一个消息消费者会从Broker服务器中获取到消息，并对消息进行相关业务处理。 RocketMQ中的消息消费者都是以消费者组（Consumer Group）的形式出现的。消费者组是同一类消费者的集合，这类Consumer消费的是同一个Topic类型的消息。消费者组使得在消息消费方面，实现负载均衡（将一个Topic中的不同的Queue平均分配给同一个Consumer Group的不同的Consumer，注意，并不是将消息负载均衡）和容错（一个Consmer挂了，该Consumer Group中的其它Consumer可以接着消费原Consumer消费的Queue）的目标变得非常容易。 消费者组中Consumer的数量应该小于等于订阅Topic的Queue数量。如果超出Queue数量，则多出的Consumer将不能消费消息。 不过，一个Topic类型的消息可以被多个消费者组同时消费。 注意， 1）消费者组只能消费一个Topic的消息，不能同时消费多个Topic消息 2）一个消费者组中的消费者必须订阅完全相同的Topic 3 Name ServerNameServer是一个Broker和Topic路由的注册中心，支持Broker的动态注册和发现。 主要包括两个功能： Broker管理 路由信息管理 4 BrokerBroker充当消息中转角色，负责存储消息、转发消息。Broker在RocketMQ系统中负责接收并存储从生产者发送来的消息，同时为消费者的拉取请求做准备。Broker同时也存储着消息相关的元数据，包括消费者组消费进度偏移offset、主题、队列等。 Remoting Module：整个Broker的实体，负责处理来自clients端的请求。而这个Broker实体则由以下模块构成。 Client Manager：客户端管理器。负责接收、解析客户端(Producer&#x2F;Consumer)请求，管理客户端。例如，维护Consumer的Topic订阅信息 Store Service：存储服务。提供方便简单的API接口，处理消息存储到物理硬盘和消息查询功能。 HA Service：高可用服务，提供Master Broker 和 Slave Broker之间的数据同步功能。 Index Service：索引服务。根据特定的Message key，对投递到Broker的消息进行索引服务，同时也提供根据Message Key对消息进行快速查询的功能。 三、Linux启动1.启动nameServer和broker### 启动nameServernohup sh bin/mqnamesrv &amp;### 验证namesrv是否启动成功$ tail -f ~/logs/rocketmqlogs/namesrv.logThe Name Server boot success...### 先启动broker$ nohup sh bin/mqbroker -n localhost:9876 &amp;### 验证broker是否启动成功, 比如, broker的ip是192.168.1.2 然后名字是broker-a$ tail -f ~/logs/rocketmqlogs/Broker.log The broker[broker-a,192.169.1.2:10911] boot success... 2.消息收发在进行消息收发之前，我们需要告诉客户端NameServer的地址，RocketMQ有多种方式在客户端中设置NameServer地址，这里我们利用环境变量NAMESRV_ADDR $ export NAMESRV_ADDR=localhost:9876$ sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer SendResult [sendStatus=SEND_OK, msgId= ...$ sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer ConsumeMessageThread_%d Receive New Messages: [MessageExt... docker启动1.拉取镜像 docker pull rocketmqinc/rocketmq 2.创建数据挂载目录 mkdir -p /docker/rocketmq/data/namesrv/logs /docker/rocketmq/data/namesrv/storemkdir -p /docker/rocketmq/data/broker/logs /docker/rocketmq/data/broker/store /docker/rocketmq/conf 3.启动nameserver docker run -d --restart=always --name rmqnamesrv - -p 9876:9876 -v /Users/dongnan/devEnv/docker/rocketmq/data/namesrv/logs:/root/logs -v /Users/dongnan/devEnv/docker/rocketmq/data/namesrv/store:/root/store -e &quot;MAX_POSSIBLE_HEAP=100000000&quot; rocketmqinc/rocketmq sh mqnamesrv 4.编辑配置文件 vi /docker/rocketmq/conf/broker.confbrokerClusterName = DefaultCluster#broker名称，master和slave使用相同的名称，表明他们的主从关系brokerName = broker-a#0表示Master，大于0表示不同的slavebrokerId = 0#表示几点做消息删除动作，默认是凌晨4点deleteWhen = 04#在磁盘上保留消息的时长，单位是小时fileReservedTime = 48#有三个值：SYNC_MASTER，ASYNC_MASTER，SLAVE；同步和异步表示Master和Slave之间同步数据的机制；brokerRole = ASYNC_MASTER#刷盘策略，取值为：ASYNC_FLUSH，SYNC_FLUSH表示同步刷盘和异步刷盘；SYNC_FLUSH消息写入磁盘后才返回成功状态，ASYNC_FLUSH不需要；flushDiskType = ASYNC_FLUSH#设置broker节点所在服务器的ip地址brokerIP1 = 127.0.0.1 5.启动broker docker run -d \\ --restart=always \\--name rmqbroker \\--link rmqnamesrv:namesrv \\-p 10911:10911 \\-p 10909:10909 \\-v /Users/dongnan/devEnv/docker/rocketmq/data/broker/logs:/root/logs \\-v /Users/dongnan/devEnv/docker/rocketmq/data/broker/store:/root/store \\-v /Users/dongnan/devEnv/docker/rocketmq/conf/broker.conf:/opt/rocketmq-4.4.0/conf/broker.conf \\-e &quot;NAMESRV_ADDR=namesrv:9876&quot; \\-e &quot;MAX_POSSIBLE_HEAP=200000000&quot; \\rocketmqinc/rocketmq \\sh mqbroker -c /opt/rocketmq-4.4.0/conf/broker.conf 6.部署rocketmq界面 docker pull styletang/rocketmq-console-ngdocker run -d -p 8082:8080 -e &quot;JAVA_OPTS=-Drocketmq.config.namesrvAddr=rmqnamesrv:9876 -Drocketmq.config.isVIPChannel=false&quot; styletang/rocketmq-console-ng 四、集群搭建1.数据复制和刷盘策略 复制策略 复制策略是Broker的Master与Slave间的数据同步方式。分为同步复制与异步复制： 同步复制：消息写入master后，master会等待slave同步数据成功后才向producer返回成功ACK 异步复制：消息写入master后，master立即向producer返回成功ACK，无需等待slave同步数据成功 异步复制策略会降低系统的写入延迟，RT变小，提高了系统的吞吐量 刷盘策略 刷盘策略指的是broker中消息的落盘方式，即消息发送到broker内存后消息持久化到磁盘的方式。分为同步刷盘与异步刷盘： 同步刷盘：当消息持久化到broker的磁盘后才算是消息写入成功 异步刷盘：当消息写入到broker的内存后即表示消息写入成功，无需等待消息持久化到磁盘。 最佳实践 一般会为Master配置RAID10磁盘阵列，然后再为其配置一个slave。即利用了RAID10磁盘阵列的高效、安全性，又解决了可能会影响订阅的问题 五、工作原理1.消息的生产消息的生产过程Producer可以将消息写入到某broker中的某Queue中，过程如下 Producer发送消息之前，会先向NameServer发出获取消息Topic的路由信息的请求 NameServer返回该Topic的路由表及Broker列表 Producer根据代码中指定的Queue选择策略，从Queue列表中选出一个队列，用于后续存储消息 Producer对消息做一些特殊的处理，例如，消息本身超过4M，则进行压缩 Producer向选择出的Queue所在的broker发出RPC请求，将消息发送到选择出的Queue 路由表：实际是一个Map，key为Topic的名称，value是一个QueueData实例列表，QueueData并不是一个Queue对应一个QueueData，而是一个Broker中该Topic的所有Queue对应一个QueueData。即，只要涉及到该Topic的Broker，一个Broker对应一个QueueData。QueueData中包含brokerName。简单来说，路由表的key为Topic名称，value则为所有涉及该Topic的 BrokerName列表。 一套brokerName名称相同的Master-Slave小集群对应一个 BrokerData。BrokerData中包含brokerName及一个map。该map的key为brokerId，value为该 broker对应的地址。brokerId为0表示该broker为Master，非0表示Slave。 Queue选择算法轮询算法 最小投递延迟算法 2.消息的存储 abort：该文件在Broker启动后会自动创建，正常关闭Broker，该文件会自动消失。若在没有启动Broker的情况下，发现这个文件是存在的，则说明之前Broker的关闭是非正常关闭。 checkpoint：其中存储着commitlog、consumequeue、index文件的最后刷盘时间戳 commitlog：其中存放着commitlog文件，而消息是写在commitlog文件中的 config：存放着Broker运行期间的一些配置数据 consumequeue：其中存放着consumequeue文件，队列就存放在这个目录中 index：其中存放着消息索引文件indexFile lock：运行期间使用到的全局资源锁 3.Rebalance机制​ rebalance即再均衡，指的是，将一个topic下的多个Queue在同一个Consumer Group中的多个Consumer间进行重新分配的过程。 Rebalance限制 ​ 由于⼀个队列最多分配给⼀个消费者，因此当某个消费者组下的消费者实例数量大于队列的数量时，多余的消费者实例将分配不到任何队列 4.Queue分配算法 平均分配策略 该算法是要根据avg = QueueCount / ConsumerCount 的计算结果进行分配的。如果能够整除，则按顺序将avg个Queue逐个分配Consumer；如果不能整除，则将多余出的Queue按照Consumer顺序逐个分配。 环形平均策略 环形平均算法是指，根据消费者的顺序，依次在由queue队列组成的环形图中逐个分配。 一致性hash策略 该算法会将consumer的hash值作为Node节点存放到hash环上，然后将queue的hash值也放到hash环上，通过顺时针方向，距离queue最近的那个consumer就是该queue要分配的consumer。 同机房策略 该算法会根据queue的部署机房位置和consumer的位置，过滤出当前consumer相同机房的queue。然后按照平均分配策略或环形平均策略对同机房queue进行分配。如果没有同机房queue，则按照平均分配策略或环形平均策略对所有queue进行分配。 六、offset管理这里的offset指的是Consumer的消费进度offset。消费进度offset是用来记录每个Queue的不同消费组的消费进度的。根据消费进度记录器的不同，可以分为两种模式：本地模式和远程模式。 1 offset本地管理模式当消费模式为广播消费时，offset使用本地模式存储。因为每条消息会被所有的消费者消费，每个消费者管理自己的消费进度，各个消费者之间不存在消费进度的交集。Consumer在广播消费模式下offset相关数据以json的形式持久化到Consumer本地磁盘文件中，默认文件路径为当前用户主目录下的.rocketmq_offsets&#x2F;${clientId}&#x2F;${group}&#x2F;Offsets.json。其中${clientId}为当前消费者id，默认为ip@DEFAULT；${group}为消费者组名称。 2 offset远程管理模式当消费模式为集群消费时，offset使用远程模式管理。因为所有Cosnumer实例对消息采用的是均衡消费，所有Consumer共享Queue的消费进度。Consumer在集群消费模式下offset相关数据以json的形式持久化到Broker磁盘文件中，文件路径为当前用户主目录下的store&#x2F;config&#x2F;consumerOffset.json。 Broker启动时会加载这个文件，并写入到一个双层Map（ConsumerOffsetManager）。外层map的key为topic@group，value为内层map。内层map的key为queueId，value为offset。当发生Rebalance时，新的Consumer会从该Map中获取到相应的数据来继续消费。 集群模式下offset采用远程管理模式，主要是为了保证Rebalance机制。 3 offset用途消费者是如何从最开始持续消费消息的？消费者要消费的第一条消息的起始位置是用户自己通过consumer.setConsumeFromWhere()方法指定的。 Properties properties = new Properties();properties.put(PropertyKeyConst.GROUP_ID, &quot;GID_jodie_test_3&quot;);Consumer consumer = ONSFactory.createConsumer(properties);consumer.subscribe(&quot;jodie_test_A&quot;, &quot;TagB&quot;, new MessageListener() &#123;public Action consume(Message message, ConsumeContext context) &#123;System.out.println(message.getMsgID());return Action.CommitMessage;&#125;&#125;); 1 2 3 4 5 6 7 8 9 在Consumer启动后，其要消费的第一条消息的起始位置常用的有三种，这三种位置可以通过枚举类型常量设置。这个枚举类型为ConsumeFromWhere。 CONSUME_FROM_LAST_OFFSET：从queue的当前最后一条消息开始消费 CONSUME_FROM_FIRST_OFFSET：从queue的第一条消息开始消费 CONSUME_FROM_TIMESTAMP：从指定的具 体时间戳位置的消息开始消费。这个具体时间戳是通过另外一个语句指定的 。 consumer.setConsumeTimestamp(“20210701080000”) yyyyMMddHHmmss 当消费完一批消息后，Consumer会提交其消费进度offset给Broker，Broker在收到消费进度后会将其更新到那个双层Map（ConsumerOffsetManager）及consumerOffset.json文件中，然后向该Consumer进行ACK，而ACK内容中包含三项数据：当前消费队列的最小offset（minOffset）、最大offset（maxOffset）、及下次消费的起始offset（nextBeginOffset）。 4 重试队列当rocketMQ对消息的消费出现异常时，会将发生异常的消息的offset提交到Broker中的重试队列。系统在发生消息消费异常时会为当前的topic@group创建一个重试队列，该队列以%RETRY%开头，到达重试时间后进行消费重试。 5 offset的同步提交与异步提交集群消费模式下，Consumer消费完消息后会向Broker提交消费进度offset，其提交方式分为两种： ​ 同步提交：消费者在消费完一批消息后会向broker提交这些消息的offset，然后等待broker的成功响应。若在等待超时之前收到了成功响应，则继续读取下一批消息进行消费（从ACK中获取nextBeginOffset）。若没有收到响应，则会重新提交，直到获取到响应。而在这个等待过程中，消费者是阻塞的。其严重影响了消费者的吞吐量。 ​ 异步提交：消费者在消费完一批消息后向broker提交offset，但无需等待Broker的成功响应，可以继续读取并消费下一批消息。这种方式增加了消费者的吞吐量。但需要注意，broker在收到提交的offset后，还是会向消费者进行响应的。可能还没有收到ACK，此时Consumer会从Broker中直接获取nextBeginOffset。 七、消费幂等1 什么是消费幂等​ 当出现消费者对某条消息重复消费的情况时，重复消费的结果与消费一次的结果是相同的，并且多次消费并未对业务系统产生任何负面影响，那么这个消费过程就是消费幂等的。 ​ 幂等：若某操作执行多次与执行一次对系统产生的影响是相同的，则称该操作是幂等的。在互联网应用中，尤其在网络不稳定的情况下，消息很有可能会出现重复发送或重复消费。如果重复的消息可能会影响业务处理，那么就应该对消息做幂等处理。 2 消息重复的场景分析什么情况下可能会出现消息被重复消费呢？最常见的有以下三种情况： 发送时消息重复当一条消息已被成功发送到Broker并完成持久化，此时出现了网络闪断，从而导致Broker对Producer应答失败。 如果此时Producer意识到消息发送失败并尝试再次发送消息，此时Broker中就可能会出现两条内容相同并且Message ID也相同的消息，那么后续Consumer就一定会消费两次该消息。 消费时消息重复消息已投递到Consumer并完成业务处理，当Consumer给Broker反馈应答时网络闪断，Broker没有接收到消费成功响应。为了保证消息至少被消费一次的原则，Broker将在网络恢复后再次尝试投递之前已被处理过的消息。此时消费者就会收到与之前处理过的内容相同、Message ID也相同的消息。Rebalance时消息重复当Consumer Group中的Consumer数量发生变化时，或其订阅的Topic的Queue数量发生变化时，会触发Rebalance，此时Consumer可能会收到曾经被消费过的消息。 3 通用解决方案两要素 幂等解决方案的设计中涉及到两项要素：幂等令牌，与唯一性处理。只要充分利用好这两要素，就可以设计出好的幂等解决方案。 幂等令牌：是生产者和消费者两者中的既定协议，通常指具备唯一业务标识的字符串。例如，订单号、流水号。一般由Producer随着消息一同发送来的。唯一性处理：服务端通过采用一定的算法策略，保证同一个业务逻辑不会被重复执行成功多次。例如，对同一笔订单的多次支付操作，只会成功一次。 解决方案对于常见的系统，幂等性操作的通用性解决方案是： 首先通过缓存去重。在缓存中如果已经存在了某幂等令牌，则说明本次操作是重复性操作；若缓存没有命中，则进入下一步。 在唯一性处理之前，先在数据库中查询幂等令牌作为索引的数据是否存在。若存在，则说明本次操作为重复性操作；若不存在，则进入下一步。 在同一事务中完成三项操作：唯一性处理后，将幂等令牌写入到缓存，并将幂等令牌作为唯一索引的数据写入到DB中。 第 1 步已经判断过是否是重复性操作了，为什么第 2 步还要再次判断？能够进入第 2 步，说明已经不是重复操作了，第 2 次判断是否重复？当然不重复。一般缓存中的数据是具有有效期的。缓存中数据的有效期一旦过期，就是发生缓存穿透，使请求直接就到达了DBMS。 解决方案举例以支付场景为例：1. 当支付请求到达后，首先在Redis缓存中却获取key为支付流水号的缓存value。若value不空，则说明本次支付是重复操作，业务系统直接返回调用侧重复支付标识；若value为空，则进入下一步操作2. 到DBMS中根据支付流水号查询是否存在相应实例。若存在，则说明本次支付是重复操作，业务系统直接返回调用侧重复支付标识；若不存在，则说明本次操作是首次操作，进入下一步完成唯一性处理3. 在分布式事务中完成三项操作：\t完成支付任务\t将当前支付流水号作为key，任意字符串作为value，通过set(key, value, expireTime)将数据写入到Redis缓存\t将当前支付流水号作为主键，与其它相关数据共同写入到DBMS 4 消费幂等的实现消费幂等的解决方案很简单：为消息指定不会重复的唯一标识。因为Message ID有可能出现重复的情况，所以真正安全的幂等处理，不建议以Message ID作为处理依据。最好的方式是以业务唯一标识作为幂等处理的关键依据，而业务的唯一标识可以通过消息Key设置。 以支付场景为例，可以将消息的Key设置为订单号，作为幂等处理的依据。具体代码示例如下： 消费者收到消息时可以根据消息的Key即订单号来实现消费幂等： Message message = new Message();message.setKey(&quot;ORDERID_100&quot;);SendResult sendResult = producer.send(message); 123 RocketMQ能够保证消息不丢失，但不能保证消息不重复。 八、消息堆积与消费延迟1 概念消息处理流程中，如果Consumer的消费速度跟不上Producer的发送速度，MQ中未处理的消息会越来越多（进的多出的少），这部分消息就被称为堆积消息。消息出现堆积进而会造成消息的消费延迟。 以下场景需要重点关注消息堆积和消费延迟问题： ​ 业务系统上下游能力不匹配造成的持续堆积，且无法自行恢复。 ​ 业务系统对消息的消费实时性要求较高，即使是短暂的堆积造成的消费延迟也无法接受。 2 产生原因分析Consumer使用长轮询Pull模式消费消息时，分为以下两个阶段： consumer.registerMessageListener(new MessageListenerConcurrently() &#123;@Overridepublic ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt;msgs,ConsumeConcurrentlyContextcontext) &#123;for(MessageExt msg:msgs)&#123;String key = msg.getKeys();// 根据业务唯一标识Key做幂等处理// ......&#125;return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;&#125;&#125;); 1 2 3 4 5 6 7 8 9 101112 消息拉取Consumer通过长轮询Pull模式批量拉取的方式从服务端获取消息，将拉取到的消息缓存到本地缓冲队列中。对于拉取式消费，在内网环境下会有很高的吞吐量，所以这一阶段一般不会成为消息堆积的瓶颈。 一个单线程单分区的低规格主机(Consumer，4C8G)，其可达到几万的TPS。如果是多个分区多个线程，则可以轻松达到几十万的TPS。 消息消费Consumer将本地缓存的消息提交到消费线程中，使用业务消费逻辑对消息进行处理，处理完毕后获取到一个结果。这是真正的消息消费过程。此时Consumer的消费能力就完全依赖于消息的消费耗时和消费并发度了。如果由于业务处理逻辑复杂等原因，导致处理单条消息的耗时较长，则整体的消息吞吐量肯定不会高，此时就会导致Consumer本地缓冲队列达到上限，停止从服务端拉取消息。 结论消息堆积的主要瓶颈在于客户端的消费能力，而消费能力由消费耗时和消费并发度决定。注意，消费耗时的优先级要高于消费并发度。即在保证了消费耗时的合理性前提下，再考虑消费并发度问题。 3 消费耗时​ 影响消息处理时长的主要因素是代码逻辑。而代码逻辑中可能会影响处理时长代码主要有两种类型：CPU内部计算型代码和外部I&#x2F;O操作型代码。通常情况下代码中如果没有复杂的递归和循环的话，内部计算耗时相对外部I&#x2F;O操作来说几乎可以忽略。所以外部IO型代码是影响消息处理时长的主要症结所在。外部IO操作型代码举例： 读写外部数据库，例如对远程MySQL的访问 读写外部缓存系统，例如对远程Redis的访问 下游系统调用，例如Dubbo的RPC远程调用，Spring Cloud的对下游系统的Http接口调用 关于下游系统调用逻辑需要进行提前梳理，掌握每个调用操作预期的耗时，这样做是为了能够判断消费逻辑中IO操作的耗时是否合理。通常消息堆积是由于下游系统出现了服务异常或达到了DBMS容量限制，导致消费耗时增加。服务异常，并不仅仅是系统中出现的类似 500 这样的代码错误，而可能是更加隐蔽的问题。例如，网络带宽问题。达到了DBMS容量限制，其也会引发消息的消费耗时增加。 4 消费并发度一般情况下，消费者端的消费并发度由单节点线程数和节点数量共同决定，其值为单节点线程数*节点数量。不过，通常需要优先调整单节点的线程数，若单机硬件资源达到了上限，则需要通过横向扩展来提高消费并发度。 单节点线程数，即单个Consumer所包含的线程数量 节点数量，即Consumer Group所包含的Consumer数量 对于普通消息、延时消息及事务消息，并发度计算都是单节点线程数*节点数量。但对于顺序消息则是不同的。顺序消息的消费并发度等于Topic的Queue分区数量。 ​ 1 ）全局顺序消息：该类型消息的Topic只有一个Queue分区。其可以保证该Topic的所有消息被顺序消费。为了保证这个全局顺序性，Consumer Group中在同一时刻只能有一个Consumer的一个线程进行消费。所以其并发度为 1 。 ​ 2 ）分区顺序消息：该类型消息的Topic有多个Queue分区。其仅可以保证该Topic的每个Queue分区中的消息被顺序消费，不能保证整个Topic中消息的顺序消费。为了保证这个分区顺序性，每个Queue分区中的消息在Consumer Group中的同一时刻只能有一个Consumer的一个线程进行消费。即，在同一时刻最多会出现多个Queue分蘖有多个Consumer的多个线程并行消费。所以其并发度为Topic的分区数量。 九、消息的清理消息被消费过后会被清理掉吗？不会的。消息是被顺序存储在commitlog文件的，且消息大小不定长，所以消息的清理是不可能以消息为单位进行清理的，而是以commitlog文件为单位进行清理的。否则会急剧下降清理效率，并实现逻辑复杂。commitlog文件存在一个过期时间，默认为 72 小时，即三天。除了用户手动清理外，在以下情况下也会被自动清理，无论文件中的消息是否被消费过： 文件过期，且到达清理时间点（默认为凌晨 4 点）后，自动清理过期文件 文件过期，且磁盘空间占用率已达过期清理警戒线（默认75%）后，无论是否达到清理时间点，都会自动清理过期文件 磁盘占用率达到清理警戒线（默认85%）后，开始按照设定好的规则清理文件，无论是否过期。 默认会从最老的文件开始清理 &#x3D;&#x3D;磁盘占用率达到系统危险警戒线（默认90%）后，Broker将拒绝消息写入&#x3D;&#x3D; 需要注意以下几点：1 ）对于RocketMQ系统来说，删除一个1G大小的文件，是一个压力巨大的IO操作。在删除过程中，系统性能会骤然下降。所以，其默认清理时间点为凌晨 4 点，访问量最小的时间。也正因如果，我们要保障磁盘空间的空闲率，不要使系统出现在其它时间点删除commitlog文件的情况。 2 ）官方建议RocketMQ服务的Linux文件系统采用ext4。因为对于文件删除操作，ext4要比ext3性能更好 第 4 章 RocketMQ应用一、普通消息1 消息发送分类Producer对于消息的发送方式也有多种选择，不同的方式会产生不同的系统效果。 同步发送消息同步发送消息是指，Producer发出一条消息后，会在收到MQ返回的ACK之后才发下一条消息。该方式的消息可靠性最高，但消息发送效率太低。 异步发送消息异步发送消息是指，Producer发出消息后无需等待MQ返回ACK，直接发送下一条消息。该方式的消息可靠性可以得到保障，消息发送效率也可以。 单向发送消息单向发送消息是指，Producer仅负责发送消息，不等待、不处理MQ的ACK。该发送方式时MQ也不返回ACK。该方式的消息发送效率最高，但消息可靠性较差。 二、顺序消息1 什么是顺序消息顺序消息指的是，严格按照消息的发送顺序进行消费的消息(FIFO)。 默认情况下生产者会把消息以Round Robin轮询方式发送到不同的Queue分区队列；而消费消息时会从多个Queue上拉取消息，这种情况下的发送和消费是不能保证顺序的。如果将消息仅发送到同一个Queue中，消费时也只从这个Queue上拉取消息，就严格保证了消息的顺序性。 2 有序性分类根据有序范围的不同，RocketMQ可以严格地保证两种消息的有序性：&#x3D;&#x3D;分区有序与全局有序&#x3D;&#x3D;。 全局有序当发送和消费参与的Queue只有一个时所保证的有序是整个Topic中消息的顺序， 称为全局有序。 在创建Topic时指定Queue的数量。有三种指定方式： 1 ）在代码中创建Producer时，可以指定其自动创建的Topic的Queue数量 2 ）在RocketMQ可视化控制台中手动创建Topic时指定Queue数量 3 ）使用mqadmin命令手动创建Topic时指定Queue数量 分区有序​ 如果有多个Queue参与，其仅可保证在该Queue分区队列上的消息顺序，则称为分区有序。 ​ 如何实现Queue的选择？在定义Producer时我们可以指定消息队列选择器，而这个选择器是我们自己实现了MessageQueueSelector接口定义的。在定义选择器的选择算法时，一般需要使用选择key。这个选择key可以是消息key也可以是其它数据。但无论谁做选择key，都不能重复，都是唯一的。 ​ 一般性的选择算法是，让选择key（或其hash值）与该Topic所包含的Queue的数量取模，其结果即为选择出的Queue的QueueId。 ​ 取模算法存在一个问题：不同选择key与Queue数量取模结果可能会是相同的，即不同选择key的消息可能会出现在相同的Queue，即同一个Consuemr可能会消费到不同选择key的消息。这个问题如何解决？一般性的作法是，从消息中获取到选择key，对其进行判断。若是当前Consumer需要消费的消息，则直接消费，否则，什么也不做。这种做法要求选择key要能够随着消息一起被Consumer获取到。此时使用消息key作为选择key是比较好的做法。 ​ 以上做法会不会出现如下新的问题呢？不属于那个Consumer的消息被拉取走了，那么应该消费该消息的Consumer是否还能再消费该消息呢？同一个Queue中的消息不可能被同一个Group中的不同Consumer同时消费。所以，消费现一个Queue的不同选择key的消息的Consumer一定属于不同的Group。而不同的Group中的Consumer间的消费是相互隔离的，互不影响的。 三、延时消息1 什么是延时消息当消息写入到Broker后，在指定的时长后才可被消费处理的消息，称为延时消息。采用RocketMQ的延时消息可以实现定时任务的功能，而无需使用定时器。典型的应用场景是，电商交易中超时未支付关闭订单的场景， 12306 平台订票超时未支付取消订票的场景。 在电商平台中，订单创建时会发送一条延迟消息。这条消息将会在 30 分钟后投递给后台业务系统（Consumer），后台业务系统收到该消息后会判断对应的订单是否已经完成支付。如果未完成，则取消订单，将商品再次放回到库存；如果完成支付，则忽略。 2 延时等级延时消息的延迟时长不支持随意时长的延迟，是通过特定的延迟等级来指定的。延时等级定义在RocketMQ服务端MessageStoreConfig类中的如下变量中： 即，若指定的延时等级为 3 ，则表示延迟时长为10s，即延迟等级是从 1 开始计数的。当然，如果需要自定义的延时等级，可以通过在broker加载的配置中新增如下配置（例如下面增加了 1天这个等级1d）。配置文件在RocketMQ安装目录下的conf目录中。 3 延时消息实现原理messageDelayLevel &#x3D; 1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m1h 2h 1d 1 具体实现方案是：修改消息​ Producer将消息发送到Broker后，Broker会首先将消息写入到commitlog文件，然后需要将其分发到相应的consumequeue。不过，在分发之前，系统会先判断消息中是否带有延时等级。若没有，则直接正常分发；若有则需要经历一个复杂的过程： ​ 修改消息的Topic为SCHEDULE_TOPIC_XXXX ​ 根据延时等级，在consumequeue目录中SCHEDULE_TOPIC_XXXX主题下创建出相应的queueId目录与consumequeue文件（如果没有这些目录与文件的话）。 ​ 延迟等级delayLevel与queueId的对应关系为queueId &#x3D; delayLevel -1 ​ 需要注意，在创建queueId目录时，并不是一次性地将所有延迟等级对应的目录全部创建完毕，而是用到哪个延迟等级创建哪个目录 ​ 修改消息索引单元内容。索引单元中的Message Tag HashCode部分原本存放的是消息的Tag的Hash值。现修改为消息的投递时间。投递时间是指该消息被重新修改为原Topic后再次被写入到commitlog中的时间。投递时间 &#x3D; 消息存储时间 + 延时等级时间。消息存储时间指的是消息被发送到Broker时的时间戳。将消息索引写入到SCHEDULE_TOPIC_XXXX主题下相应的consumequeue中 SCHEDULE_TOPIC_XXXX目录中各个延时等级Queue中的消息是如何排序的？ ​ 是按照消息投递时间排序的。一个Broker中同一等级的所有延时消息会被写入到consumequeue目录中SCHEDULE_TOPIC_XXXX目录下相同Queue中。即一个Queue中消息投递时间的延迟等级时间是相同的。那么投递时间就取决于于消息存储时间了。即按照消息被发送到Broker的时间进行排序的。 投递延时消息 ​ Broker内部有一个延迟消息服务类ScheuleMessageService，其会消费SCHEDULE_TOPIC_XXXX中的消息，即按照每条消息的投递时间，将延时消息投递到目标Topic中。不过，在投递之前会从commitlog中将原来写入的消息再次读出，并将其原来的延时等级设置为 0 ，即原消息变为了一条不延迟的普通消息。然后再次将消息投递到目标Topic中。 ​ ScheuleMessageService在Broker启动时，会创建并启动一个定时器TImer，用于执行相应的定时任务。系统会根据延时等级的个数，定义相应数量的TimerTask，每个TimerTask负责一个延迟等级消息的消费与投递。每个TimerTask都会检 测相应Queue队列的第一条消息是否到期。若第一条消息未到期，则后面的所有消息更不会到期（消息是按照投递时间排序的）；若第一条消息到期了，则将该消息投递到目标Topic，即消费该消息。 将消息重新写入commitlog 延迟消息服务类ScheuleMessageService将延迟消息再次发送给了commitlog，并再次形成新的消息索引条目，分发到相应Queue。 这其实就是一次普通消息发送。只不过这次的消息Producer是延迟消息服务类ScheuleMessageService。 四、事务消息1 问题引入​ 这里的一个需求场景是：工行用户A向建行用户B转账 1 万元。 ​ 我们可以使用同步消息来处理该需求场景： 1. 工行系统发送一个给B增款 1 万元的同步消息M给Broker2. 消息被Broker成功接收后，向工行系统发送成功ACK3. 工行系统收到成功ACK后从用户A中扣款 1 万元4. 建行系统从Broker中获取到消息M5. 建行系统消费消息M，即向用户B中增加 1 万元 ​ 这其中是有问题的：若第 3 步中的扣款操作失败，但消息已经成功发送到了Broker。对于MQ来说，只要消息写入成功，那么这个消息就可以被消费。此时建行系统中用户B增加了 1 万元。出现了数据不一致问题。 2 解决思路​ 解决思路是，让第 1 、 2 、 3 步具有原子性，要么全部成功，要么全部失败。即消息发送成功后，必须要保证扣款成功。如果扣款失败，则回滚发送成功的消息。而该思路即使用事务消息。这里要使用分布式事务解决方案。使用事务消息来处理该需求场景： 事务管理器TM向事务协调器TC发起指令，开启全局事务 工行系统发一个给B增款 1 万元的事务消息M给TC TC会向Broker发送半事务消息prepareHalf，将消息M预提交到Broker。此时的建行系统是看不到Broker中的消息M的 Broker会将预提交执行结果Report给TC。 如果预提交失败，则TC会向TM上报预提交失败的响应，全局事务结束；如果预提交成功，TC会调用工行系统的回调操作，去完成工行用户A的预扣款 1 万元的操作 工行系统会向TC发送预扣款执行结果，即本地事务的执行状态 TC收到预扣款执行结果后，会将结果上报给TM。 TM会根据上报结果向TC发出不同的确认指令 TC在接收到指令后会向Broker与工行系统发出确认指令 预扣款执行结果存在三种可能性：// 描述本地事务执行状态public enum LocalTransactionState &#123;COMMIT_MESSAGE, // 本地事务执行成功ROLLBACK_MESSAGE, // 本地事务执行失败UNKNOW, // 不确定，表示需要进行回查以确定本地事务的执行结果&#125; 8. TM会根据上报结果向TC发出不同的确认指令若预扣款成功（本地事务状态为COMMIT_MESSAGE），则TM向TC发送Global Commit指令若预扣款失败（本地事务状态为ROLLBACK_MESSAGE），则TM向TC发送Global Rollback指令若现未知状态（本地事务状态为UNKNOW），则会触发工行系统的本地事务状态回查操作。回查操作会将回查结果，即COMMIT_MESSAGE或ROLLBACK_MESSAGE Report给TC。TC将结果上报给TM，TM会再向TC发送最终确认指令Global Commit或Global Rollback TC接收的若是Global Commit指令，则向Broker与工行系统发送Branch Commit指令。此时Broker中的消息M才可被建行系统看到；此时的工行用户A中的扣款操作才真正被确认TC接收到的若是Global Rollback指令，则向Broker与工行系统发送Branch Rollback指令。此时Broker中的消息M将被撤销；工行用户A中的扣款操作将被回滚 以上方案就是为了确保消息投递与扣款操作能够在一个事务中，要成功都成功，有一个失败，则全部回滚。 以上方案并不是一个典型的XA模式。因为XA模式中的分支事务是异步的，而事务消息方案中的消息预提交与预扣款操作间是同步的。 3 基础分布式事务 对于分布式事务，通俗地说就是，一次操作由若干分支操作组成，这些分支操作分属不同应用，分布在不同服务器上。分布式事务需要保证这些分支操作要么全部成功，要么全部失败。分布式事务与普通事务一样，就是为了保证操作结果的一致性。 事务消息RocketMQ提供了类似X&#x2F;Open XA的分布式事务功能，通过事务消息能达到分布式事务的最终一致。XA是一种分布式事务解决方案，一种分布式事务处理模式。 半事务消息暂不能投递的消息，发送方已经成功地将消息发送到了Broker，但是Broker未收到最终确认指令，此时该消息被标记成“暂不能投递”状态，即不能被消费者看到。处于该种状态下的消息即半事务消息。 本地事务状态Producer回调操作执行的结果为本地事务状态，其会发送给TC，而TC会再发送给TM。TM会根据TC发送来的本地事务状态来决定全局事务确认指令。 消息回查消息回查，即重新查询本地事务的执行状态。本例就是重新到DB中查看预扣款操作是否执行成功。注意，消息回查不是重新执行回调操作。回调操作是进行预扣款操作，而消息回查则是查看预扣款操作执行的结果。 引发消息回查的原因最常见的有两个： 1)回调操作返回UNKNWON 2)TC没有接收到TM的最终全局事务确认指令 RocketMQ中的消息回查设置 关于消息回查，有三个常见的属性设置。它们都在broker加载的配置文件中设置，例如： // 描述本地事务执行状态public enum LocalTransactionState &#123;COMMIT_MESSAGE, // 本地事务执行成功ROLLBACK_MESSAGE, // 本地事务执行失败UNKNOW, // 不确定，表示需要进行回查以确定本地事务的执行结果&#125; 1 2 3 4 5 6 transactionTimeout=20，指定TM在 20 秒内应将最终确认状态发送给TC，否则引发消息回查。默认为 60 秒transactionCheckMax=5，指定最多回查 5 次，超过后将丢弃消息并记录错误日志。默认 15 次。transactionCheckInterval=10，指定设置的多次消息回查的时间间隔为 10 秒。默认为 60 秒。 4 XA模式三剑客XA协议XA（Unix Transaction）是一种分布式事务解决方案，一种分布式事务处理模式，是基于XA协议的。XA协议由Tuxedo（Transaction for Unix has been Extended for Distributed Operation，分布式操作扩展之后的Unix事务系统）首先提出的，并交给X&#x2F;Open组织，作为资源管理器与事务管理器的接口标准。 XA模式中有三个重要组件：TC、TM、RM。TC Transaction Coordinator，事务协调者。维护全局和分支事务的状态，驱动全局事务提交或回滚。RocketMQ中Broker充当着TC。 TMTransaction Manager，事务管理器。定义全局事务的范围：开始全局事务、提交或回滚全局事务。它实际是全局事务的发起者。RocketMQ中事务消息的Producer充当着TM。 RMResource Manager，资源管理器。管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。RocketMQ中事务消息的Producer及Broker均是RM。 5 XA模式架构XA模式是一个典型的2PC，其执行原理如下： TM向TC发起指令，开启一个全局事务。 根据业务要求，各个RM会逐个向TC注册分支事务，然后TC会逐个向RM发出预执行指令。 各个RM在接收到指令后会在进行本地事务预执行。 RM将预执行结果Report给TC。当然，这个结果可能是成功，也可能是失败。 TC在接收到各个RM的Report后会将汇总结果上报给TM，根据汇总结果TM会向TC发出确认指令。若所有结果都是成功响应，则向TC发送Global Commit指令。只要有结果是失败响应，则向TC发送Global Rollback指令。 TC在接收到指令后再次向RM发送确认指令。事务消息方案并不是一个典型的XA模式。因为XA模式中的分支事务是异步的，而事务消息方案中的消息预提交与预扣款操作间是同步的。 6 注意事务消息不支持延时消息 对于事务消息要做好幂等性检查，因为事务消息可能不止一次被消费（因为存在回滚后再提交的情况） 7 代码举例定义工行事务监听器 public class ICBCTransactionListener implements TransactionListener &#123;// 回调操作方法// 消息预提交成功就会触发该方法的执行，用于完成本地事务@Overridepublic LocalTransactionState executeLocalTransaction(Message msg,Object arg) &#123;System.out.println(&quot;预提交消息成功：&quot; + msg);// 假设接收到TAGA的消息就表示扣款操作成功，TAGB的消息表示扣款失败，// TAGC表示扣款结果不清楚，需要执行消息回查if (StringUtils.equals(&quot;TAGA&quot;, msg.getTags())) &#123;return LocalTransactionState.COMMIT_MESSAGE;&#125; else if (StringUtils.equals(&quot;TAGB&quot;, msg.getTags())) &#123;return LocalTransactionState.ROLLBACK_MESSAGE;&#125; else if (StringUtils.equals(&quot;TAGC&quot;, msg.getTags())) &#123;return LocalTransactionState.UNKNOW;&#125;return LocalTransactionState.UNKNOW;&#125; // 消息回查方法// 引发消息回查的原因最常见的有两个：// 1)回调操作返回UNKNWON// 2)TC没有接收到TM的最终全局事务确认指令@Overridepublic LocalTransactionState checkLocalTransaction(MessageExt msg) &#123;System.out.println(&quot;执行消息回查&quot; + msg.getTags());return LocalTransactionState.COMMIT_MESSAGE;&#125;&#125; 定义事物消息生产者 public class TransactionProducer &#123;public static void main(String[] args) throws Exception &#123;TransactionMQProducer producer = newTransactionMQProducer(&quot;tpg&quot;);producer.setNamesrvAddr(&quot;rocketmqOS:9876&quot;); /*** 定义一个线程池* @param corePoolSize 线程池中核心线程数量* @param maximumPoolSize 线程池中最多线程数* @param keepAliveTime 这是一个时间。当线程池中线程数量大于核心线程数量是，直接使用普通消息的SomeConsumer作为消费者即可。* 多余空闲线程的存活时长* @param unit 时间单位* @param workQueue 临时存放任务的队列，其参数就是队列的长度* @param threadFactory 线程工厂*/ExecutorService executorService = new ThreadPoolExecutor( 2 , 5 ,100 , TimeUnit.SECONDS,new ArrayBlockingQueue&lt;Runnable&gt;( 2000 ), newThreadFactory() &#123;@Overridepublic Thread newThread(Runnable r) &#123;Thread thread = new Thread(r);thread.setName(&quot;client-transaction-msg-check-thread&quot;);return thread;&#125;&#125;); 定义消费者// 为生产者指定一个线程池producer.setExecutorService(executorService);// 为生产者添加事务监听器producer.setTransactionListener(new ICBCTransactionListener()); producer.start(); String[] tags = &#123;&quot;TAGA&quot;,&quot;TAGB&quot;,&quot;TAGC&quot;&#125;;for (int i = 0 ; i &lt; 3 ; i++) &#123;byte[] body = (&quot;Hi,&quot; + i).getBytes();Message msg = new Message(&quot;TTopic&quot;, tags[i], body);// 发送事务消息// 第二个参数用于指定在执行本地事务时要使用的业务参数SendResult sendResult =producer.sendMessageInTransaction(msg,null);System.out.println(&quot;发送结果为：&quot; +sendResult.getSendStatus());&#125;&#125;&#125; public class SomeConsumer &#123; public static void main(String[] args) throws MQClientException &#123;// 定义一个pull消费者 // DefaultLitePullConsumer consumer = newDefaultLitePullConsumer(&quot;cg&quot;);// 定义一个push消费者DefaultMQPushConsumer consumer = newDefaultMQPushConsumer(&quot;cg&quot;);// 指定nameServerconsumer.setNamesrvAddr(&quot;rocketmqOS:9876&quot;);// 指定从第一条消息开始消费 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET);// 指定消费topic与tagconsumer.subscribe(&quot;TTopic&quot;, &quot;*&quot;);// 指定采用“广播模式”进行消费，默认为“集群模式”// consumer.setMessageModel(MessageModel.BROADCASTING); // 注册消息监听器consumer.registerMessageListener(newMessageListenerConcurrently() &#123; // 一旦broker中有了其订阅的消息就会触发该方法的执行，// 其返回值为当前consumer消费的状态@Overridepublic ConsumeConcurrentlyStatusconsumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123;// 逐条消费消息for (MessageExt msg : msgs) &#123;System.out.println(msg);&#125;// 返回消费状态：消费成功return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;&#125;&#125;); // 开启消费者消费consumer.start();System.out.println(&quot;Consumer Started&quot;);&#125;&#125; 五、批量消息1 批量发送消息发送限制生产者进行消息发送时可以一次发送多条消息，这可以大大提升Producer的发送效率。不过需要注意以下几点： ​ 批量发送的消息必须具有相同的Topic​ 批量发送的消息必须具有相同的刷盘策略​ 批量发送的消息不能是延时消息与事务消息 批量发送大小默认情况下，一批发送的消息总大小不能超过4MB字节。如果想超出该值，有两种解决方案： 方案一：将批量消息进行拆分，拆分为若干不大于4M的消息集合分多次批量发送 方案二：在Producer端与Broker端修改属性 ​ Producer端需要在发送之前设置Producer的maxMessageSize属性 ​ Broker端需要修改其加载的配置文件中的maxMessageSize属性 生产者发送的消息大小生产者通过send()方法发送的Message，并不是直接将Message序列化后发送到网络上的，而是通过这个Message生成了一个字符串发送出去的。这个字符串由四部分构成：Topic、消息Body、消息日志（占 20 字节），及用于描述消息的一堆属性key-value。这些属性中包含例如生产者地址、生产时间、要发送的QueueId等。最终写入到Broker中消息单元中的数据都是来自于这些属性。 2 批量消费消息修改批量属性Consumer的MessageListenerConcurrently监听接口的consumeMessage()方法的第一个参数为消息列表，但默认情况下每次只能消费一条消息。若要使其一次可以消费多条消息，则可以通过修改Consumer的consumeMessageBatchMaxSize属性来指定。不过，该值不能超过 32 。因为默认情况下消费者每次可以拉取的消息最多是 32 条。若要修改一次拉取的最大值，则可通过修改Consumer的pullBatchSize属性来指定。 存在的问题Consumer的pullBatchSize属性与consumeMessageBatchMaxSize属性是否设置的越大越好？当然不是。 pullBatchSize值设置的越大，Consumer每拉取一次需要的时间就会越长，且在网络上传输出现问题的可能性就越高。若在拉取过程中若出现了问题，那么本批次所有消息都需要全部重新拉取。consumeMessageBatchMaxSize值设置的越大，Consumer的消息并发消费能力越低，且这批被消费的消息具有相同的消费结果。因为consumeMessageBatchMaxSize指定的一批消息只会使用一个线程进行处理，且在处理过程中只要有一个消息处理异常，则这批消息需要全部重新再次消费处理。 3 代码举例该批量发送的需求是，不修改最大发送4M的默认值，但要防止发送的批量消息超出4M的限制。定义消息列表分割器 // 消息列表分割器：其只会处理每条消息的大小不超4M的情况。// 若存在某条消息，其本身大小大于4M，这个分割器无法处理，// 其直接将这条消息构成一个子列表返回。并没有再进行分割public class MessageListSplitter implements Iterator&lt;List&lt;Message&gt;&gt; &#123;// 指定极限值为4Mprivate final int SIZE_LIMIT = 4 * 1024 * 1024 ;// 存放所有要发送的消息private final List&lt;Message&gt; messages;// 要进行批量发送消息的小集合起始索引private int currIndex;public MessageListSplitter(List&lt;Message&gt; messages) &#123;this.messages = messages;&#125; @Overridepublic boolean hasNext() &#123;// 判断当前开始遍历的消息索引要小于消息总数return currIndex &lt; messages.size();&#125; @Overridepublic List&lt;Message&gt; next() &#123;int nextIndex = currIndex;// 记录当前要发送的这一小批次消息列表的大小int totalSize = 0 ;for (; nextIndex &lt; messages.size(); nextIndex++) &#123;// 获取当前遍历的消息Message message = messages.get(nextIndex); // 统计当前遍历的message的大小int tmpSize = message.getTopic().length() +message.getBody().length;Map&lt;String, String&gt; properties = message.getProperties();for (Map.Entry&lt;String, String&gt; entry :properties.entrySet()) &#123;tmpSize += entry.getKey().length() +entry.getValue().length();&#125;tmpSize = tmpSize + 20 ; // 判断当前消息本身是否大于4Mif (tmpSize &gt; SIZE_LIMIT) &#123;if (nextIndex - currIndex == 0 ) &#123;nextIndex++;&#125;break;&#125; if (tmpSize + totalSize &gt; SIZE_LIMIT) &#123;break;&#125; else &#123;totalSize += tmpSize;&#125; &#125; // end-for // 获取当前messages列表的子集合[currIndex, nextIndex)List&lt;Message&gt; subList = messages.subList(currIndex, nextIndex);// 下次遍历的开始索引currIndex = nextIndex;return subList;&#125;&#125; 定义批量消息生产者public class BatchProducer &#123;public static void main(String[] args) throws Exception &#123;DefaultMQProducer producer = new DefaultMQProducer(&quot;pg&quot;);producer.setNamesrvAddr(&quot;rocketmqOS:9876&quot;);// 指定要发送的消息的最大大小，默认是4M// 不过，仅修改该属性是不行的，还需要同时修改broker加载的配置文件中的// maxMessageSize属性// producer.setMaxMessageSize(8 * 1024 * 1024);producer.start(); // 定义要发送的消息集合List&lt;Message&gt; messages = new ArrayList&lt;&gt;();for (int i = 0 ; i &lt; 100 ; i++) &#123;byte[] body = (&quot;Hi,&quot; + i).getBytes();Message msg = new Message(&quot;someTopic&quot;, &quot;someTag&quot;, body);messages.add(msg);&#125; // 定义消息列表分割器，将消息列表分割为多个不超出4M大小的小列表MessageListSplitter splitter = newMessageListSplitter(messages);while (splitter.hasNext()) &#123;try &#123;List&lt;Message&gt; listItem = splitter.next();producer.send(listItem);&#125; catch (Exception e) &#123;e.printStackTrace();&#125;&#125;producer.shutdown();&#125;&#125; 定义批量消息消费者public class BatchConsumer &#123;public static void main(String[] args) throws MQClientException &#123;DefaultMQPushConsumer consumer = newDefaultMQPushConsumer(&quot;cg&quot;);consumer.setNamesrvAddr(&quot;rocketmqOS:9876&quot;); 六、消息过滤消息者在进行消息订阅时，除了可以指定要订阅消息的Topic外，还可以对指定Topic中的消息根据指定条件进行过滤，即可以订阅比Topic更加细粒度的消息类型。对于指定Topic消息的过滤有两种过滤方式：Tag过滤与SQL过滤。 1 Tag过滤通过consumer的subscribe()方法指定要订阅消息的Tag。如果订阅多个Tag的消息，Tag间使用或运算符(双竖线||)连接。 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET);consumer.subscribe(&quot;someTopicA&quot;, &quot;*&quot;); // 指定每次可以消费 10 条消息，默认为 1consumer.setConsumeMessageBatchMaxSize( 10 );// 指定每次可以从Broker拉取 40 条消息，默认为 32consumer.setPullBatchSize( 40 ); consumer.registerMessageListener(newMessageListenerConcurrently() &#123; @Overridepublic ConsumeConcurrentlyStatusconsumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123;for (MessageExt msg : msgs) &#123;System.out.println(msg);&#125;// 消费成功的返回结果return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;// 消费异常时的返回结果// return ConsumeConcurrentlyStatus.RECONSUME_LATER;&#125;&#125;); consumer.start();System.out.println(&quot;Consumer Started&quot;);&#125;&#125; 2 SQL过滤SQL过滤是一种通过特定表达式对事先埋入到消息中的用户属性进行筛选过滤的方式。通过SQL过滤，可以实现对消息的复杂过滤。不过，只有使用PUSH模式的消费者才能使用SQL过滤。SQL过滤表达式中支持多种常量类型与运算符。 支持的常量类型：数值：比如： 123 ，3.1415 字符：必须用单引号包裹起来，比如：’abc’布尔：TRUE 或 FALSENULL：特殊的常量，表示空 支持的运算符有：数值比较：&gt;，&gt;&#x3D;，&lt;，&lt;&#x3D;，BETWEEN，&#x3D;字符比较：&#x3D;，&lt;&gt;，IN逻辑运算 ：AND，OR，NOTNULL判断：IS NULL 或者 IS NOT NULL默认情况下Broker没有开启消息的SQL过滤功能，需要在Broker加载的配置文件中添加如下属性，以开启该功能： 在启动Broker时需要指定这个修改过的配置文件。例如对于单机Broker的启动，其修改的配置文件是conf&#x2F;broker.conf，启动时使用如下命令： 3 代码举例定义Tag过滤Producer DefaultMQPushConsumer consumer = newDefaultMQPushConsumer(&quot;CID_EXAMPLE&quot;);consumer.subscribe(&quot;TOPIC&quot;, &quot;TAGA || TAGB || TAGC&quot;); 1 2 1 enablePropertyFilter = true 1 sh bin/mqbroker -n localhost:9876 -c conf/broker.conf &amp; public class FilterByTagProducer &#123;public static void main(String[] args) throws Exception &#123;DefaultMQProducer producer = new DefaultMQProducer(&quot;pg&quot;);producer.setNamesrvAddr(&quot;rocketmqOS:9876&quot;);producer.start();String[] tags = &#123;&quot;myTagA&quot;,&quot;myTagB&quot;,&quot;myTagC&quot;&#125;;for (int i = 0 ; i &lt; 10 ; i++) &#123; 1 2 3 4 5 6 7 定义Tag过滤Consumer 定义SQL过滤Producer byte[] body = (&quot;Hi,&quot; + i).getBytes();String tag = tags[i%tags.length];Message msg = new Message(&quot;myTopic&quot;,tag,body);SendResult sendResult = producer.send(msg);System.out.println(sendResult);&#125;producer.shutdown();&#125;&#125; 8910111213141516 public class FilterByTagConsumer &#123; public static void main(String[] args) throws Exception &#123;DefaultMQPushConsumer consumer = newDefaultMQPushConsumer(&quot;pg&quot;);consumer.setNamesrvAddr(&quot;rocketmqOS:9876&quot;); consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); consumer.subscribe(&quot;myTopic&quot;, &quot;myTagA || myTagB&quot;);consumer.registerMessageListener(newMessageListenerConcurrently() &#123;@Overridepublic ConsumeConcurrentlyStatusconsumeMessage(List&lt;MessageExt&gt; msgs,ConsumeConcurrentlyContext context) &#123;for (MessageExt me:msgs)&#123;System.out.println(me);&#125;return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;&#125;&#125;);consumer.start();System.out.println(&quot;Consumer Started&quot;);&#125;&#125; 1 2 3 4 5 6 7 8 9 1011 121314151617181920212223 public class FilterBySQLProducer &#123;public static void main(String[] args) throws Exception &#123;DefaultMQProducer producer = new DefaultMQProducer(&quot;pg&quot;);producer.setNamesrvAddr(&quot;rocketmqOS:9876&quot;); 1234 定义SQL过滤Consumer producer.start(); for (int i = 0 ; i &lt; 10 ; i++) &#123;try &#123;byte[] body = (&quot;Hi,&quot; + i).getBytes();Message msg = new Message(&quot;myTopic&quot;, &quot;myTag&quot;, body);msg.putUserProperty(&quot;age&quot;, i + &quot;&quot;);SendResult sendResult = producer.send(msg);System.out.println(sendResult);&#125; catch (Exception e) &#123;e.printStackTrace();&#125;&#125;producer.shutdown();&#125;&#125; 56789101112131415161718192021 public class FilterBySQLConsumer &#123; public static void main(String[] args) throws Exception &#123;DefaultMQPushConsumer consumer = newDefaultMQPushConsumer(&quot;pg&quot;);consumer.setNamesrvAddr(&quot;rocketmqOS:9876&quot;); consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); consumer.subscribe(&quot;myTopic&quot;, MessageSelector.bySql(&quot;age between0 and 6&quot;)); consumer.registerMessageListener(newMessageListenerConcurrently() &#123;@Overridepublic ConsumeConcurrentlyStatusconsumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContextcontext) &#123;for (MessageExt me:msgs)&#123;System.out.println(me);&#125;return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;&#125;&#125;);consumer.start();System.out.println(&quot;Consumer Started&quot;); 1 2 3 4 5 6 7 8 9 10 1112 1314151617181920 七、消息发送重试机制1 说明Producer对发送失败的消息进行重新发送的机制，称为消息发送重试机制，也称为消息重投机制。 对于消息重投，需要注意以下几点：生产者在发送消息时，若采用同步或异步发送方式，发送失败会重试，但oneway消息发送方式发送失败是没有重试机制的只有普通消息具有发送重试机制，顺序消息是没有的消息重投机制可以保证消息尽可能发送成功、不丢失，但可能会造成消息重复。消息重复在RocketMQ中是无法避免的问题消息重复在一般情况下不会发生，当出现消息量大、网络抖动，消息重复就会成为大概率事件producer主动重发、consumer负载变化（发生Rebalance，不会导致消息重复，但可能出现重复消费）也会导致重复消息消息重复无法避免，但要避免消息的重复消费。避免消息重复消费的解决方案是，为消息添加唯一标识（例如消息key），使消费者对消息进行消费判断来避免重复消费消息发送重试有三种策略可以选择：同步发送失败策略、异步发送失败策略、消息刷盘失败策略 2 同步发送失败策略对于普通消息，消息发送默认采用round-robin策略来选择所发送到的队列。如果发送失败，默认重试 2次。但在重试时是不会选择上次发送失败的Broker，而是选择其它Broker。当然，若只有一个Broker其也只能发送到该Broker，但其会尽量发送到该Broker上的其它Queue。 同时，Broker还具有失败隔离功能，使Producer尽量选择未发生过发送失败的Broker作为目标 Broker。其可以保证其它消息尽量不发送到问题Broker，为了提升消息发送效率，降低消息发送耗时。 思考：让我们自己实现失败隔离功能，如何来做？ &#125;&#125; 212223 // 创建一个producer，参数为Producer Group名称DefaultMQProducer producer = new DefaultMQProducer(&quot;pg&quot;);// 指定nameServer地址producer.setNamesrvAddr(&quot;rocketmqOS:9876&quot;);// 设置同步发送失败时重试发送的次数，默认为 2 次producer.setRetryTimesWhenSendFailed( 3 );// 设置发送超时时限为5s，默认3sproducer.setSendMsgTimeout( 5000 ); 1 2 3 4 5 6 7 8 1 ）方案一：Producer中维护某JUC的Map集合，其key是发生失败的时间戳，value为Broker实例。Producer中还维护着一个Set集合，其中存放着所有未发生发送异常的Broker实例。选择目标Broker是从该Set集合中选择的。再定义一个定时任务，定期从Map集合中将长期未发生发送异常的Broker清理出去，并添加到Set集合。 2 ）方案二：为Producer中的Broker实例添加一个标识，例如是一个AtomicBoolean属性。只要该Broker上发生过发送异常，就将其置为true。选择目标Broker就是选择该属性值为false的Broker。再定义一个定时任务，定期将Broker的该属性置为false。 3 ）方案三：为Producer中的Broker实例添加一个标识，例如是一个AtomicLong属性。只要该Broker上发生过发送异常，就使其值增一。选择目标Broker就是选择该属性值最小的Broker。若该值相同，采用轮询方式选择。 如果超过重试次数，则抛出异常，由Producer去保证消息不丢。当然当生产者出现RemotingException、MQClientException和MQBrokerException时，Producer会自动重投消息。 3 异步发送失败策略异步发送失败重试时，异步重试不会选择其他broker，仅在同一个broker上做重试，所以该策略无法保证消息不丢。 4 消息刷盘失败策略消息刷盘超时（Master或Slave）或slave不可用（slave在做数据同步时向master返回状态不是SEND_OK）时，默认是不会将消息尝试发送到其他Broker的。不过，对于重要消息可以通过在Broker的配置文件设置retryAnotherBrokerWhenNotStoreOK属性为true来开启。 八、消息消费重试机制1 顺序消息的消费重试对于顺序消息，当Consumer消费消息失败后，为了保证消息的顺序性，其会自动不断地进行消息重试，直到消费成功。消费重试默认间隔时间为 1000 毫秒。重试期间应用会出现消息消费被阻塞的情况。 DefaultMQProducer producer = new DefaultMQProducer(&quot;pg&quot;);producer.setNamesrvAddr(&quot;rocketmqOS:9876&quot;);// 指定异步发送失败后不进行重试发送producer.setRetryTimesWhenSendAsyncFailed( 0 ); 1234 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;cg&quot;);// 顺序消息消费失败的消费重试时间间隔，单位毫秒，默认为 1000 ，其取值范围为[10,30000]consumer.setSuspendCurrentQueueTimeMillis( 100 ); 12 3 重试次数 与上次重试的间隔时间 重试次数 与上次重试的间隔时间1 10 秒 9 7 分钟2 30 秒 10 8 分钟3 1 分钟 11 9 分钟4 2 分钟 12 10 分钟5 3 分钟 13 20 分钟6 4 分钟 14 30 分钟7 5 分钟 15 1 小时8 6 分钟 16 2 小时由于对顺序消息的重试是无休止的，不间断的，直至消费成功，所以，对于顺序消息的消费，务必要保证应用能够及时监控并处理消费失败的情况，避免消费被永久性阻塞。注意，顺序消息没有发送失败重试机制，但具有消费失败重试机制2 无序消息的消费重试对于无序消息（普通消息、延时消息、事务消息），当Consumer消费消息失败时，可以通过设置返回状态达到消息重试的效果。不过需要注意，无序消息的重试只对集群消费方式生效，广播消费方式不 提供失败重试特性。即对于广播消费，消费失败后，失败消息不再重试，继续消费后续消息。 3 消费重试次数与间隔对于无序消息集群消费下的重试消费，每条消息默认最多重试 16 次，但每次重试的间隔时间是不同的，会逐渐变长。每次重试的间隔时间如下表。若一条消息在一直消费失败的前提下，将会在正常消费后的第 4 小时 46 分后进行第 16 次重试。若仍然失败，则将消息投递到死信队列修改消费重试次数对于修改过的重试次数，将按照以下策略执行：若修改值小于 16 ，则按照指定间隔进行重试若修改值大于 16 ，则超过 16 次的重试时间间隔均为 2 小时对于Consumer Group，若仅修改了一个Consumer的消费重试次数，则会应用到该Group中所有其它Consumer实例。若出现多个Consumer均做了修改的情况，则采用覆盖方式生效。即最后被修改的值会覆盖前面设置的值。 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;cg&quot;);// 修改消费重试次数consumer.setMaxReconsumeTimes( 10 ); 123 4 重试队列对于需要重试消费的消息，并不是Consumer在等待了指定时长后再次去拉取原来的消息进行消费，而是将这些需要重试消费的消息放入到了一个特殊Topic的队列中，而后进行再次消费的。这个特殊的队列就是重试队列。 当出现需要进行重试消费的消息时，Broker会为每个消费组都设置一个Topic名称为%RETRY%consumerGroup@consumerGroup的重试队列。 1 ）这个重试队列是针对消息才组的，而不是针对每个Topic设置的（一个Topic的消息可以让多个消费者组进行消费，所以会为这些消费者组各创建一个重试队列） 2 ）只有当出现需要进行重试消费的消息时，才会为该消费者组创建重试队列注意，消费重试的时间间隔与延时消费的延时等级十分相似，除了没有延时等级的前两个时间外，其它的时间都是相同的Broker对于重试消息的处理是通过延时消息实现的。先将消息保存到SCHEDULE_TOPIC_XXXX延迟队 列中，延迟时间到后，会将消息投递到%RETRY%consumerGroup@consumerGroup重试队列中。 5 消费重试配置方式集群消费方式下，消息消费失败后若希望消费重试，则需要在消息监听器接口的实现中明确进行如下三种方式之一的配置：方式 1 ：返回ConsumeConcurrentlyStatus.RECONSUME_LATER（推荐）方式 2 ：返回Null方式 3 ：抛出异常 6 消费不重试配置方式集群消费方式下，消息消费失败后若不希望消费重试，则在捕获到异常后同样也返回与消费成功后的相同的结果，即ConsumeConcurrentlyStatus.CONSUME_SUCCESS，则不进行消费重试。 九、死信队列1 什么是死信队列当一条消息初次消费失败，消息队列会自动进行消费重试；达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息，此时，消息队列不会立刻将消息丢弃，而是将其发送到该消费者对应的特殊队列中。这个队列就是死信队列（Dead-Letter Queue，DLQ），而其中的消息则称为死信消息（Dead-Letter Message，DLM）。 死信队列是用于处理无法被正常消费的消息的。2 死信队列的特征死信队列具有如下特征：死信队列中的消息不会再被消费者正常消费，即DLQ对于消费者是不可见的 死信存储有效期与正常消息相同，均为 3 天（commitlog文件的过期时间）， 3 天后会被自动删除死信队列就是一个特殊的Topic，名称为%DLQ%consumerGroup@consumerGroup，即每个消费者组都有一个死信队列如果一个消费者组未产生死信消息，则不会为其创建相应的死信队列 3 死信消息的处理实际上，当一条消息进入死信队列，就意味着系统中某些地方出现了问题，从而导致消费者无法正常消费该消息，比如代码中原本就存在Bug。因此，对于死信消息，通常需要开发人员进行特殊处理。最关键的步骤是要排查可疑因素，解决代码中可能存在的Bug，然后再将原来的死信消息再次进行投递消费。","categories":["RocketMQ"]},{"title":"设计模式","path":"/2023/11/27/设计模式/设计模式/","content":"设计原则设计模式七大原则 单一职责原则、接口隔离原则、依赖倒置原则、里氏替换原则、开闭原则、迪米特法则、合成复用原则 单一职责原则​ 对类来说，即一个类应该只负责一项职责。如类A负责两个不同职责：职责1，职责2。当职责1需求变更而改变A时，可能造成职责2执行错误，所以需要将类A的粒度分解为A1，A2。 单一职责原则注意事项和细节 1）降低类的复杂度，一个类只负责一项职责 2）提高类的可读性，可维护性 3）降低变更引起的风险 接口隔离原则​ 客户端不应该依赖他它需要的接口，即一个类对另一个类对依赖应该建立在最小的接口上 应该将Interface1拆分为独立的几个接口，类A和C分别与他们需要的接口建立依赖关系。 依赖倒置原则 高层模块不应该依赖低层模块，二者都应该依赖其抽象 抽象不应该依赖细节，细节应该依赖抽象 依赖倒置的中心思想是面向接口编程 依赖关系传递的三种方式 接口传递 构造方法传递 setter方式传递 package com.atguigu.principle.inversion.improve;public class DependencyPass &#123;public static void main(String[] args) &#123;// TODO Auto-generated method stubChangHong changHong = new ChangHong();// OpenAndClose openAndClose = new OpenAndClose();// openAndClose.open(changHong);//通过构造器进行依赖传递// OpenAndClose openAndClose = new OpenAndClose(changHong);// openAndClose.open();//通过 setter 方法进行依赖传递OpenAndClose openAndClose = new OpenAndClose();openAndClose.setTv(changHong);openAndClose.open();&#125;&#125;// 方式 1： 通过接口传递实现依赖// 开关的接口// interface IOpenAndClose &#123;// public void open(ITV tv); //抽象方法,接收接口// &#125;//// interface ITV &#123; //ITV 接口// public void play();// &#125;//// class ChangHong implements ITV &#123;//// @Override// public void play() &#123;// // TODO Auto-generated method stub// System.out.println(&quot;长虹电视机，打开&quot;);// &#125;//// &#125;//// 实现接口// class OpenAndClose implements IOpenAndClose&#123;// public void open(ITV tv)&#123;// tv.play();// &#125;// &#125;// 方式 2: 通过构造方法依赖传递// interface // public void open(); //抽象方法// &#125;// interface ITV &#123; //ITV 接口// public void play();// &#125;// class OpenAndClose implements IOpenAndClose&#123;// public ITV tv; //成员// public OpenAndClose(ITV tv)&#123; //构造器// this.tv = tv;// &#125;// public void open()&#123;// this.tv.play();// &#125;// &#125;// 方式 3 , 通过 setter 方法传递interface IOpenAndClose &#123;public void open(); // 抽象方法public void setTv(ITV tv);&#125;interface ITV &#123; // ITV 接口public void play();&#125;class OpenAndClose implements IOpenAndClose &#123;private ITV tv;public void setTv(ITV tv) &#123;this.tv = tv;&#125;public void open() &#123;this.tv.play();&#125;&#125;class ChangHong implements ITV &#123;@Overridepublic void play() &#123;// TODO Auto-generated method stubSystem.out.println(&quot;长虹电视机，打开&quot;);&#125;&#125; 迪米特法则 一个对象应该对其他对象保持最少的了解 类与类关系越密切，耦合度越大 迪米特法则(Demeter Principle)又叫最少知道原则，即一个类对自己依赖的类知道的越少越好。也就是说，对于被依赖的类不管多么复杂，都尽量将逻辑封装在类的内部。对外除了提供的 public 方法，不对外泄露任何信息 迪米特法则还有个更简单的定义：只与直接的朋友通信 直接的朋友：每个对象都会与其他对象有耦合关系，只要两个对象之间有耦合关系，我们就说这两个对象之间是朋友关系。耦合的方式很多，依赖，关联，组合，聚合等。其中，我们称出现成员变量，方法参数，方法返回值中的类为直接的朋友，而出现在局部变量中的类不是直接的朋友。也就是说，陌生的类最好不要以局部变量的形式出现在类的内部。 UML 用来描述系统中的类（对象）本身的组成和类（对象）之间的各种静态关系 类之间的关系：依赖、泛化（继承）、实现、关联、聚合和组合 设计模式设计模式分为三种类型，共23种 创建型模式： 单例模式、抽象工厂模式、原型模式、建造者模式、工厂模式 对象的创建由相关的工厂来完成（各种工厂模式） 对象的创建由一个建造者来完成（建造者模式） 对象的创建由原来的对象克隆完成（原型模式） 对象在系统中始终只有一个实例（单例模式） 结构型模式：适配器模式、桥接模式、装饰模式、组合模式、外观模式、亨元模式、代理模式 行为型模式：模版方法模式、命令模式、访问者模式、迭代器模式、观察者模式、中介者模式、备忘录模式、解释器模式、状态模式、策略模式、责任链模式 创建型模式单例模式​ 采取一定的方法保证在整个软件系统中，对某个类只能存在一个对象实例，并且该类只提供一个取得其对象实例的方法（静态方法）。 ​ 一个单一的类，负责创建自己的对象，同时确保系统中只有单个对象被创建 ​ 单例特点 某个类只有一个实例（构造器私有） 它必须自己创建这个实例（自己编写实例化逻辑） 它必须自行向整个系统提供这个实例（对外提供实例化方法） 1.饿汉式（静态常量）//饿汉式（静态变量）class Person&#123; //1.构造器私有化（防止new） private Person()&#123; &#125; //2.本类内部创建对象实例 private final static Person instance = new Person(); //3.提供一个共有的静态方法，返回实例对象 public static Person getInstance()&#123; return instance; &#125;&#125; 优缺点 ​\t1）优点：在类装载时就完成了初始化，避免了线程同步问题。 ​\t2）缺点：没有达到lazy loading的效果，如果从始至终没有使用这个实例，会造成内存浪费。 2.饿汉式（静态代码块）class Person&#123; //1.构造器私有化 private Person()&#123; &#125; //2.本类内部创建对象实例 private static Person instance; //在静态代码块中创建代理对象 static &#123; instance = new Person(); &#125; //3.提供一个共有的静态方法，返回实例对象 public Person getInstance()&#123; return instance; &#125;&#125; 3.懒汉式（线程不安全）- 不推荐使用class Person&#123; private static Person instance; private Person()&#123; &#125; //提供一个静态的共有方法，当使用到该方法时，才去创建instance //即懒汉式 public static Person getInstance()&#123; if (instance == null)&#123; instance = new Person(); &#125; return instance; &#125;&#125; 优缺点 1）实现了lazy loading，但是只能在单线程下使用 2）如果在多线程下，如果一个线程进入了if判断，还未来得及往下执行，另一个线程也通过了这个判断语句，这时会产生多个实例 4.懒汉式（线程安全）- 不推荐使用class Singleton&#123; private static Singleton instance; private Singleton()&#123; &#125; //提供一个静态的共有方法，当使用到该方法时，才去创建instance //即懒汉式 public static synchronized Singleton getInstance()&#123; if (instance == null)&#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 5.双重检查-实际开发推荐class Singleton&#123; private static volatile Singleton instance; private Singleton()&#123; &#125; //提供一个静态的共有方法，加入双重检查代码，解决线程问题，同时解决懒加载问题 //即懒汉式 public static synchronized Singleton getInstance()&#123; if (instance == null)&#123; synchronized (Singleton.class)&#123; if (instance == null)&#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 优缺点： 1）双重检查在多线程常用，进行两次if判读，可以保证线程安全 2）实例化代码只用执行一次，后面再次访问时，判断if成立，直接return实例化对象 6.静态内部类class Singleton&#123; private Singleton()&#123; &#125; //写一个静态内部类，该类中有一个静态属性 Singleton private static class SingletonInstance&#123; private static final Singleton INSTANCE = new Singleton(); &#125; //提供一个静态共有方法，直接返回SingletonInstance.INSTANCE public static synchronized Singleton getInstance()&#123; return SingletonInstance.INSTANCE; &#125;&#125; 7.枚举推荐使用 总结1）单例模式保证了系统内存中只存在一个对象，节省了系统资源，对于一些需要频繁创建销毁的对象，使用单例模式可以提高系统性能 2）当想实例化一个单例类的时候，必须要记住使用相应的获取对象的方法，而不是使用new 3）使用场景：需要频繁进行创建销毁的对象、创建对象时耗时过多或耗费资源过多但又经常用到的对象 工厂模式简单工厂模式1.简单工厂模式是属于创建型模式，是工厂模式的一种。简单工厂模式是由一个工厂对象决定创建出哪一种产品类的实例。简单工厂模式是工厂模式家族中最简单实用的模式。 2.简单工厂模式：定义了一个创建对象的类，由这个类来封装实例化对象的行为(代码) 3.在软件开发中，当我们会用到大量的创建某种、某类或者某批对象时，就会使用到工厂模式. //将pizza类做成抽象的public abstract class Pizza &#123; protected String name; //准备原材料，不同的披萨不一样，所以用抽象方法 public abstract void prepare(); public void bake() &#123; System.out.println(name + &quot; baking;&quot;); &#125; public void cut() &#123; System.out.println(name + &quot; cutting;&quot;); &#125; public void box() &#123; System.out.println(name + &quot; boxing;&quot;); &#125; public void setName(String name) &#123; this.name = name; &#125;&#125;/** * @Author: dongnan * @CreateTime: 2023/01/09 11:35 * 简单工厂类 */public class SimpleFactory &#123; public Pizza createPizza(String orderType)&#123; System.out.println(&quot;使用简单工厂模式&quot;); Pizza pizza = null; if (orderType.equals(&quot;greek&quot;))&#123; pizza = new GreekPizza(); pizza.setName(orderType); &#125; else if (orderType.equals(&quot;chess&quot;))&#123; pizza = new ChessPizza(); pizza.setName(orderType); &#125; //输出pizza制作过程 pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); return pizza; &#125;&#125; 工厂方法模式工厂方法模式设计方案：将披萨项目的实例化功能抽象成抽象方法，在不同的口味点餐子类中具体实现。 工厂方法模式：定义了一个创建对象的抽象方法，由子类决定要实例化的类。工厂方法模式将对象的实例化推迟到子类。 public abstract class OrderPizza &#123; //定义一个抽象方法,createPizza,让各个工厂子类自己实现 abstract Pizza createPizza(String orderType); //构造器 public OrderPizza()&#123; Pizza pizza = null; String orderType;//订购pizza的类型 do&#123; orderType = getType(); pizza = createPizza(orderType);//抽象方法，由工厂子类完成 //输出pizza制作过程 pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); &#125;while (true); &#125; //定义一个简单工厂对象// SimpleFactory simpleFactory;// Pizza pizza = null;//// //构造器// public OrderPizza(SimpleFactory simpleFactory)&#123;// setFactory(simpleFactory);// &#125;//// public void setFactory(SimpleFactory simpleFactory)&#123;// String orderType = &quot;&quot;;//用户输入// this.simpleFactory = simpleFactory;//设置简单工厂对象//// do &#123;// orderType = getType();// pizza = this.simpleFactory.createPizza(orderType);//// if (pizza != null)&#123;// //成功//// &#125;// &#125; while (true);// &#125; //写一个方法可以获取客户希望订购的披萨种类 private String getType() &#123; try &#123; BufferedReader strin = new BufferedReader(new InputStreamReader(System.in)); System.out.println(&quot;input pizza type:&quot;); String str = strin.readLine(); return str; &#125; catch (IOException e) &#123; e.printStackTrace(); return &quot;&quot;; &#125; &#125;&#125;public class LDOrderPizza extends OrderPizza&#123; @Override Pizza createPizza(String orderType) &#123; Pizza pizza = null; if (orderType.equals(&quot;chess&quot;))&#123; pizza = new LDChessPizza(); &#125; else if (orderType.equals(&quot;pepper&quot;)) &#123; pizza = new BJPepperPizza(); &#125; return pizza; &#125;&#125;public class BJOrderPizza extends OrderPizza&#123; @Override Pizza createPizza(String orderType) &#123; Pizza pizza = null; if (orderType.equals(&quot;chess&quot;))&#123; pizza = new BJChessPizza(); &#125; else if (orderType.equals(&quot;pepper&quot;)) &#123; pizza = new BJPepperPizza(); &#125; return pizza; &#125;&#125;public class PizzaStore &#123; public static void main(String[] args) &#123; //创建北京口味的各种披萨// new BJOrderPizza(); //创建伦敦口味的各种披萨 new LDOrderPizza(); &#125;&#125; 抽象工厂模式 抽象工厂模式：定义了一个interface用于创建相关或有依赖关系的对象簇，而无需指明具体的类 抽象工厂模式可以将简单工厂模式和工厂方法模式进行整合。 从设计层面看，抽象工厂模式就是对简单工厂模式的改进(或者称为进一步的抽象)。 将工厂抽象成两层，AbsFactory(抽象工厂) 和 具体实现的工厂子类。程序员可以根据创建对象类型使用对应的工厂子类。这样将单个的简单工厂类变成了工厂簇，更利于代码的维护和扩展。 /** * @Author: dongnan * @CreateTime: 2023/01/09 16:47 * 一个抽象工厂模式的抽象层（接口） */public interface AbsFactory &#123; //让下面的工厂子类来具体实现 public Pizza createPizza(String orderType);&#125;/** * @Author: dongnan * @CreateTime: 2023/01/09 16:59 * 工厂子类 */public class BJFactory implements AbsFactory&#123; @Override public Pizza createPizza(String orderType) &#123; Pizza pizza = null; if (orderType.equals(&quot;cheese&quot;))&#123; pizza = new BJChessPizza(); &#125; else if (orderType.equals(&quot;pepper&quot;)) &#123; pizza = new BJPepperPizza(); &#125; return pizza; &#125;&#125;/** * @Author: dongnan * @CreateTime: 2023/01/09 16:59 * 工厂子类 */public class LDFactory implements AbsFactory&#123; @Override public Pizza createPizza(String orderType) &#123; Pizza pizza = null; if (orderType.equals(&quot;cheese&quot;))&#123; pizza = new LDChessPizza(); &#125; else if (orderType.equals(&quot;pepper&quot;)) &#123; pizza = new LDPepperPizza(); &#125; return pizza; &#125;&#125;/** * @Author: dongnan * @CreateTime: 2023/01/09 17:02 */public class OrderPizza &#123; AbsFactory factory; public OrderPizza(AbsFactory factory)&#123; setFactory(factory); &#125; private void setFactory(AbsFactory factory)&#123; Pizza pizza = null; String orderType = &quot;&quot;; this.factory = factory; do &#123; orderType = getType(); pizza = factory.createPizza(orderType); if (null != pizza)&#123; pizza.prepare(); pizza.bake(); pizza.cut(); pizza.box(); &#125; else &#123; System.out.println(&quot;订购失败&quot;); break; &#125; &#125;while (true); &#125; //写一个方法可以获取客户希望订购的披萨种类 private String getType() &#123; try &#123; BufferedReader strin = new BufferedReader(new InputStreamReader(System.in)); System.out.println(&quot;input pizza type:&quot;); String str = strin.readLine(); return str; &#125; catch (IOException e) &#123; e.printStackTrace(); return &quot;&quot;; &#125; &#125;&#125; 原型模式​\t用于创建重复的对象，同时又能保证性能 深拷贝和浅拷贝深拷贝和浅拷贝区别是，在有指针的情况下，浅拷贝只是增加了一个指针指向已经存在的内存，而深拷贝就是增加一个指针并且申请一个新的内存，使这个增加的指针指向这个新的内存，采用深拷贝的情况下，释放内存的时候就不会出现在浅拷贝时重复释放同一内存的错误。 浅拷贝是使用默认的clone方法来实现。 深拷贝的实现方式 1）重写clone方法 2）通过对象序列化实现深拷贝 建造者模式创建的东西细节复杂，还必须暴露给使用者。屏蔽过程而不屏蔽细节。 1) 建造者模式（Builder Pattern） 又叫生成器模式，是一种对象构建模式。它可以将复杂对象的建造过程抽象出来（抽象类别），使这个抽象过程的不同实现方法可以构造出不同表现（属性）的对象。 2) 建造者模式 是一步一步创建一个复杂的对象，它允许用户只通过指定复杂对象的类型和内容就可以构建它们，用户不需要知道内部的具体构建细节。 建造者模式的四个角色 1) Product（产品角色）： 一个具体的产品对象。 2) Builder（抽象建造者）： 创建一个Product对象的各个部件指定的 接口&#x2F;抽象类。 3) ConcreteBuilder（具体建造者）： 实现接口，构建和装配各个部件。 4) Director（指挥者）： 构建一个使用Builder接口的对象。它主要是用于创建一个复杂的对象。它主要有两个作用，一是：隔离了客户与对象的生产过程，二是：负责控制产品对象的生产过程。 /** * @Author: dongnan * @CreateTime: 2023/01/11 16:22 * 产品-&gt;product */public class House &#123; private String baise; private String wall; private String roofed; public String getBaise() &#123; return baise; &#125; public void setBaise(String baise) &#123; this.baise = baise; &#125; public String getWall() &#123; return wall; &#125; public void setWall(String wall) &#123; this.wall = wall; &#125; public String getRoofed() &#123; return roofed; &#125; public void setRoofed(String roofed) &#123; this.roofed = roofed; &#125; &#125;/** * @Author: dongnan * @CreateTime: 2023/01/11 16:25 * 抽象的建造者 */public abstract class HouseBuilder &#123; protected House house = new House(); //将建造的流程写好，抽象的方法 public abstract void buildBasic(); public abstract void buildWall(); public abstract void roofed(); //建造房子,将产品返回 public House buildHouse()&#123; return house; &#125;&#125;/** * @Author: dongnan * @CreateTime: 2023/01/11 16:28 * 具体的建造类 */public class CommonHouse extends HouseBuilder &#123; @Override public void buildBasic() &#123; System.out.println(&quot;普通房子打地基5m&quot;); &#125; @Override public void buildWall() &#123; System.out.println(&quot;普通房子砌墙10cm&quot;); &#125; @Override public void roofed() &#123; System.out.println(&quot;普通房子屋顶&quot;); &#125;&#125; 结构型模式 结构型模式关注点“怎样组合对象&#x2F;类？”所以我们关注下类的组合关系 类结构型模式关心类的组合，由多个类可以组合成一个更大的（继承） 对象结构型模式关心类与对象的组合，通过关联关系在一个类中定义另一个类的实例对象（组合） 根据“合成复用原则”，在系统中尽量使用关联关系来替代继承关系，因此大部分结构型模式都是对象结构型模式。 适配器模式（Adapter Pattern）：两个不兼容接口之间适配的桥梁 桥接模式（Bridge Pattern）：相同功能抽象化与实现化解耦，抽象与实现可以独立升级。 过滤器模式（Filter、Criteria Pattern）：使用不同的标准来过滤一组对象 组合模式（Composite Pattern）：相似对象进行组合，形成树形结构 装饰器模式（Decorator Pattern）：向一个现有的对象添加新的功能，同时又不改变其结构 外观模式（Facade Pattern）：向现有的系统添加一个接口，客户端访问此接口来隐藏系统的复杂性。 享元模式（Flyweight Pattern）：尝试重用现有的同类对象，如果未找到匹配的对象，则创建新对象 代理模式（Proxy Pattern）：一个类代表另一个类的功能 适配器模式基本介绍 适配器模式(Adapter Pattern)将某个类的接口转换成客户端期望的另一个接口表示，主的目的是兼容性，让原本因接口不匹配不能一起工作的两个类可以协同工作。其别名为包装器(Wrapper) 适配器模式属于结构型模式 主要分为三类：类适配器模式、对象适配器模式、接口适配器模式 适配器模式（Adapter）包含以下主要角色。 目标（target）接口：可以是抽象类或接口。客户希望直接用的接口。 适配者类（Adaptee）类：隐藏的转换接口。 适配器类（Adapter）：它是一个转换器，通过继承或者引用适配者的对象，把适配者接口转换成目标接口。 类适配器类适配器的原理就是通过继承来实现适配器功能。具体做法：让Adapter实现Target接口，并且继承Adaptee，这样Adapter就具备Target和Adaptee可以将两者进行转化。 /** * 1、系统原有接口 player：可以播放电影，并且返回字幕 * * */public interface Player &#123; String play();&#125; /** * 2、系统原有接口，可以翻译文字内容 */public interface Translator &#123; String translate(String content);&#125; /** * 电影播放器 * 阅读器 * .... */public class MoviePlayer implements Player &#123; @Override public String play() &#123; System.out.println(&quot;正在播放：宋老师的宝贵时间.avi&quot;); String content = &quot;你好&quot;; System.out.println(content); //并且打印出字幕 return content; &#125;&#125; /** * ZH_JP翻译器 * ZH_EN翻译器 * ..... */public class Zh_JPTranslator implements Translator&#123; @Override public String translate(String content) &#123; if(&quot;你好&quot;.equals(content))&#123; return &quot;空尼几哇&quot;; &#125; if (&quot;什么&quot;.equals(content))&#123; return &quot;纳尼&quot;; &#125; return &quot;*******&quot;; &#125;&#125; /** * 1、在原有系统上增加一个适配器。让适配器可以把电影的中文字幕翻译成友人理解的日文字幕 * * 客户调用方法的时候用适配器操作即可。 * * 类结构型模式： * 对象结构型模式： * */public class JPMovieAdapter implements Player &#123; public JPMovieAdapter()&#123; &#125; @Override public String play() &#123; return null; &#125;&#125; /** * * 适配器 * 1、系统原有两个已存在接口 player、translate没有任何关系 * * 需求，现在一个小....日本友人。看电影字幕是中文的不习惯。 * * 2、我们在不改变原有系统的基础上实现这个功能就需要一个适配器 * * 系统原来存在的所有接口都不能动。扩展一个新的类，来连接两个之前不同的类 * */public class MainTest &#123; public static void main(String[] args) &#123; //1、友人想要看电影带日文字幕 MoviePlayer moviePlayer = new MoviePlayer(); moviePlayer.play(); &#125;&#125; /** * 继承的方式：类结构模型，适配转换到了翻译器的功能上 * * */public class JPMoviePlayerAdapter extends Zh_JPTranslator implements Player &#123; private Player target;//被适配对象 public JPMoviePlayerAdapter(Player target)&#123; this.target = target; &#125; @Override public String play() &#123; String play = target.play(); //转换字幕 String translate = translate(play); System.out.println(&quot;日文：&quot;+translate); return play; &#125;&#125; 对象适配器对象适配器的原理就是通过组合来实现适配器功能。具体做法：让Adapter实现Target接口，然后内部持有Adaptee实例，然后在Target接口规定的方法内转换Adaptee。 具体代码实现很简单就是将Adapter代码修改一下，原来是继承 Adaptee，现在是持有 Adaptee示例 具体代码如下： /** * 组合的方式：对象结构模型，适配转换到了翻译器的功能上 * * （继承、组合）、封装、多态 * * * */public class JPMoviePlayerAdapter implements Player &#123; //组合的方式 private Translator translator = new Zh_JPTranslator(); private Player target;//被适配对象 public JPMoviePlayerAdapter(Player target)&#123; this.target = target; &#125; @Override public String play() &#123; String play = target.play(); //转换字幕 String translate = translator.translate(play); System.out.println(&quot;日文：&quot;+translate); return play; &#125;&#125; public class MainTest &#123; public static void main(String[] args) &#123; // MeiYanDecorator decorator = new MeiYanDecorator(manTikTok); JPMoviePlayerAdapter adapter = new JPMoviePlayerAdapter(new MoviePlayer()); adapter.play(); &#125;&#125; 接口适配器​ 接口适配器的关注点与类适配器和对象适配器的关注点不太一样，类适配器和对象适配器着重于将系统存在的一个角色(Adaptee）转化成目标接口（Target）所需内容，而接口适配器的使用场景是解决接口方法过多，如果直接实现接口，那么类会多出许多空实现的方法，类显得很臃肿。此时，使用接口适配器就能让我们只实现我们需要的接口方法，目标更清晰。 ​ 其实接口适配器和对象适配器就很像了无非是对象适配器关注点是一个对象的转换，接口可能是多个他的关注点不再是对象转换而是接口转换，这些接口可能需要用到很多个源对象，也就是说这样设计之后转换器中持有的对象就可能是多个了。 public interface ILoginService &#123; void login(String username, String password);&#125; public class LoginService implements ILoginService&#123; @Override public void login(String username, String password) &#123; System.out.println(&quot;登录成功：&quot;); System.out.println(&quot;用户：&quot; + username); System.out.println(&quot;密码：&quot; + password); &#125;&#125; public interface IThirdLoginAdapter &#123; boolean support(IThirdLoginAdapter adapter); void login(String userName,String openId);&#125; public abstract class ThirdLoginAdapter implements IThirdLoginAdapter&#123; public final ILoginService loginService; public ThirdLoginAdapter(ILoginService loginService) &#123; this.loginService = loginService; &#125;&#125; public class QQLoginAdapter extends ThirdLoginAdapter&#123; public QQLoginAdapter(ILoginService loginService) &#123; super(loginService); &#125; @Override public boolean support(IThirdLoginAdapter adapter) &#123; return adapter instanceof QQLoginAdapter; &#125; @Override public void login(String userName, String openId) &#123; &#125;&#125; public class WeChatLoginAdapter extends ThirdLoginAdapter&#123; public WeChatLoginAdapter(ILoginService loginService) &#123; super(loginService); &#125; @Override public boolean support(IThirdLoginAdapter adapter) &#123; return adapter instanceof WeChatLoginAdapter; &#125; @Override public void login(String userName, String openId) &#123; System.out.println(&quot;微信登录:&quot;); super.loginService.login(userName, openId); &#125;&#125; public class LoginAdapter implements ILoginAdapter&#123; private final ILoginService loginService; public LoginAdapter(ILoginService loginService) &#123; this.loginService = loginService; &#125; @Override public void qqLogin(String userName,String openId) &#123; login(userName,openId,WeChatLoginAdapter.class); &#125; @Override public void weChatLogin(String userName,String openId) &#123; login(userName,openId,WeChatLoginAdapter.class); &#125; private void login(String userName, String openId, Class&lt;? extends IThirdLoginAdapter&gt; clazz) &#123; try &#123; IThirdLoginAdapter instance = clazz.getDeclaredConstructor(ILoginService.class).newInstance(this.loginService); if(instance.support(instance))&#123; instance.login(userName,openId); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; public class Client &#123; public static void main(String[] args) &#123; ILoginService loginService = new LoginService(); loginService.login(&quot;土豆&quot;,&quot;123&quot;); ILoginAdapter adapter = new LoginAdapter(loginService); adapter.qqLogin(&quot;qwe&quot;,&quot;123qwe234234&quot;); &#125;&#125; 桥接模式桥接模式就是为了避免直接继承带来的子类爆炸。 桥接模式是指：将实现与抽象放在两个不同的类层次中，使两个层次可以独立改变。 在现实生活中，某些类具有两个或多个维度的变化，如图形既可按形状分，又可按颜色分。如何设计类似于 Photoshop 这样的软件，能画不同形状和不同颜色的图形呢？如果用继承方式，m 种形状和 n 种颜色的图形就有 m×n 种，不但对应的子类很多，而且扩展困难。不同颜色和字体的文字、不同品牌和功率的汽车 桥接将继承转为关联，降低类之间的耦合度，减少代码量 Bridge模式基于类的最小设计原则，通过使用封装、聚合及继承等行为让不同的类承担不同的职责。它的主要特点是把抽象（Abstraction）与行为实现(Implementation)分离开来，从而可以保持各部分的独立性以及应对他们的功能扩展 桥接模式分为一下四个角色： Abstract：定义抽象接口，拥有一个Implementor类型的对象引用 RefinedAbstraction：扩展Abstraction中的接口定义 Implementor：实现部分，可以为接口或者是抽象类，其方法不一定要与抽象部分中的一致，一般情况下是由实现部分提供基本的操作，而抽象部分定义的则是基于实现部分操作的业务方法 ConcreteImplementorA &#x2F;B： 实现Implementor接口，给出具体实现 装饰者模式 适配器是连接两个类，可以增强一个类，装饰器是增强一个类 向一个现有的对象添加新的功能，同时又不改变其结构。属于对象结构型模式。 创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下，提供了额外的功能。 抽象构件（Component）角色： 定义一个抽象接口以规范准备接收附加责任的对象。 具体构件（ConcreteComponent）角色： 实现抽象构件，通过装饰角色为其添加一些职责。 抽象装饰（Decorator）角色： 继承抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。 具体装饰（ConcreteDecorator）角色： 实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。 组合模式基本介绍 组合模式（Composite Pattern），又叫部分整体模式，它创建了对象组的树形结构，将对象组合成树状结构以表示“整体-部分”的层次关系。 组合模式依据树形结构来组合对象，用来表示部分以及整体层次。 这种类型的设计模式属于结构型模式。 组合模式使得用户对单个对象和组合对象的访问具有一致性，即：组合能让客户以一致的方式处理个别对象以及组合对象 对原理结构图的说明-即(组合模式的角色及职责) Component :这是组合中对象声明接口，在适当情况下，实现所有类共有的接口默认行为,用于访问和管理Component 子部件, Component 可以是抽象类或者接口 Leaf : 在组合中表示叶子节点，叶子节点没有子节点 Composite:非叶子节点，用于存储子部件，在Component接口中实现子部件的相关操作，比如增加、删除 组合模式注意事项 简化客户端操作。客户端只需要面向一致的对象而不用考虑整体部分或者节点叶子的部分 具有较强的扩展性。当我们要更改组合对象时，我们只需要调整内部的层次关系，客户端不用做出任何改动 需要&#x3D;&#x3D;遍历组织结构，或者处理的对象具有树形结构时，非常适用组合模式&#x3D;&#x3D; 要求较高的抽象性，&#x3D;&#x3D;如果节点和叶子有很多差异性的话，比如很多方法和属性都不一样，不适合使用组合模式&#x3D;&#x3D; 外观模式基本介绍 外观模式（Facade），也叫“过程模式：外观模式为子系统中的一组接口提供一个一致的界面，此模式定义了一个高层接口，这个接口使得这一子系统更加容易使用 外观模式通过定义一个一致的接口，用以屏蔽内部子系统的细节，使得调用端只需跟这个接口发生调用，而无需关心这个子系统的内部细节 原理类图的说明(外观模式的角色) 外观类(Facade): 为调用端提供统一的调用接口, 外观类知道哪些子系统负责处理请求,从而将调用端的请求代理给适当子系统对象 调用者(Client): 外观接口的调用者 子系统的集合：指模块或者子系统，处理Facade 对象指派的任务，他是功能的实际提供者 外观模式的注意事项和细节 外观模式对外屏蔽了子系统的细节，因此外观模式降低了客户端对子系统使用的复杂性 外观模式对客户端与子系统的耦合关系，让子系统内部的模块更易维护和扩展 通过合理的使用外观模式，可以帮我们更好的划分访问的层次 当系统需要进行分层设计时，可以考虑使用Facade模式 在维护一个遗留的大型系统时，可能这个系统已经变得非常难以维护和扩展，此时可以考虑为新系统开发一个Facade类，来提供遗留系统的比较清晰简单的接口，让新系统与Facade类交互，提高复用性 不能过多的或者不合理的使用外观模式，使用外观模式好，还是直接调用模块好。要以让系统有层次，利于维护为目的。 享元模式基本介绍 享元模式（Flyweight Pattern） 也叫 蝇量模式: 运用共享技术有效地支持大量细粒度的对象 常用于系统底层开发，解决系统的性能问题。像数据库连接池，里面都是创建好的连接对象，在这些连接对象中有我们需要的则直接拿来用，避免重新创建，如果没有我们需要的，则创建一个 享元模式能够解决重复对象的内存浪费的问题，当系统中有大量相似对象，需要缓冲池时。不需总是创建新对象，可以从缓冲池里拿。这样可以降低系统内存，同时提高效率 享元模式经典的应用场景就是池技术了，String常量池、数据库连接池、缓冲池等等都是享元模式的应用，享元模式是池技术的重要实现方式 享元模式把一个对象的状态分为内部状态和外部状态，内部状态是不变的可以共享的相同内容，外部状态是变化的是需要外部环境来设置的不能共享的内容，需要注意内部状态和外部状态的区分 FlyWeight 是抽象的享元角色, 他是产品的抽象类, 同时定义出对象的外部状态和内部状态(后面介绍) 的接口或实现 ConcreteFlyWeight 是具体的享元角色，是具体的产品类，实现抽象角色定义相关业务 UnSharedConcreteFlyWeight 是不可共享的角色，一般不会出现在享元工厂 FlyWeightFactory 享元工厂类，用于构建一个池容器，同时提供从池中获取对象 代理模式 代理模式(Proxy Pattern) 是一种结构型设计模式， 让你能够提供对象的替代品或其占位符。 代理控制着对于原对象的访问， 并允许在将请求提交给对象前后进行一些处理。代理模式给某一个对象提供一个代理，并由代理对象控制对原对象的引用。代理模式的英文叫做Proxy或Surrogate，它是一种对象结构型模式。 主要包含三种角色： 1.抽象主题角色（Subject）：主要职责是声明真实主题与代理的共同接口方法，该类可以是接口也可以是抽象类 2.真实主题角色（RealSubject）：也被称为被代理类，该类定义的代理所表示的真实对象，是负责执行系统真正的逻辑业务对象 3.代理主题角色（Proxy）：也被称为代理类，其内部持有RealSubject的引用，因此具备完全的对Real Subject的代理权，客户端调用代理对象的方法，同时也调用被代理对象的方法，但是会在代理对象前后增加一些处理代码 代理模式的通用写法Subject： public interface Subject &#123; void request();&#125; RealSubject: public class RealSubject implements Subject&#123; @Override public void request() &#123; System.out.println(&quot;原始类处理请求&quot;); &#125;&#125; Proxy: public class Proxy implements Subject&#123; private Subject subject; public Proxy(Subject subject) &#123; this.subject = subject; &#125; @Override public void request() &#123; before(); this.subject.request(); after(); &#125; private void before()&#123; System.out.println(&quot;处理请求之前&quot;); &#125; private void after()&#123; System.out.println(&quot;处理请求之后&quot;); &#125;&#125; 客户端调用： public class Client &#123; public static void main(String[] args) &#123; RealSubject realSubject = new RealSubject(); Proxy proxy = new Proxy(realSubject); proxy.request(); &#125;&#125; 静态代理这种代理方式需要代理对象和目标对象实现一样的接口，且代理类的代码是在运行之前就需要写好的 优点是：可以在不修改目标对象的前提下扩展目标对象的功能。 缺点也很明显 如果接口需要增加一个方法对应的代理类和被代理类都需要添加新的实现。 冗余。由于代理对象要实现与目标对象一致的接口，会产生过多的代理类。 代码示例比如买房，通常卖家只有房源，而卖家一般是没有房源的所以需要找到中介提供房源然后卖房。其中卖房的人就是被代理类，中介相当于代理类。 public interface IPerson &#123; void sellHouse();&#125;public class Seller implements IPerson&#123; @Override public void sellHouse() &#123; System.out.println(&quot;我要卖房&quot;); &#125;&#125;public class Intermediary implements IPerson &#123; private final IPerson person; public Intermediary(IPerson person)&#123; this.person = person; &#125; @Override public void sellHouse() &#123; before(); person.sellHouse(); after(); &#125; private void before()&#123; System.out.println(&quot;发布房源&quot;); &#125; private void after()&#123; System.out.println(&quot;交易完成&quot;); &#125;&#125; 动态代理基于JDK实现动态代理代码示例还是上述的例子我们用JDK动态代理来实现(接口以及卖家代码都不变,新增代理处理类) public class JdkIntermediary implements InvocationHandler &#123; private IPerson target; public IPerson getInstance(IPerson target)&#123; this.target = target; Class&lt;?&gt; clz = target.getClass(); return (IPerson)Proxy.newProxyInstance(clz.getClassLoader(),clz.getInterfaces(),this); &#125; private void before()&#123; System.out.println(&quot;发布房源&quot;); &#125; private void after()&#123; System.out.println(&quot;交易完成&quot;); &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; before(); Object result = method.invoke(this.target, args); after(); return result; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; IPerson person = new Seller(); JdkIntermediary jdkIntermediary = new JdkIntermediary(); IPerson instance = jdkIntermediary.getInstance(person); instance.sellHouse(); &#125;&#125; 看上去可能有些人说JdkIntermediary不就是代理类吗不也是在运行之前生成的吗，但是其实JdkIntermediary并不是代理类，它可以理解成代理处理程序我们可以断点看一下真正的代 理类是$Proxy0这个类是在运行时生成的。 优点：接口中新增方法不需要再像静态代理来一样每个方法都实现一遍了，维护比较方便缺点：被代理类必须实现接口 行为型模式策略模式策略模式使用的就是面向对象的继承和多态机制，从未实现同一行为在不同场景下具备不同实现。 通俗点将就是在我们业务逻辑中肯定有很多switch case或者是很多if else,我们的业务是需要通过场景判断选择其中一个逻辑执行，这个时候就可以使用策略模式（需要注意的是策略模式一定是只需要执行很多逻辑算法中的一个，而不是执行多个逻辑算法，如果是执行多个就不太适合使用策略模式） 总结一下：策略模式适用场景： 针对一个问题有多个不同的处理方式，每种方式之间没有耦合（可以独立解决问题） 算法逻辑需要自由切换的情况下 需要屏蔽算法逻辑规则的场景（应为高层是不接触算法逻辑的实现的） 其中： 上下文角色（Context）:主要使用来操作策略上下文，做到屏蔽高层模块对策略、算法直接访问，也可以在这里封装可能存在的变化 抽象策略角色（Strategy）:主要是规定策略或者算法的行为 具体策略角色（ConcreteStrategy）：策略或者是算法的实现 支付案例实现（策略设计模式）package com.example.demo.test.pay;import java.util.Objects;public class Order &#123; private String uid; private String orderId; private double amount; public Order(String uid, String orderId, double amount) &#123; this.uid = uid; this.orderId = orderId; this.amount = amount; &#125; public void pay(String payType)&#123; if(Objects.equals(payType, &quot;AliPay&quot;))&#123; AliPay aliPay = new AliPay(); PayContext payContext = new PayContext(aliPay); payContext.pay(this.uid, this.amount); &#125;else if(Objects.equals(payType, &quot;WeChatPay&quot;))&#123; WeChatPay weChatPay = new WeChatPay(); PayContext payContext = new PayContext(weChatPay); payContext.pay(this.uid, this.amount); &#125;else if(Objects.equals(payType, &quot;UnionPay&quot;))&#123; UnionPay unionPay = new UnionPay(); PayContext payContext = new PayContext(unionPay); payContext.pay(this.uid, this.amount); &#125; &#125;&#125;public interface IPay &#123; String getName(); Double queryBalance(String uid); default void pay(String uid,Double price)&#123; Double currentAmount = queryBalance(uid); if(currentAmount &lt; price)&#123; System.out.println(getName() + &quot;余额不足&quot;); &#125;else&#123; System.out.println(getName() + &quot;支付成功&quot;); &#125; &#125;&#125;package com.example.demo.test.pay;public class AliPay implements IPay&#123; @Override public String getName() &#123; return &quot;支付宝&quot;; &#125; @Override public Double queryBalance(String uid) &#123; return 900.0; &#125;&#125;package com.example.demo.test.pay;public class WeChatPay implements IPay&#123; @Override public String getName() &#123; return &quot;微信支付&quot;; &#125; @Override public Double queryBalance(String uid) &#123; return 200.0; &#125;&#125;package com.example.demo.test.pay;public class UnionPay implements IPay&#123; @Override public String getName() &#123; return &quot;银行卡&quot;; &#125; @Override public Double queryBalance(String uid) &#123; return 10000.0; &#125;&#125; 责任链模式责任链模式是将链中的每一个节点看做是一个对象，每个节点处理的请求均不相同，且内部自动维护下一个节点对象，当一个请求从链式的首段发出时，会沿着链的路径依次传递给每一个节点对象，直至有对象处理这个请求位置，属于行为模式。 这里需要注意的是每个节点都能对对象进行一定的处理(也可以不处理)，处理完成之后节点再进行判断还要进行后续处理还是说传递给下一个节点。 首先举一个日常的例子，比如我们申请开发票，首先我们要写好报销单，首先要你的部门领导审批，部门领导审批不通过直接打回，审批通过再由公司的总经理审批这里审批通过才算成审批完成。这种情况就很适合使用责任链模式。 总结一下责任链主要适用一下几种情况： 多个对象可以处理同一个请求，但是具体由那个对象处理完成则在运行时决定。 不明确指定接收者的情况下，向多个对象中的一个提交一个请求 通过类图可以看到总共包含以下角色： 抽象处理者：主要是定义处理请求的方法以及维护下一个处理结点的对象的引用 具体处理者：处理的具体实现 首先创建抽象类规定抽象方法以及维护下一个节点 public abstract class Handler &#123; protected Handler next; public void setNext(Handler next) &#123; this.next = next; &#125; public abstract void doHandler(User user);&#125; 然后就是创建多个实现逻辑的节点对象: public class ValidatedHandler extends Handler &#123; @Override public void doHandler(User user) &#123; if(user.getUsername() == null || user.getPassword() == null) &#123; System.out.println(&quot;账户或者密码为null&quot;); &#125;else&#123; this.next.doHandler(user); &#125; &#125;&#125;public class UserHandler extends Handler&#123; @Override public void doHandler(User user) &#123; queryUserInfo(user); if(user.getRoleName() == null)&#123; System.out.println(&quot;没有找到用户&quot;); &#125;else&#123; this.next.doHandler(user); &#125; &#125; private static void queryUserInfo(User user)&#123; if(Objects.equals(user.getUsername(), &quot;土豆&quot;) &amp;&amp; Objects.equals(user.getPassword(), &quot;666666&quot;)) &#123; user.setRoleName(&quot;超管&quot;); &#125;else if(Objects.equals(user.getUsername(), &quot;土豆2号&quot;) &amp;&amp; Objects.equals(user.getPassword(), &quot;666666&quot;))&#123; user.setRoleName(&quot;普通员工&quot;); &#125; &#125;&#125;public class AuthHandler extends Handler&#123; @Override public void doHandler(User user) &#123; if(!Objects.equals(user.getRoleName(), &quot;超管&quot;))&#123; System.out.println(&quot;没有权限&quot;); &#125; System.out.println(&quot;登入成功&quot;); &#125;&#125; 最后调用： public static void main(String[] args) &#123; User user = new User(&quot;土豆&quot;,&quot;666666&quot;); Handler validatedHandler = new ValidatedHandler(); Handler userHandler = new UserHandler(); Handler authHandler = new AuthHandler(); validatedHandler.setNext(userHandler); userHandler.setNext(authHandler); validatedHandler.doHandler(user);&#125; 门面模式 门面模式，是指提供一个统一的接口去访问多个子系统的多个不同的接口，它为子系统中的一组接口提供一个统一的高层接口。使得子系统更容易使用。 主要角色有： 外观角色（Facade）:也称门面角色，系统对外的统一接口 子系统角色（SubSystem）:可以同时有一个或者多个SubSystem，SubSystem并不知道Facade的存在，Facade对于SubSystem而言仅仅只是一个客户端 public class Facade &#123; private final SystemA systemA = new SystemA(); private final SystemB systemB = new SystemB(); private final SystemC systemC = new SystemC(); public void doSomething() &#123; systemA.doSomething(); systemB.doSomething(); systemC.doSomething(); &#125;&#125;public class SystemA &#123; public void doSomething() &#123; System.out.println(&quot;调用A系统&quot;); &#125;&#125;public class SystemB &#123; public void doSomething() &#123; System.out.println(&quot;调用B系统&quot;); &#125;&#125;public class SystemC &#123; public void doSomething() &#123; System.out.println(&quot;调用C系统&quot;); &#125;&#125;public class Client &#123; public static void main(String[] args) &#123; Facade facade = new Facade(); facade.doSomething(); &#125;&#125; 门面模式应用场景 对分层结构系统构建时，使用门面模式定义子系统中每层的入口点可以简化子系统之间的依赖关系。 当一个复杂系统的子系统很多时，外门面模式可以为系统设计一个简单的接口供外界访问 当客户端与多个子系统之间存在很大的联系时，引入门面模式可将它们分离，从而提高子系统的独立性和可移植性 门面模式优缺点优点： 简化了调用过程，无需深入了解子系统 减少系统依赖 更好的划分层次 遵循迪米特法则 缺点： 不符合开闭原则 有可能出现违背单一职责原则 模版方法模式 模板方法模式(Template Method Pattern）又叫模板模式，是指定义一个操作中的算法的框架，而将一些步骤延迟到子类中。使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤，属于行为型设计模式。 具体代码实现： public abstract class AbstractTemplate &#123; public abstract void step1(); public abstract void step2(); public abstract void step3(); public abstract boolean hookMethod(); public void templateMethod()&#123; step1(); step2(); if(hookMethod())&#123; step3(); &#125; &#125;&#125;public class ConcreateTemplate extends AbstractTemplate&#123; public void step1()&#123; System.out.println(&quot;step1&quot;); &#125; public void step2()&#123; System.out.println(&quot;step2&quot;); &#125; public void step3()&#123; System.out.println(&quot;step3&quot;); &#125; public boolean hookMethod()&#123; return true; &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; ConcreateTemplate concreateTemplate = new ConcreateTemplate(); concreateTemplate.templateMethod(); &#125;&#125; 这里需要注意的是这个hookMethod钩子函数可有可无，设计钩子方法的主要目的是用来干预执行流程，使得我们控制行为流程更加灵活，更符合实际业务的需求。钩子方法的返回值一般为适合条件分支语句的返回值（如 boolean、 int等）。具体可以根据自己的业务场景来决定是否需要使用钩子方法 我们封装一下rocketmq发送下消息的流程，rocketmq的流程是比较固定的，大致分为三步，启动生产者、发送消息、关闭应用程序，我们可以吧这三步固定在抽象类中，让子类去实现： 抽象模板： public abstract class RocketMQProducerTemplate &#123; DefaultMQProducer producer; public RocketMQProducerTemplate()&#123; this.producer = new DefaultMQProducer(&quot;tudou1&quot;); producer.setNamesrvAddr(&quot;127.0.0.1:9876&quot;); &#125; public abstract void start(); public abstract void send(String message); public void close()&#123; producer.shutdown(); &#125; public void sendMessage(String message) &#123; start(); send(message); close(); &#125;&#125; 实际实现： public class ProducerA extends RocketMQProducerTemplate&#123; @Override public void start() &#123; try&#123; producer.start(); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125; @Override public void send(String message) &#123; Message msg = new Message(&quot;test&quot;,&quot;TestA&quot;, message.getBytes()); try&#123; SendResult result = super.producer.send(msg); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; 模板方法优缺点优点： 利用模板方法将相同处理逻辑的代码放到抽象父类中，可以提高代码的复用性。 将不同的代码不同的子类中，通过对子类的扩展增加新的行为，提高代码的扩展性。 把不变的行为写在父类上，去除子类的重复代码，提供了一个很好的代码复用平台，符合开闭原则。 缺点： 类的个数增加，应为每种逻辑都需要新增一个实现类 系统复杂度增加 如果父类新增抽象方法，子类都需要实现一遍 迭代器模式 迭代器模式 （Iterator Pattern）又称为游标模式(Cursor Pattern)，它提供一种顺序访问集合&#x2F; 容器对象元素的方法，而又无须暴露集合内部表示(到底是列表、栈还是树等)。迭代器模式可以为不同的容器提供一致的 遍历行为，而不用关心容器内容元素组成结构，属于行为型模式。 实现代码如下（这里使用的集合是list,也可以使用其他集合这里就不一一展示了） public interface Iterator&lt;T&gt; &#123; T next(); boolean hasNext();&#125;public class ConcreteIterator&lt;T&gt; implements Iterator&lt;T&gt;&#123; private final List&lt;T&gt; list; private int cursor = 0; public ConcreteIterator(List&lt;T&gt; list)&#123; this.list = list; &#125; @Override public T next() &#123; if(hasNext()) &#123; return list.get(cursor++); &#125; return null; &#125; @Override public boolean hasNext() &#123; return list.size() &gt; cursor; &#125;&#125;public interface IAggregate &#123; void add(User user); void remove(User user); Iterator&lt;User&gt; iterator();&#125;public class ConcreteAggregate implements IAggregate&#123; private final List&lt;User&gt; users = new ArrayList&lt;&gt;(); @Override public void add(User user) &#123; users.add(user); &#125; @Override public void remove(User user) &#123; users.remove(user); &#125; @Override public Iterator&lt;User&gt; iterator() &#123; return new ConcreteIterator&lt;&gt;(users); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; User user1 = new User(&quot;张三&quot;, &quot;123&quot;); User user2 = new User(&quot;张三2&quot;, &quot;123&quot;); User user3 = new User(&quot;张三3&quot;, &quot;123&quot;); User user4 = new User(&quot;张三4&quot;, &quot;123&quot;); User user5 = new User(&quot;张三5&quot;, &quot;123&quot;); User user6 = new User(&quot;张三6&quot;, &quot;123&quot;); User user7 = new User(&quot;张三7&quot;, &quot;123&quot;); ConcreteAggregate concreteAggregate = new ConcreteAggregate(); concreteAggregate.add(user1); concreteAggregate.add(user2); concreteAggregate.add(user3); concreteAggregate.add(user4); concreteAggregate.add(user5); concreteAggregate.add(user6); concreteAggregate.add(user7); Iterator&lt;User&gt; iterator = concreteAggregate.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next().toString()); &#125; &#125;&#125; 迭代器模式的优缺点优点： 多态迭代：为不同的聚合结构提供一致的遍历接口，即一个迭代接口可以访问不同的集合对象 简化集合对象接口：迭代器模式将集合对象本身应该提供的元索迭代接口抽取到了迭代器 中，使集合对象无须关心具体迭代行为 元素迭代功能多样化：每个集合对象都可以提供一个或多个不同的迭代器，使的同种元素聚 合结构可以有不同的迭代行为； 解耦迭代与集合：迭代器模式封装了具体的迭代算法，迭代算法的变化，不会影响到集合 对象的架构 缺点： 对于比较简单的遍历（像数组或者有序列表），使用迭代器方式遍历较为繁琐。 命令模式 将一个请求封装为一个对象，从而使你可用不同的请求对客户进行参数化，对请求排队或记录请求日志，以及支持可撤销的操作 命令模式( Command Pattern) 是对命令的封装，每一个命令都是一个操作：请求的一方 发出请求要求执行一个操作；接收的一方收到请求，并执行操作。命令模式解耦了请求方和接 收方，请求方只需请求执行命令，不用关心命令是怎样被接收，怎样被操作以及是否被执行⋯等。命令模式属于行为型模式。 命令模式的应用层场景当系统的某项操作具备命令语义时，且命令实现不稳定（变化），那么可以通过命令模式解 耦请求与实现，利用抽象命令接口使请求方代码架构稳定，封装接收方具体命令实现细节。接 收方与抽象命令接口呈现弱耦合（内部方法无需一致），具备良好的扩展性。命令模式适用于 以下应用场景： 现实语义中具备 “命令”的操作（如命令菜单，shell 命令⋯）； 请求调用者和请求的接收者需要解耦，使得调用者和接收者不直接交互； 需要抽象出等待执行的行为，比如撤销(Undo)操作和恢复(Redo)等操作； 需要支持命令宏（即命令组合操作）。 public interface ICommand &#123; void execute();&#125;public class ConcreteCommand implements ICommand&#123; private final Receiver receiver; public ConcreteCommand(Receiver receiver) &#123; this.receiver = receiver; &#125; @Override public void execute() &#123; receiver.action(); &#125;&#125;public class Receiver &#123; public void action()&#123; System.out.println(&quot;接收方接收到命令并执行&quot;); &#125;&#125;public class Invoker &#123; private final ICommand command; public Invoker(ICommand command) &#123; this.command = command; &#125; public void action()&#123; command.execute(); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; ConcreteCommand concreteCommand = new ConcreteCommand(new Receiver()); Invoker invoker = new Invoker(concreteCommand); invoker.action(); &#125;&#125; 从上面的类图中就可以发现Invoker和Receiver是没有耦合的，Invoker通过Command和Receiver建立联系的，这里 Invoker就相当于我们平时写业务中的一个业务逻辑的实现，你可以理解成是一个 service,而 Receiver相当于这个业务中的具体某个功能的实现，如果此时业务的需求需要变动，此时我们只需要更改Command中Receiver的应用或者为了符合开闭原则我们完全可以重新创建一个Command,对应者新的Receiver即可，这样对该对于我们整体的业务逻辑是没有改动的。 同时也可以结合装饰器模式，在原有的功能基础上增加一些其他的功能比如日志的收集等等，如果命令不是一个单独的命令而是一个命令的集合 Command会对应着多个命令，同时 Receiver也对应这多个方法，如果能对 Receiver进行抽象，这不就演变成了桥接模式，变成了command和Receiver两个变化的维度，这样扩展性更好 命令模式优缺点优点： 通过引入中间件（抽象接口），解耦了命令请求与实现； 扩展性良好，可以很容易地增加新命令； 支持组合命令，支持命令队列； 可以在现有命令的基础上，增加额外功能（比如日志记录…，结合装饰器模式更酸爽）。 缺点： 具体命令类可能过多； 命令模式的结果其实就是接收方的执行结果，但是为了以命令的形式进行架构，解耦请求与实现，引入了额外类型结构（引入了请求方与抽象命令接口），增加了理解上的困难（不过这 也是设计模式带来的一个通病，抽象必然会引入额外类型；抽象肯定比紧密难理解）。 观察者模式 观察者模式(Observer Pattern），又叫发布-订阅（Publish&#x2F;Subscribe）模式、模型-视图 Model&#x2F;View ） 模式、源-监听器（Source&#x2F;Listener）模式或从属者(Dependents）模式。定义一种一对多的依赖关系，一个主题对象可被多个观察者对象同时监听，使得每当主题对象状态变化时，所有依赖于它的对象都会得到通知并被自动更新。属于行为型模式。 ex：比如MQ、异步队列 以下几点适合该模式 1.当一个抽象模型包含两个方面内容，其中一个方面依赖于另一个方面 2.其他一个或多个对象的变化依赖于另一个对象的变化 3.实现类似广播机制的功能，无需知道具体收听者，只需分发广播，系统中感兴趣的对象会自动接收广播 4.多层次嵌套使用，形成一种链式触发机制，使得事件具备跨域（跨越两种观察者类型）通知 具体代码如下： 首先是被观察者它主要的功能是： 一个目标可以被多个观察者观察 目标提供对观察者注册和退订的维护 当目标的状态发生变化时，目标负责通知所有注册的、有效的观察者 ISubject它的主要作用就是定义接口（主要是订阅和取消订阅以及通知的接口） public interface ISubject &#123; void attach(IObserver observer); void detach(IObserver observer); void notifyObservers();&#125; 然后是具体的被观察对象ConcreteSubject这里主要实现具体方法 public class ConcreteSubject implements ISubject&#123; private static final List&lt;IObserver&gt; observers = new ArrayList&lt;&gt;(); @Override public void attach(IObserver observer) &#123; observers.add(observer); &#125; @Override public void detach(IObserver observer) &#123; observers.remove(observer); &#125; @Override public void notifyObservers() &#123; for (IObserver observer : observers) &#123; observer.update(); &#125; &#125;&#125; 然后就是观察者，观察者的功能就比较少了就是实现方法等待被通知 public interface IObserver &#123; void update();&#125;public class ConcreteObserver implements IObserver&#123; @Override public void update() &#123; System.out.println(&quot;被观察者更新了数据&quot;); &#125;&#125; 最后再看一下客户端的使用： public class Client &#123; public static void main(String[] args) &#123; ISubject subject = new ConcreteSubject(); IObserver observer = new ConcreteObserver(); subject.attach(observer); subject.notifyObservers(); &#125;&#125; 观察者模式的优缺点优点： 观察者和被观察者是松耦合（抽象耦合）的，符合依赖倒置原则 分离了表示层（观察者） 和数据逻辑层（被观察者），并且建立了一套触发机制，使得数据的变 化可以响应到多个表示层上； 实现了一对多的通讯机制，支持事件注册机制，支持兴趣分发机制，当被观察者触发事件时，只 有感兴趣的观察者可以接收到通知。 缺点： 如果观察者数量过多，则事件通知会耗时较长； 事件通知呈线性关系，如果其中一个观察者处理事件卡壳，会影响后续的观察者接收该事件； 如果观察者和被观察者之间存在循环依赖，则可能造成两者之间的循环调用，导致系统崩溃。","categories":["设计模式"]},{"title":"JUC","path":"/2023/11/27/JUC笔记/JUC/","content":"JUC 高并发编程• 课程内容概览 1 、什么是JUC 2 、Lock接口 3 、线程间通信 4 、集合的线程安全 5 、多线程锁 6 、Callable接口 7 、JUC三大辅助类: CountDownLatch CyclicBarrier Semaphore 8 、读写锁: ReentrantReadWriteLock 9 、阻塞队列 10 、ThreadPool线程池 11 、Fork&#x2F;Join框架 12 、CompletableFuture 1 什么是 JUC1.1 JUC简介在Java中，线程部分是一个重点，本篇文章说的JUC也是关于线程的。JUC就是java.util .concurrent工具包的简称。这是一个处理线程的工具包，JDK1.5开始出现的。 1.2 进程与线程进程（Process） 是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。 在当代面向线程设计的计算机结构中，进程是线程的容器。程序是指令、数据及其组织形式的描述，进程是程序的实体。是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。程序是指令、数据及其组织形式的描述，进程是程序的实体。线程（thread） 是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。 总结来说:进程：指在系统中正在运行的一个应用程序；程序一旦运行就是进程；进程——资源分配的最小单位。 线程：系统分配处理器时间资源的基本单元，或者说进程之内独立执行的一个单元执行流。线程——程序执行的最小单位。 1.3 线程的状态1.3.1 线程状态枚举类Thread.Statepublic enum State &#123;/*** Thread state for a thread which has not yet started. */ NEW,(新建)/*** Thread state for a runnable thread. A thread in the runnable* state is executing in the Java virtual machine but it may* be waiting for other resources from the operating system* such as processor. */ RUNNABLE,（准备就绪）/*** Thread state for a thread blocked waiting for a monitor lock.* A thread in the blocked state is waiting for a monitor lock* to enter a synchronized block/method or* reenter a synchronized block/method after calling* &#123;@link Object#wait() Object.wait&#125;. */ BLOCKED,（阻塞）/*** Thread state for a waiting thread.* A thread is in the waiting state due to calling one of the* following methods:* &lt;ul&gt;* &lt;li&gt;&#123;@link Object#wait() Object.wait&#125; with no timeout&lt;/li&gt;* &lt;li&gt;&#123;@link #join() Thread.join&#125; with no timeout&lt;/li&gt;* &lt;li&gt;&#123;@link LockSupport#park() LockSupport.park&#125;&lt;/li&gt;* &lt;/ul&gt; ** &lt;p&gt;A thread in the waiting state is waiting for another thread to* perform a particular action. ** For example, a thread that has called &lt;tt&gt;Object.wait()&lt;/tt&gt;* on an object is waiting for another thread to call* &lt;tt&gt;Object.notify()&lt;/tt&gt; or &lt;tt&gt;Object.notifyAll()&lt;/tt&gt; on* that object. A thread that has called &lt;tt&gt;Thread.join()&lt;/tt&gt;* is waiting for a specified thread to terminate. */ WAITING,（不见不散）/*** Thread state for a waiting thread with a specified waiting time.* A thread is in the timed waiting state due to calling one of* the following methods with a specified positive waiting time:* &lt;ul&gt;* &lt;li&gt;&#123;@link #sleep Thread.sleep&#125;&lt;/li&gt;* &lt;li&gt;&#123;@link Object#wait(long) Object.wait&#125; with timeout&lt;/li&gt;* &lt;li&gt;&#123;@link #join(long) Thread.join&#125; with timeout&lt;/li&gt;* &lt;li&gt;&#123;@link LockSupport#parkNanos LockSupport.parkNanos&#125;&lt;/li&gt;* &lt;li&gt;&#123;@link LockSupport#parkUntil LockSupport.parkUntil&#125;&lt;/li&gt;* &lt;/ul&gt; */ TIMED_WAITING,（过时不候）/*** Thread state for a terminated thread.* The thread has completed execution. */ TERMINATED;(终结) &#125; 1.3.2 wait&#x2F;sleep的区别（ 1 ）sleep是Thread的静态方法，wait是Object的方法，任何对象实例都能调用。 （ 2 ）sleep不会释放锁，它也不需要占用锁。wait会释放锁，但调用它的前提是当前线程占有锁(即代码要在synchronized中)。 （ 3 ）它们都可以被interrupted方法中断。 1.4 并发与并行1.4.1 串行模式串行表示所有任务都一一按先后顺序进行。串行意味着必须先装完一车柴才能运送这车柴，只有运送到了，才能卸下这车柴，并且只有完成了这整个三个步骤，才能进行下一个步骤。 串行是一次只能取得一个任务，并执行这个任务 。 1.4.2 并行模式并行意味着可以同时取得多个任务，并同时去执行所取得的这些任务。并行模式相当于将长长的一条队列，划分成了多条短队列，所以并行缩短了任务队列的长度。并行的效率从代码层次上强依赖于多进程&#x2F;多线程代码，从硬件角度上则依赖于多核CPU。 1.4.3 并发并发(concurrent)指的是多个程序可以同时运行的现象，更细化的是多进程可以同时运行或者多指令可以同时运行 。但这不是重点，在描述并发的时候也不会去扣这种字眼是否精确，&#x3D;&#x3D;并发的重点在于它是一种现象&#x3D;&#x3D;, &#x3D;&#x3D;并发描述的是多进程同时运行的现象&#x3D;&#x3D;。但实际上，对于单核心CPU来说，同一时刻只能运行一个线程。所以，这里的”同时运行”表示的不是真的同一时刻有多个 线程运行的现象，这是并行的概念，而是提供一种功能让用户看来多个程序同时运行起来了，但实际上这些程序中的进程不是一直霸占CPU的，而是执行一会停一会。要解决大并发问题，通常是将大任务分解成多个小任务 , 由于操作系统对进程的调度是随机的，所以切分成多个小任务后，可能会从任一小任务处执行。这可能会出现一些现象： 可能出现一个小任务执行了多次，还没开始下个任务的情况。这时一般会采用队列或类似的数据结构来存放各个小任务的成果 可能出现还没准备好第一步就执行第二步的可能。这时，一般采用多路复用或异步的方式，比如只有准备好产生了事件通知才执行某个任务。 可以多进程&#x2F;多线程的方式并行执行这些小任务。也可以单进程&#x2F;单线程执行这些小任务，这时很可能要配合多路复用才能达到较高的效率 1.4.4 小结(重点)并发： 同一时刻多个线程在访问同一个资源，多个线程对一个点 例子：春运抢票 电商秒杀… 并行： 多项工作一起执行，之后再汇总 例子：泡方便面，电水壶烧水，一边撕调料倒入桶中 1.5管程管程(monitor)是保证了同一时刻只有一个进程在管程内活动,即管程内定义的操作在同一时刻只被一个进程调用(由编译器实现).但是这样并不能保证进程以设计的顺序执行 JVM中同步是基于进入和退出管程(monitor)对象实现的，每个对象都会有一个管程(monitor)对象，管程(monitor)会随着java对象一同创建和销毁 执行线程首先要持有管程对象，然后才能执行方法，当方法完成之后会释放管程，方法在执行时候会持有管程，其他线程无法再获取同一个管程 1.6用户线程和守护线程用户线程: 平时用到的普通线程,自定义线程 守护线程: 运行在后台,是一种特殊的线程,比如垃圾回收 当主线程结束后,用户线程还在运行,JVM存活 如果没有用户线程,都是守护线程,JVM结束 2 、Lock 接口2.1 Synchronized2.1.1 Synchronized关键字回顾synchronized是Java中的关键字，是一种同步锁。它修饰的对象有以下几种： 修饰一个代码块，被修饰的代码块称为同步语句块，其作用的范围是大括号{}括起来的代码，作用的对象是调用这个代码块的对象； 修饰一个方法，被修饰的方法称为同步方法，其作用的范围是整个方法，作用的对象是调用这个方法的对象；虽然可以使用synchronized来定义方法，但synchronized并不属于方法定义的一部分，因此，synchronized关键字不能被继承。如果在父类中的某个方法使用了synchronized关键字，而在子类中覆盖了这个方法，在子类中的这个方法默认情况下并不是同步的，而必须显式地在子类的这个方法中加上synchronized关键字才可以。当然，还可以在子类方法中调用父类中相应的方法，这样虽然子类中的方法不是同步的，但子类调用了父类的同步方法，因此，子类的方法也就相当于同步了。 修改一个静态的方法，其作用的范围是整个静态方法，作用的对象是这个类的所有对象； 修改一个类，其作用的范围是synchronized后面括号括起来的部分，作用主的对象是这个类的所有对象。 2.1.2 售票案例class Ticket &#123;// 票数private int number = 30 ;// 操作方法：卖票public synchronized void sale() &#123;// 判断：是否有票if ( number &gt; 0 ) &#123;System. out .println(Thread. currentThread ().getName()+ &quot; :&quot; +( number --)+ &quot; &quot; + number );&#125;&#125;&#125; 如果一个代码块被synchronized修饰了，当一个线程获取了对应的锁，并执行该代码块时，其他线程便只能一直等待，等待获取锁的线程释放锁，而这里获取锁的线程释放锁只会有两种情况： 1 ）获取锁的线程执行完了该代码块，然后线程释放对锁的占有； 2 ）线程执行发生异常，此时JVM会让线程自动释放锁。那么如果这个获取锁的线程由于要等待IO或者其他原因（比如调用sleep方法）被阻塞了，但是又没有释放锁，其他线程便只能干巴巴地等待，试想一下，这多么影响程序执行效率。因此就需要有一种机制可以不让等待的线程一直无期限地等待下去（比如只等待一定的时间或者能够响应中断），通过Lock就可以办到。 2.2 什么是LockLock锁实现提供了比使用同步方法和语句可以获得的更广泛的锁操作。它们允许更灵活的结构，可能具有非常不同的属性，并且可能支持多个关联的条件对象。Lock提供了比synchronized更多的功能。 Lock与的Synchronized区别 Lock不是Java语言内置的，synchronized是Java语言的关键字，因此是内置特性。Lock是一个类，通过这个类可以实现同步访问； Lock和synchronized有一点非常大的不同，采用synchronized不需要用户去手动释放锁，当 synchronized 方法或者 synchronized 代码块执行完之后，系统会自动让线程释放对锁的占用；而Lock则必须要用户去手动释放锁，如果没有主动释放锁，就有可能导致出现死锁现象。 2.2.1 Lock接口public interface Lock &#123;\tvoid lock();\tvoid lockInterruptibly() throws InterruptedException;\tboolean tryLock();\tboolean tryLock(long time, TimeUnit unit) throws InterruptedException;\tvoid unlock();\tCondition newCondition();&#125; 下面来逐个讲述Lock接口中每个方法的使用2.2.2 locklock()方法是平常使用得最多的一个方法，就是用来获取锁。如果锁已被其他线程获取，则进行等待。采用Lock，必须主动去释放锁，并且在发生异常时，不会自动释放锁。因此一般来说，使用Lock必须在try{}catch{}块中进行，并且将释放锁的操作放在finally块中进行，以保证锁一定被被释放，防止死锁的发生。通常使用Lock来进行同步的话，是以下面这种形式去使用的： Lock lock = ...;lock.lock();try&#123;//处理任务&#125;catch(Exception ex)&#123;&#125;finally&#123;lock.unlock(); //释放锁&#125; 2.2. 3 newCondition​ 关键字synchronized与wait()&#x2F;notify()这两个方法一起使用可以实现等待&#x2F;通知模式， Lock锁的newContition()方法返回Condition对象，Condition类也可以实现等待&#x2F;通知模式。 ​ 用notify()通知时，JVM会随机唤醒某个等待的线程， 使用Condition类可以进行选择性通知， Condition比较常用的两个方法： await()会使当前线程等待,同时会释放锁,当其他线程调用signal()时,线程会重新获得锁并继续执行。 signal()用于唤醒一个等待的线程。 &#x3D;&#x3D; 注意：在调用Condition的await()&#x2F;signal()方法前，也需要线程持有相关的Lock锁，调用await()后线程会释放这个锁，在singal()调用后会从当前Condition对象的等待队列中，唤醒 一个线程，唤醒的线程尝试获得锁， 一旦获得锁成功就继续执行。 &#x3D;&#x3D; 2.3 ReentrantLockReentrantLock，意思是“可重入锁”，关于可重入锁的概念将在后面讲述。 ReentrantLock是唯一实现了Lock接口的类，并且ReentrantLock提供了更多的方法。下面通过一些实例看具体看一下如何使用。 public class Test &#123;private ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;();public static void main(String[] args) &#123;final Test test = new Test();new Thread()&#123;public void run() &#123;test.insert(Thread.currentThread());&#125;;&#125;.start();new Thread()&#123;public void run() &#123;test.insert(Thread.currentThread());&#125;;&#125;.start();&#125;public void insert(Thread thread) &#123;Lock lock = new ReentrantLock(); //注意这个地方lock.lock();try &#123;System.out.println(thread.getName()+&quot;得到了锁&quot;);for(int i= 0 ;i&lt; 5 ;i++) &#123;arrayList.add(i);&#125;&#125; catch (Exception e) &#123;// TODO: handle exception&#125;finally &#123;System.out.println(thread.getName()+&quot;释放了锁&quot;);lock.unlock();&#125;&#125;&#125; 2.4 ReadWriteLockReadWriteLock也是一个接口，在它里面只定义了两个方法： public interface ReadWriteLock &#123;/*** Returns the lock used for reading.** @return the lock used for reading.*/Lock readLock();##### /*** Returns the lock used for writing.** @return the lock used for writing.*/Lock writeLock();&#125; 一个用来获取读锁，一个用来获取写锁。也就是说将文件的读写操作分开，分成 2 个锁来分配给线程，从而使得多个线程可以同时进行读操作。下面的ReentrantReadWriteLock 实现了ReadWriteLock接口。ReentrantReadWriteLock里面提供了很多丰富的方法，不过最主要的有两个方法：readLock()和writeLock()用来获取读锁和写锁。 下面通过几个例子来看一下ReentrantReadWriteLock具体用法。 假如有多个线程要同时进行读操作的话，先看一下 synchronized 达到的效果： #### public class Test &#123;#### private ReentrantReadWriteLock rwl = new#### ReentrantReadWriteLock();#### public static void main(String[] args) &#123;#### final Test test = new Test();#### new Thread()&#123;#### public void run() &#123;#### test.get(Thread.currentThread());#### &#125;;#### &#125;.start();#### new Thread()&#123;#### public void run() &#123;#### test.get(Thread.currentThread());#### &#125;;#### &#125;.start();#### &#125;#### public synchronized void get(Thread thread) &#123;#### long start = System.currentTimeMillis();#### while(System.currentTimeMillis() - start &lt;= 1) &#123;#### System.out.println(thread.getName()+&quot;正在进行读操作&quot;);#### &#125;#### System.out.println(thread.getName()+&quot;读操作完毕&quot;);#### &#125;#### &#125; 而改成用读写锁的话： #### public class Test &#123;#### private ReentrantReadWriteLock rwl = new#### ReentrantReadWriteLock();#### public static void main(String[] args) &#123;#### final Test test = new Test();#### new Thread()&#123;#### public void run() &#123;#### test.get(Thread.currentThread());#### &#125;;#### &#125;.start();#### new Thread()&#123;#### public void run() &#123;#### test.get(Thread.currentThread());#### &#125;;#### &#125;.start();#### &#125;#### public void get(Thread thread) &#123;#### rwl.readLock().lock();#### try &#123;#### long start = System.currentTimeMillis();#### while(System.currentTimeMillis() - start &lt;= 1) &#123;#### System.out.println(thread.getName()+&quot;正在进行读操作&quot;);#### &#125;#### System.out.println(thread.getName()+&quot;读操作完毕&quot;);#### &#125; finally &#123;#### rwl.readLock().unlock();#### &#125;#### &#125;#### &#125; 说明thread1和thread2在同时进行读操作。这样就大大提升了读操作的效率。 &#x3D;&#x3D; 注意: &#x3D;&#x3D; 如果有一个线程已经占用了读锁，则此时其他线程如果要申请写锁，则申请写锁的线程会一直等待释放读锁。 如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或者读锁，则申请的线程会一直等待释放写锁。 2.5 小结(重点)Lock和synchronized有以下几点不同： Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现； synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁； Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断； 通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。 Lock可以提高多个线程进行读操作的效率。 在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。 3 线程间通信线程间通信的模型有两种：共享内存和消息传递，以下方式都是基本这两种模型来实现的。我们来基本一道面试常见的题目来分析场景—两个线程，一个线程对当前数值加 1 ，另一个线程对当前数值减1,要求用线程间通信 3. 1 synchronized方案package com.atguigu.test;/*** volatile关键字实现线程交替加减*/public class TestVolatile &#123;/*** 交替加减* @param args*/public static void main(String[] args)&#123;DemoClass demoClass = new DemoClass();new Thread(() -&gt;&#123;for (int i = 0; i &lt; 5; i++) &#123;demoClass.increment();&#125;&#125;, &quot;线程A&quot;).start();new Thread(() -&gt;&#123;for (int i = 0; i &lt; 5; i++) &#123;demoClass.decrement();&#125;&#125;, &quot;线程B&quot;).start();&#125;&#125;package com.atguigu.test;class DemoClass&#123;//加减对象private int number = 0;##### /**##### * 加 1##### */public synchronized void increment() &#123;try &#123;while (number != 0)&#123;this.wait();&#125;number++;System.out.println(&quot;--------&quot; + Thread.currentThread().getName() + &quot;加一成功----------,值为:&quot; + number);notifyAll();&#125;catch (Exception e)&#123;e.printStackTrace();&#125;&#125;##### /**##### * 减一##### */public synchronized void decrement()&#123;try &#123;while (number == 0)&#123;this.wait();&#125;number--;System.out.println(&quot;--------&quot; + Thread.currentThread().getName() + &quot;减一成功----------,值为:&quot; + number);notifyAll();&#125;catch (Exception e)&#123;e.printStackTrace();&#125;&#125;&#125; 3. 2 Lock方案package com.atguigu.test;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;class DemoClass&#123;//加减对象private int number = 0;##### //声明锁private Lock lock = new ReentrantLock();##### //声明钥匙private Condition condition = lock.newCondition();##### /**##### * 加 1##### */public void increment() &#123;try &#123;lock.lock();while (number != 0)&#123;condition.await();&#125;number++;System.out.println(&quot;--------&quot; + Thread.currentThread().getName() + &quot;加一成功----------,值为:&quot; + number);condition.signalAll();&#125;catch (Exception e)&#123;e.printStackTrace();&#125;finally &#123;lock.unlock();&#125;&#125;##### /**##### * 减一##### */public void decrement()&#123;try &#123;lock.lock();while (number == 0)&#123;condition.await();&#125;number--;System.out.println(&quot;--------&quot; + Thread.currentThread().getName() + &quot;减一成功----------,值为:&quot; + number);condition.signalAll();&#125;catch (Exception e)&#123;e.printStackTrace();&#125;finally &#123;lock.unlock();##### &#125;##### &#125;##### &#125; 3 .4 线程间定制化通信4.4.1 案例介绍&#x3D;&#x3D; 问题: A线程打印 5 次A，B线程打印 10 次B，C线程打印 15 次C,按照此顺序循环 10 轮 &#x3D;&#x3D; 4.4.2 实现流程package com.atguigu.test;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;class DemoClass&#123;//通信对象:0--打印A 1---打印B 2----打印Cprivate int number = 0;//声明锁private Lock lock = new ReentrantLock();//声明钥匙Aprivate Condition conditionA = lock.newCondition();//声明钥匙Bprivate Condition conditionB = lock.newCondition();//声明钥匙Cprivate Condition conditionC = lock.newCondition();##### /**##### * A打印 5 次##### */public void printA(int j)&#123;try &#123;lock.lock();while (number != 0)&#123;conditionA.await();&#125;System.out.println(Thread.currentThread().getName() + &quot;输出A,第&quot; + j + &quot;轮开始&quot;);//输出 5 次Afor (int i = 0; i &lt; 5; i++) &#123;System.out.println(&quot;A&quot;);&#125;//开始打印Bnumber = 1;//唤醒BconditionB.signal();&#125;catch (Exception e)&#123;e.printStackTrace();&#125;finally &#123;lock.unlock();&#125;&#125;##### /**##### * B打印 10 次##### */public void printB(int j)&#123;try &#123;lock.lock();while (number != 1)&#123;conditionB.await();&#125;System.out.println(Thread.currentThread().getName() + &quot;输出B,第&quot; + j + &quot;轮开始&quot;);//输出 10 次Bfor (int i = 0; i &lt; 10; i++) &#123;System.out.println(&quot;B&quot;);&#125;//开始打印Cnumber = 2;//唤醒CconditionC.signal();&#125;catch (Exception e)&#123;e.printStackTrace();&#125;finally &#123;lock.unlock();&#125;&#125;##### /**##### * C打印 15 次##### */public void printC(int j)&#123;try &#123;lock.lock();while (number != 2)&#123;conditionC.await();&#125;System.out.println(Thread.currentThread().getName() + &quot;输出C,第&quot; + j + &quot;轮开始&quot;);//输出 15 次Cfor (int i = 0; i &lt; 15; i++) &#123;System.out.println(&quot;C&quot;);&#125;System.out.println(&quot;-----------------------------------------&quot;);//开始打印Anumber = 0;//唤醒AconditionA.signal();&#125;catch (Exception e)&#123;e.printStackTrace();&#125;finally &#123;lock.unlock();&#125;##### &#125;##### &#125;##### 测试类package com.atguigu.test;##### /*** volatile关键字实现线程交替加减*/public class TestVolatile &#123;/*** 交替加减* @param args*/public static void main(String[] args)&#123;DemoClass demoClass = new DemoClass();new Thread(() -&gt;&#123;for (int i = 1; i &lt;= 10; i++) &#123;demoClass.printA(i);&#125;&#125;, &quot;A线程&quot;).start();new Thread(() -&gt;&#123;for (int i = 1; i &lt;= 10; i++) &#123;demoClass.printB(i);&#125;&#125;, &quot;B线程&quot;).start();new Thread(() -&gt;&#123;for (int i = 1; i &lt;= 10; i++) &#123;demoClass.printC(i);&#125;&#125;, &quot;C线程&quot;).start();&#125;&#125; 4 集合的线程安全4 .1 集合操作DemoNotSafeDemo package com.atguigu.test;import java.util.ArrayList;import java.util.List;import java.util.UUID;/**##### * 集合线程安全案例##### */public class NotSafeDemo &#123;/*** 多个线程同时对集合进行修改* @param args */ public static void main(String[] args) &#123; List list = new ArrayList();for (int i = 0 ; i &lt; 100 ; i++) &#123;new Thread(() -&gt;&#123;list.add(UUID.randomUUID().toString());System.out.println(list);&#125;, &quot;线程&quot; + i).start();&#125;&#125;&#125;异常内容java.util.ConcurrentModificationException**问题: 为什么会出现并发修改异常?**查看ArrayList的add方法源码/*** Appends the specified element to the end of this list. ** @param e element to be appended to this list* @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */ public boolean add(E e) &#123; ensureCapacityInternal(size + 1 ); // Increments modCount!! elementData[size++] = e; return true; &#125; &#x3D;&#x3D; 那么我们如何去解决List类型的线程安全问题? &#x3D;&#x3D; 4 .2 VectorVector 是矢量队列 ，它是JDK1.0版本添加的类。继承于AbstractList，实现了List, RandomAccess, Cloneable这些接口。 Vector 继承了AbstractList，实现了List；所以， 它是一个队列，支持相关的添加、删除、修改、遍历等功能 。 Vector 实现了RandmoAccess接口，即 提供了随机访问功能 。 RandmoAccess是java中用来被List实现，为List提供快速访问功能的。在Vector中，我们即可以通过元素的序号快速获取元素对象；这就是快速随机访问。 Vector 实现了Cloneable接口，即实现clone()函数。它能被克隆。 &#x3D;&#x3D;和ArrayList不同，Vector中的操作是线程安全的。&#x3D;&#x3D; NotSafeDemo代码修改 package com.atguigu.test;import java.util.ArrayList;import java.util.List;import java.util.UUID;import java.util.Vector;/*** 集合线程安全案例 */ public class NotSafeDemo &#123;/*** 多个线程同时对集合进行修改* @param args */ public static void main(String[] args) &#123; List list = new Vector();for (int i = 0 ; i &lt; 100 ; i++) &#123;new Thread(() -&gt;&#123;list.add(UUID.randomUUID().toString());System.out.println(list);&#125;, &quot;线程&quot; + i).start();&#125;&#125;&#125; 现在没有运行出现并发异常,为什么? 查看Vector的add方法 &#x2F;** Appends the specified element to the end of this Vector. @param e element to be appended to this Vector @return {@code true} (as specified by {@link Collection#add}) @since 1.2*&#x2F;public synchronized boolean add(E e) {modCount++;ensureCapacityHelper(elementCount + 1 );elementData[elementCount++] &#x3D; e;return true; } add方法被synchronized同步修辞,线程安全!因此没有并发异常 4 .3 CollectionsCollections提供了方法synchronizedList保证list是同步线程安全的 NotSafeDemo代码修改package com.atguigu.test;import java.util.*;/*** 集合线程安全案例 */public class NotSafeDemo &#123;/*** 多个线程同时对集合进行修改* @param args */ public static void main(String[] args) &#123; List list = Collections.synchronizedList(new ArrayList&lt;&gt;()); for (int i = 0 ; i &lt; 100 ; i++) &#123; new Thread(() -&gt;&#123; list.add(UUID.randomUUID().toString()); System.out.println(list); &#125;, &quot;线程&quot; + i).start(); &#125; &#125; &#125; 没有并发修改异常 查看方法源码 /*** Returns a synchronized (thread-safe) list backed by the specified* list. In order to guarantee serial access, it is critical that* &lt;strong&gt;all&lt;/strong&gt; access to the backing list is accomplished* through the returned list.&lt;p&gt; ** It is imperative that the user manually synchronize on the returned* list when iterating over it:* &lt;pre&gt;* List list = Collections.synchronizedList(new ArrayList());* ...* synchronized (list) &#123;* Iterator i = list.iterator(); // Must be in synchronized block* while (i.hasNext())* foo(i.next());* &#125;* &lt;/pre&gt;* Failure to follow this advice may result in non-deterministic behavior.##### ** &lt;p&gt;The returned list will be serializable if the specified list is* serializable.** @param &lt;T&gt; the class of the objects in the list* @param list the list to be &quot;wrapped&quot; in a synchronized list.* @return a synchronized view of the specified list.*/public static &lt;T&gt; List&lt;T&gt; synchronizedList(List&lt;T&gt; list) &#123;return (list instanceof RandomAccess?new SynchronizedRandomAccessList&lt;&gt;(list) :new SynchronizedList&lt;&gt;(list));&#125; 4 .4 CopyOnWriteArrayList(重点)首先我们对CopyOnWriteArrayList进行学习,其特点如下: 它相当于线程安全的ArrayList。和ArrayList一样，它是个可变数组；但是和ArrayList不同的时，它具有以下特性： 它最适合于具有以下特征的应用程序：List 大小通常保持很小，只读操作远多于可变操作，需要在遍历期间防止线程间的冲突。 它是线程安全的。 因为通常需要复制整个基础数组，所以可变操作（add()、set() 和 remove()等等）的开销很大。 迭代器支持 hasNext(), next()等不可变操作，但不支持可变 remove()等操作。 使用迭代器进行遍历的速度很快，并且不会与其他线程发生冲突。在构造迭代器时，迭代器依赖于不变的数组快照。 独占锁效率低：采用读写分离思想解决 写线程获取到锁，其他写线程阻塞 复制思想： 当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行 Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这时候会抛出来一个新的问题，也就是数据不一致的问题。如果写线程还没来得及写回内存，其他的线程就会读到了脏数据。 &#x3D;&#x3D; 这就是CopyOnWriteArrayList 的思想和原理。就是拷贝一份。 &#x3D;&#x3D; NotSafeDemo代码修改 #### package com.atguigu.test;#### import java.util.*;#### import java.util.concurrent.CopyOnWriteArrayList;#### /**#### * 集合线程安全案例#### */#### public class NotSafeDemo &#123;#### /**#### * 多个线程同时对集合进行修改#### * @param args#### */#### public static void main(String[] args) &#123;#### List list = new CopyOnWriteArrayList();#### for (int i = 0 ; i &lt; 100 ; i++) &#123;#### new Thread(() -&gt;&#123;#### list.add(UUID.randomUUID().toString());#### System.out.println(list);#### &#125;, &quot;线程&quot; + i).start();#### &#125;#### &#125;#### &#125;#### 没有线程安全问题 原因分析 ( 重点 ): &#x3D;&#x3D; 动态数组与线程安全 &#x3D;&#x3D; 下面从“动态数组”和“线程安全”两个方面进一步对CopyOnWriteArrayList的原理进行说明。 “动态数组”机制 它内部有个“volatile数组”(array)来保持数据。在“添加&#x2F;修改&#x2F;删除”数据时，都会新建一个数组，并将更新后的数据拷贝到新建的数组中，最后再将该数组赋值给“volatile数组”, 这就是它叫做CopyOnWriteArrayList的原因。由于它在“添加&#x2F;修改&#x2F;删除”数据时，都会新建数组，所以涉及到修改数据的操作，CopyOnWriteArrayList效率很低；但是单单只是进行遍历查找的话，效率比较高。 “线程安全”机制 通过volatile和互斥锁来实现的。 通过“volatile数组”来保存数据的。一个线程读取volatile数组时，总能看到其它线程对该volatile变量最后的写入；就这样，通过volatile提供了“读取到的数据总是最新的”这个机制的保证。 通过互斥锁来保护数据。在“添加&#x2F;修改&#x2F;删除”数据时，会先“获取互斥锁”，再修改完毕之后，先将数据更新到“volatile数组”中，然后再“释放互斥锁”，就达到了保护数据的目的。 4 .5 小结(重点)1.线程安全与线程不安全集合 集合类型中存在线程安全与线程不安全的两种,常见例如: ArrayList —– Vector HashMap —–HashTable 但是以上都是通过synchronized关键字实现,效率较低 2.Collections构建的线程安全集合 3.java.util.concurrent并发包下 CopyOnWriteArrayList CopyOnWriteArraySet类型,通过动态数组与线程安全的方面保证线程安全 5 多线程锁5 .1 锁的八个问题演示class Phone { public static synchronized void sendSMS() throws Exception {&#x2F;&#x2F; 停留 4 秒TimeUnit. SECONDS .sleep( 4 );System. out .println( “——sendSMS” );} public synchronized void sendEmail() throws Exception {System. out .println( “——sendEmail” );} public void getHello() {System. out .println( “——getHello” );}}&#x2F;** @Description: 8 锁 1 标准访问，先打印短信还是邮件——sendSMS——sendEmail 2 停 4 秒在短信方法内，先打印短信还是邮件——sendSMS——sendEmail 3 新增普通的hello方法，是先打短信还是hello——getHello——sendSMS 4 现在有两部手机，先打印短信还是邮件——sendEmail——sendSMS 5 两个静态同步方法， 1 部手机，先打印短信还是邮件——sendSMS——sendEmail 6 两个静态同步方法， 2 部手机，先打印短信还是邮件 ——sendSMS——sendEmail 7 1个静态同步方法,1个普通同步方法， 1 部手机，先打印短信还是邮件——sendEmail——sendSMS 8 1个静态同步方法,1个普通同步方法， 2 部手机，先打印短信还是邮件——sendEmail——sendSMS 结论: 一个对象里面如果有多个synchronized方法，某一个时刻内，只要一个线程去调用其中的一个synchronized方法了，其它的线程都只能等待，换句话说，某一个时刻内，只能有唯一一个线程去访问这些synchronized方法锁的是当前对象this，被锁定后，其它的线程都不能进入到当前对象的其它的synchronized方法加个普通方法后发现和同步锁无关换成两个对象后，不是同一把锁了，情况立刻变化。synchronized实现同步的基础：Java中的每一个对象都可以作为锁。 具体表现为以下 3 种形式。对于普通同步方法，锁是当前实例对象。对于静态同步方法，锁是当前类的Class对象。对于同步方法块，锁是Synchonized括号里配置的对象 当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。也就是说如果一个实例对象的非静态同步方法获取锁后，该实例对象的其他非静态同步方法必须等待获取锁的方法释放锁后才能获取锁，可是别的实例对象的非静态同步方法因为跟该实例对象的非静态同步方法用的是不同的锁，所以毋须等待该实例对象已获取锁的非静态同步方法释放锁就可以获取他们自己的锁。所有的静态同步方法用的也是同一把锁——类对象本身，这两把锁是两个不同的对象，所以静态同步方法与非静态同步方法之间是不会有竞态条件的。但是一旦一个静态同步方法获取锁后，其他的静态同步方法都必须等待该方法释放锁后才能获取锁，而不管是同一个实例对象的静态同步方法之间，还是不同的实例对象的静态同步方法之间，只要它们同一个类的实例对象！ 6 Callable&amp;Future 接口6 .1 Callable接口目前我们学习了有两种创建线程的方法-一种是通过创建Thread类，另一种是通过使用Runnable创建线程。但是，Runnable缺少的一项功能是，当线程终止时（即run（）完成时），我们无法使线程返回结果。为了支持此功能，Java中提供了Callable接口。 &#x3D;&#x3D; 现在我们学习的是创建线程的第三种方案—Callable接口 &#x3D;&#x3D; Callable接口的特点如下(重点) 为了实现Runnable，需要实现不返回任何内容的run（）方法，而对于Callable，需要实现在完成时返回结果的call（）方法。 call（）方法可以引发异常，而run（）则不能。 为实现Callable而必须重写call方法 不能直接替换runnable,因为Thread类的构造方法根本没有Callable 创建新类MyThread实现runnable接口class MyThread implements Runnable&#123;@Overridepublic void run() &#123;&#125;&#125;新类MyThread2实现callable接口class MyThread2 implements Callable&lt;Integer&gt;&#123;@Overridepublic Integer call() throws Exception &#123;return 200 ;&#125;&#125; 6 .2 Future接口当call（）方法完成时，结果必须存储在主线程已知的对象中，以便主线程可以知道该线程返回的结果。为此，可以使用Future对象。 将Future视为保存结果的对象–它可能暂时不保存结果，但将来会保存（一旦Callable返回）。Future基本上是主线程可以跟踪进度以及其他线程的结果的一种方式。要实现此接口，必须重写 5 种方法，这里列出了重要的方法,如下: public boolean cancel（boolean mayInterrupt）： 用于停止任务。 &#x3D;&#x3D;如果尚未启动，它将停止任务。如果已启动，则仅在mayInterrupt为true时才会中断任务。&#x3D;&#x3D; public Object get（）抛出InterruptedException，ExecutionException：用于获取任务的结果。 &#x3D;&#x3D;如果任务完成，它将立即返回结果，否则将等待任务完成，然后返回结果。&#x3D;&#x3D; public boolean isDone（）： 如果任务完成，则返回true，否则返回false 可以看到Callable和Future做两件事-Callable与Runnable类似，因为它封装了要在另一个线程上运行的任务，而Future用于存储从另一个线程获得的结果。实际上，future也可以与Runnable一起使用。要创建线程，需要Runnable。为了获得结果，需要future。 6. 3 FutureTaskJava库具有具体的FutureTask类型，该类型实现Runnable和Future，并方便地将两种功能组合在一起。 可以通过为其构造函数提供Callable来创建FutureTask。然后，将FutureTask对象提供给Thread的构造函数以创建Thread对象。因此，间接地使用Callable创建线程。 核心原理:(重点)在主线程中需要执行比较耗时的操作时，但又不想阻塞主线程时，可以把这些作业交给Future对象在后台完成 当主线程将来需要时，就可以通过Future对象获得后台作业的计算结果或者执行状态 一般FutureTask多用于耗时的计算，主线程可以在完成自己的任务后，再去获取结果。 仅在计算完成时才能检索结果；如果计算尚未完成，则阻塞 get 方法 一旦计算完成，就不能再重新开始或取消计算 get方法而获取结果只有在计算完成时获取，否则会一直阻塞直到任务转入完成状态，然后会返回结果或者抛出异常 get只计算一次,因此get方法放到最后 demo案例6. 4 使用Callable和FutureCallableDemo案例/*** CallableDemo案列*/public class CallableDemo &#123;/*** 实现runnable接口*/static class MyThread1 implements Runnable&#123;/*** run方法*/@Overridepublic void run() &#123;try &#123;##### System.out.println(Thread.currentThread().getName() + &quot;线程进入了run方法&quot;);&#125;catch (Exception e)&#123;e.printStackTrace();&#125;&#125;&#125;##### /*** 实现callable接口*/static class MyThread2 implements Callable&#123;/*** call方法* @return* @throws Exception*/@Overridepublic Long call() throws Exception &#123;try &#123;System.out.println(Thread.currentThread().getName() + &quot;线程进入了call方法,开始准备睡觉&quot;);Thread.sleep(1000);System.out.println(Thread.currentThread().getName() + &quot;睡醒了&quot;);&#125;catch (Exception e)&#123;e.printStackTrace();&#125;return System.currentTimeMillis();&#125;&#125; public static void main(String[] args) throws Exception&#123;//声明runableRunnable runable = new MyThread1();//声明callableCallable callable = new MyThread2();//future-callableFutureTask&lt;Long&gt; futureTask2 = new FutureTask(callable);//线程二new Thread(futureTask2, &quot;线程二&quot;).start();for (int i = 0; i &lt; 10; i++) &#123;Long result1 = futureTask2.get();System.out.println(result1);&#125;//线程一new Thread(runable,&quot;线程一&quot;).start();&#125;&#125; 6. 5 小结(重点) 在主线程中需要执行比较耗时的操作时，但又不想阻塞主线程时，可以把这些作业交给Future对象在后台完成, 当主线程将来需要时，就可以通过Future对象获得后台作业的计算结果或者执行状态 一般FutureTask多用于耗时的计算，主线程可以在完成自己的任务后，再去 获取结果 仅在计算完成时才能检索结果；如果计算尚未完成，则阻塞 get 方法。一旦计算完成，就不能再重新开始或取消计算。get方法而获取结果只有在计算完成时获取，否则会一直阻塞直到任务转入完成状态，然后会返回结果或者抛出异常。 只计算一次 7 JUC 三大辅助类JUC中提供了三种常用的辅助类，通过这些辅助类可以很好的解决线程数量过多时Lock锁的频繁操作。这三种辅助类为： CountDownLatch: 减少计数 CyclicBarrier: 循环栅栏 Semaphore: 信号灯 下面我们分别进行详细的介绍和学习 7 .1 减少计数CountDownLatchCountDownLatch类可以设置一个计数器，然后通过countDown方法来进行减 1 的操作，使用await方法等待计数器不大于 0 ，然后继续执行await方法之后的语句。 CountDownLatch主要有两个方法，当一个或多个线程调用await方法时，这些线程会阻塞 其它线程调用countDown方法会将计数器减1(调用countDown方法的线程不会阻塞) 当计数器的值变为 0 时，因await方法阻塞的线程会被唤醒，继续执行 场景: 6个同学陆续离开教室后值班同学才可以关门。CountDownLatchDemo package com.atguigu.test;import java.util.concurrent.CountDownLatch;/*** CountDownLatchDemo*/public class CountDownLatchDemo &#123;/**##### * 6个同学陆续离开教室后值班同学才可以关门* @param args*/public static void main(String[] args) throws Exception&#123;//定义一个数值为 6 的计数器CountDownLatch countDownLatch = new CountDownLatch(6);##### //创建 6 个同学for (int i = 1; i &lt;= 6; i++) &#123;new Thread(() -&gt;&#123;try&#123;if(Thread.currentThread().getName().equals(&quot;同学6&quot;))&#123;Thread.sleep(2000);&#125;System.out.println(Thread.currentThread().getName() + &quot;离开了&quot;);//计数器减一,不会阻塞countDownLatch.countDown();&#125;catch (Exception e)&#123;e.printStackTrace();&#125;&#125;, &quot;同学&quot; + i).start();&#125;//主线程await休息System.out.println(&quot;主线程睡觉&quot;);countDownLatch.await();//全部离开后自动唤醒主线程System.out.println(&quot;全部离开了,现在的计数器为&quot; +countDownLatch.getCount());&#125;&#125; 7 .2 循环栅栏CyclicBarrierCyclicBarrier看英文单词可以看出大概就是循环阻塞的意思，在使用中CyclicBarrier的构造方法第一个参数是目标障碍数，每次执行CyclicBarrier一次障碍数会加一，如果达到了目标障碍数，才会执行cyclicBarrier.await()之后的语句。可以将CyclicBarrier理解为加 1 操作 场景: 集齐 7 颗龙珠就可以召唤神龙CyclicBarrierDemo package com.atguigu.test;import java.util.concurrent.CyclicBarrier;##### /*** CyclicBarrierDemo案列*/public class CyclicBarrierDemo &#123;##### //定义神龙召唤需要的龙珠总数private final static int NUMBER = 7;##### /**##### * 集齐 7 颗龙珠就可以召唤神龙* @param args*/public static void main(String[] args) &#123;//定义循环栅栏CyclicBarrier cyclicBarrier = new CyclicBarrier(NUMBER, () -&gt;&#123;System.out.println(&quot;集齐&quot; + NUMBER + &quot;颗龙珠,现在召唤神龙!!!!!!!!!&quot;);&#125;);//定义 7 个线程分别去收集龙珠for (int i = 1; i &lt;= 7; i++) &#123;new Thread(()-&gt;&#123;try &#123;if(Thread.currentThread().getName().equals(&quot;龙珠 3 号&quot;))&#123;System.out.println(&quot;龙珠 3 号抢夺战开始,孙悟空开启超级赛亚人模式!&quot;);Thread.sleep(5000);System.out.println(&quot;龙珠 3 号抢夺战结束,孙悟空打赢了,拿到了龙珠 3号!&quot;);&#125;else&#123;System.out.println(Thread.currentThread().getName() + &quot;收集到了!!!!&quot;);&#125;cyclicBarrier.await();&#125;catch (Exception e)&#123;e.printStackTrace();&#125;&#125;, &quot;龙珠&quot; + i + &quot;号&quot;).start();&#125;&#125;&#125; 7 .3 信号灯SemaphoreSemaphore的构造方法中传入的第一个参数是最大信号量（可以看成最大线程池），每个信号量初始化为一个最多只能分发一个许可证。使用acquire方法获得许可证，release方法释放许可 场景: 抢车位, 6部汽车 3 个停车位SemaphoreDemo package com.atguigu.test;import java.util.concurrent.Semaphore;##### /*** Semaphore案列*/public class SemaphoreDemo &#123;##### /**##### * 抢车位, 10部汽车 1 个停车位* @param args*/public static void main(String[] args) throws Exception&#123;//定义 3 个停车位Semaphore semaphore = new Semaphore(1);//模拟 6 辆汽车停车for (int i = 1; i &lt;= 10; i++) &#123;Thread.sleep(100);//停车new Thread(() -&gt;&#123;try &#123;System.out.println(Thread.currentThread().getName() + &quot;找车位ing&quot;);semaphore.acquire();System.out.println(Thread.currentThread().getName() + &quot;汽车停车成功!&quot;);Thread.sleep(10000);&#125;catch (Exception e)&#123;e.printStackTrace();&#125;finally &#123;System.out.println(Thread.currentThread().getName() + &quot;溜了溜了&quot;);semaphore.release();&#125;&#125;, &quot;汽车&quot; + i).start();##### &#125;##### &#125;##### &#125; 8 读写锁8 .1 读写锁介绍现实中有这样一种场景：对共享资源有读和写的操作，且写操作没有读操作那么频繁。在没有写操作的时候，多个线程同时读一个资源没有任何问题，所以应该允许多个线程同时读取共享资源；但是如果一个线程想去写这些共享资源，就不应该允许其他线程对该资源进行读和写的操作了。 针对这种场景， JAVA的并发包提供了读写锁ReentrantReadWriteLock，它表示两个锁，一个是读操作相关的锁，称为共享锁；一个是写相关的锁，称为排他锁 1. 线程进入读锁的前提条件： 没有其他线程的写锁 没有写请求, 或者&#x3D;&#x3D;有写请求，但调用线程和持有锁的线程是同一个(可重入锁)。&#x3D;&#x3D; 2. 线程进入写锁的前提条件： 没有其他线程的读锁 没有其他线程的写锁 而读写锁有以下三个重要的特性： （ 1 ）公平选择性：支持非公平（默认）和公平的锁获取方式，吞吐量还是非公平优于公平。 （ 2 ）重进入：读锁和写锁都支持线程重进入。 （ 3 ）锁降级：遵循获取写锁、获取读锁再释放写锁的次序，写锁能够降级成为读锁。 8 .2 ReentrantReadWriteLockReentrantReadWriteLock 类的整体结构 public class ReentrantReadWriteLock implements ReadWriteLock, java.io.Serializable &#123; /** * 读锁 */ private final ReentrantReadWriteLock.ReadLock readerLock; /** * 写锁 */ private final ReentrantReadWriteLock.WriteLock writerLock; final Sync sync; /** * 使用默认（非公平）的排序属性创建一个新的 ReentrantReadWriteLock */ public ReentrantReadWriteLock() &#123; this(false); &#125; /** * 使用给定的公平策略创建一个新的 ReentrantReadWriteLock */ public ReentrantReadWriteLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); readerLock = new ReadLock(this); writerLock = new WriteLock(this); &#125; /** * 返回用于写入操作的锁 */ public ReentrantReadWriteLock.WriteLock writeLock() &#123; return writerLock; &#125; /** * 返回用于读取操作的锁 */ public ReentrantReadWriteLock.ReadLock readLock() &#123; return readerLock; &#125; abstract static class Sync extends AbstractQueuedSynchronizer &#123; &#125; static final class NonfairSync extends Sync &#123; &#125; static final class FairSync extends Sync &#123; &#125; public static class ReadLock implements Lock, java.io.Serializable &#123; &#125; public static class WriteLock implements Lock, java.io.Serializable &#123; &#125;&#125; 可以看到，ReentrantReadWriteLock实现了ReadWriteLock接口，ReadWriteLock接口定义了获取读锁和写锁的规范，具体需要实现类去实现；同时其还实现了Serializable接口，表示可以进行序列化，在源代码中可以看到ReentrantReadWriteLock实现了自己的序列化逻辑。 8 .3 入门案例场景: 使用ReentrantReadWriteLock 对一个hashmap进行读和写操作 8 .3.1 实现案例// 资源类class MyCache &#123; // 创建 map 集合 private volatile Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); // 创建读写锁对象 private ReadWriteLock rwLock = new ReentrantReadWriteLock(); // 放数据 public void put(String key, Object value) &#123; // 添加写锁 rwLock.writeLock().lock(); try &#123; System.out.println(Thread.currentThread().getName() + &quot; &quot; + key); // 暂停一会 TimeUnit.MICROSECONDS.sleep(300); // 放数据 map.put(key, value); System.out.println(Thread.currentThread().getName() + &quot; &quot; + key); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; // 释放写锁 rwLock.writeLock().unlock(); &#125; &#125; // 取数据 public Object get(String key) &#123; // 添加读锁 rwLock.readLock().lock(); Object result = null; try &#123; System.out.println(Thread.currentThread().getName() + &quot; &quot; +key); // 暂停一会 TimeUnit.MICROSECONDS.sleep(300); result = map.get(key); System.out.println(Thread.currentThread().getName() + &quot; &quot; +key); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123;// 释放读锁 rwLock.readLock().unlock(); &#125; return result; &#125;&#125; 8. 4 小结(重要) 在线程持有读锁的情况下，该线程不能取得写锁(因为获取写锁的时候，如果发现当前的读锁被占用，就马上获取失败，不管读锁是不是被当前线程持有)。 在线程持有写锁的情况下，该线程可以继续获取读锁（获取读锁时如果发现写锁被占用，只有写锁没有被当前线程占用的情况才会获取失败）。 原因: 当线程获取读锁的时候，可能有其他线程同时也在持有读锁，因此不能把获取读锁的线程“升级”为写锁；而对于获得写锁的线程，它一定独占了读写锁，因此可以继续让它获取读锁，当它同时获取了写锁和读锁后，还可以先释放写锁继续持有读锁，这样一个写锁就“降级”为了读锁。 9 阻塞队列9 .1 BlockingQueue简介Concurrent包中，BlockingQueue很好的解决了多线程中，如何高效安全“传输”数据的问题。通过这些高效并且线程安全的队列类，为我们快速搭建高质量的多线程程序带来极大的便利。本文详细介绍了BlockingQueue家庭中的所有成员，包括他们各自的功能以及常见使用场景。阻塞队列，顾名思义，首先它是一个队列, 通过一个共享的队列，可以使得数据由队列的一端输入，从另外一端输出；当队列是空的，从队列中获取元素的操作将会被阻塞，当队列是满的，从队列中添加元素的操作将会被阻塞。试图从空的队列中获取元素的线程将会被阻塞，直到其他线程往空的队列插入新的元素。试图向已满的队列中添加新元素的线程将会被阻塞，直到其他线程从队列中移除一个或多个元素或者完全清空，使队列变得空闲起来并后续新增 常用的队列主要有以下两种： 先进先出（FIFO）：先插入的队列的元素也最先出队列，类似于排队的功能。 从某种程度上来说这种队列也体现了一种公平性 后进先出（LIFO）：后插入队列的元素最先出队列，这种队列优先处理最近发生的事件(栈) 在多线程领域：所谓阻塞，在某些情况下会挂起线程（即阻塞），一旦条件满足，被挂起的线程又会自动被唤起为什么需要BlockingQueue？ 好处是我们不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为这一切BlockingQueue都给你一手包办了 ​ 在concurrent包发布以前，在多线程环境下，我们每个程序员都必须去自己控制这些细节，尤其还要兼顾效率和线程安全，而这会给我们的程序带来不小的复杂度。 ​ 多线程环境中，通过队列可以很容易实现数据共享，比如经典的“生产者”和“消费者”模型中，通过队列可以很便利地实现两者之间的数据共享。假设我们有若干生产者线程，另外又有若干个消费者线程。如果生产者线程需要把准备好的数据共享给消费者线程，利用队列的方式来传递数据，就可以很方便地解决他们之间的数据共享问题。但如果生产者和消费者在某个时间段内，万一发生数据处理速度不匹配的情况呢？理想情况下，如果生产者产出数据的速度大于消费者消费的速度，并且当生产出来的数据累积到一定程度的时候，那么生产者必须暂停等待一下（阻塞生产者线程），以便等待消费者线程把累积的数据处理完毕，反之亦然。 当队列中没有数据的情况下，消费者端的所有线程都会被自动阻塞（挂起），直到有数据放入队列 当队列中填满数据的情况下，生产者端的所有线程都会被自动阻塞（挂起），直到队列中有空的位置，线程被自动唤醒 9 .2 BlockingQueue核心方法![image-20230220142456775](&#x2F;Users&#x2F;dongnan&#x2F;Desktop&#x2F;code&#x2F;Note&#x2F;JUC笔记&#x2F;分析图&#x2F;BlockingQueue 核心方法.png) BlockingQueue的核心方法 ：1.放入数据 offer(anObject):表示如果可能的话,将an Object加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则返回false. （本方法不阻塞当前执行方法的线程） offer(E o, long timeout, TimeUnit unit)：可以设定等待的时间，如果在指定的时间内，还不能往队列中加入BlockingQueue，则返回失败 put(anObject):把an Object加到BlockingQueue里,如果BlockQueue没有空间,则调用此方法的线程被阻断直到BlockingQueue里面有空间再继续. 2.获取数据 poll(time): 取走BlockingQueue里排在首位的对象,若不能立即取出, 则可以等time参数规定的时间,取不到时返回null poll(long timeout, TimeUnit unit)：从BlockingQueue取出一个队首的对象，如果在指定时间内，队列一旦有数据可取，则立即返回队列中的数据。否则直到时间超时还没有数据可取，返回失败。 take(): 取走BlockingQueue里排在首位的对象,若BlockingQueue为空, 阻断进入等待状态直到BlockingQueue有新的数据被加入 ; drainTo(): 一次性从BlockingQueue获取所有可用的数据对象（还可以指定获取数据的个数），通过该方法，可以提升获取数据效率；不需要多次分批加锁或释放锁。 9 .3 入门案例import java.util.ArrayList;import java.util.List;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.TimeUnit;/*** 阻塞队列*/public class BlockingQueueDemo &#123;public static void main(String[] args) throws InterruptedException &#123;// List list = new ArrayList();BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;( 3 );//第一组// System.out.println(blockingQueue.add(&quot;a&quot;));// System.out.println(blockingQueue.add(&quot;b&quot;));// System.out.println(blockingQueue.add(&quot;c&quot;));// System.out.println(blockingQueue.element());//System.out.println(blockingQueue.add(&quot;x&quot;));// System.out.println(blockingQueue.remove());// System.out.println(blockingQueue.remove());// System.out.println(blockingQueue.remove());// System.out.println(blockingQueue.remove());// 第二组// System.out.println(blockingQueue.offer(&quot;a&quot;));// System.out.println(blockingQueue.offer(&quot;b&quot;));// System.out.println(blockingQueue.offer(&quot;c&quot;));// System.out.println(blockingQueue.offer(&quot;x&quot;));// System.out.println(blockingQueue.poll());// System.out.println(blockingQueue.poll());// System.out.println(blockingQueue.poll());// System.out.println(blockingQueue.poll());// 第三组// blockingQueue.put(&quot;a&quot;);// blockingQueue.put(&quot;b&quot;);// blockingQueue.put(&quot;c&quot;);// //blockingQueue.put(&quot;x&quot;);// System.out.println(blockingQueue.take());// System.out.println(blockingQueue.take());// System.out.println(blockingQueue.take());// System.out.println(blockingQueue.take());// 第四组System.out.println(blockingQueue.offer(&quot;a&quot;));System.out.println(blockingQueue.offer(&quot;b&quot;));System.out.println(blockingQueue.offer(&quot;c&quot;));System.out.println(blockingQueue.offer(&quot;a&quot;,3L, TimeUnit.SECONDS));##### &#125;##### &#125; 9 .4 常见的BlockingQueue9 .4.1 ArrayBlockingQueue(常用)​ 基于数组的阻塞队列实现，在ArrayBlockingQueue内部，维护了一个定长数组，以便缓存队列中的数据对象，这是一个常用的阻塞队列，除了一个定长数组外，ArrayBlockingQueue内部还保存着两个整形变量，分别标识着队列的头部和尾部在数组中的位置。 ​ ArrayBlockingQueue在生产者放入数据和消费者获取数据，都是共用同一个锁对象，由此也意味着两者无法真正并行运行，这点尤其不同于LinkedBlockingQueue；按照实现原理来分析，ArrayBlockingQueue完全可以采用分离锁，从而实现生产者和消费者操作的完全并行运行。Doug Lea之所以没这样去做，也许是因为ArrayBlockingQueue的数据写入和获取操作已经足够轻巧，以至于引入独立的锁机制，除了给代码带来额外的复杂性外，其在性能上完全占不到任何便宜。 ArrayBlockingQueue和LinkedBlockingQueue间还有一个明显的不同之处在于，前者在插入或删除元素时不会产生或销毁任何额外的对象实例，而后者则会生成一个额外的Node对象。这在长时间内需要高效并发地处理大批量数据的系统中，其对于GC的影响还是存在一定的区别。而在创建ArrayBlockingQueue时，我们还可以控制对象的内部锁是否采用公平锁，默认采用非公平锁。 &#x3D;&#x3D;一句话总结: 由数组结构组成的有界阻塞队列。&#x3D;&#x3D;9 .4.2 LinkedBlockingQueue(常用)​ 基于链表的阻塞队列，同ArrayListBlockingQueue类似，其内部也维持着一个数据缓冲队列（该队列由一个链表构成），当生产者往队列中放入一个数据时，队列会从生产者手中获取数据，并缓存在队列内部，而生产者立即返回；只有当队列缓冲区达到最大值缓存容量时（LinkedBlockingQueue可以通过构造函数指定该值），才会阻塞生产者队列，直到消费者从队列中消费掉一份数据，生产者线程会被唤醒，反之对于消费者这端的处理也基于同样的原理。而LinkedBlockingQueue之所以能够高效的处理并发数据，还因为其对于生产者端和消费者端分别采用了独立的锁来控制数据同步，这也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。 ArrayBlockingQueue和LinkedBlockingQueue是两个最普通也是最常用的阻塞队列，一般情况下，在处理多线程间的生产者消费者问题，使用这两个类足以。 &#x3D;&#x3D;一句话总结: 由链表结构组成的有界（但大小默认值为integer.MAX_VALUE）阻塞队列。&#x3D;&#x3D;9.4.3 DelayQueueDelayQueue中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素。DelayQueue是一个没有大小限制的队列，因此往队列中插入数据的操作（生产者）永远不会被阻塞，而只有获取数据的操作（消费者）才会被阻塞。 &#x3D;&#x3D;一句话总结: 使用优先级队列实现的延迟无界阻塞队列。&#x3D;&#x3D;9.4.4 PriorityBlockingQueue基于优先级的阻塞队列（优先级的判断通过构造函数传入的Compator对象来决定），但需要注意的是PriorityBlockingQueue&#x3D;&#x3D;并不会阻塞数据生产者，而只会在没有可消费的数据时，阻塞数据的消费者&#x3D;&#x3D;。 因此使用的时候要特别注意， &#x3D;&#x3D;生产者生产数据的速度绝对不能快于消费者消费数据的速度&#x3D;&#x3D; ，否则时间一长，会最终耗尽所有的可用堆内存空间。在实现PriorityBlockingQueue时，内部控制线程同步的锁采用的是 公平锁 。 &#x3D;&#x3D;一句话总结: 支持优先级排序的无界阻塞队列。&#x3D;&#x3D;9 .4.5 SynchronousQueue​ 一种无缓冲的等待队列，类似于无中介的直接交易，有点像原始社会中的生产者和消费者，生产者拿着产品去集市销售给产品的最终消费者，而消费者必须亲自去集市找到所要商品的直接生产者，如果一方没有找到合适的目标，那么对不起，大家都在集市等待。相对于有缓冲的BlockingQueue来说，少了一个中间经销商的环节（缓冲区），如果有经销商，生产者直接把产品批发给经销商，而无需在意经销商最终会将这些产品卖给那些消费者，由于经销商可以库存一部分商品，因此相对于直接交易模式，总体来说采用中间经销商的模式会吞吐量高一些（可以批量买卖）；但另一方面，又因为经销商的引入，使得产品从生产者到消费者中间增加了额外的交易环节，单个产品的及时响应性能可能会降低。 ​ 声明一个SynchronousQueue有两种不同的方式，它们之间有着不太一样的行为。 公平模式和非公平模式的区别: 公平模式：SynchronousQueue会采用公平锁，并配合一个FIFO队列来阻塞多余的生产者和消费者，从而体系整体的公平策略； 非公平模式（SynchronousQueue默认）：SynchronousQueue采用非公平锁，同时配合一个LIFO队列来管理多余的生产者和消费者，而后一种模式，如果生产者和消费者的处理速度有差距，则很容易出现饥渴的情况，即可能有某些生产者或者是消费者的数据永远都得不到处理。 &#x3D;&#x3D;一句话总结: 不存储元素的阻塞队列，也即单个元素的队列。&#x3D;&#x3D;9.4.6 LinkedTransferQueue​ LinkedTransferQueue是一个由链表结构组成的无界阻塞TransferQueue队列。相对于其他阻塞队列，LinkedTransferQueue多了tryTransfer和transfer方法。LinkedTransferQueue采用一种预占模式。意思就是消费者线程取元素时，如果队列不为空，则直接取走数据，若队列为空，那就生成一个节点（节点元素为null）入队，然后消费者线程被等待在这个节点上，后面生产者线程入队时发现有一个元素为null的节点，生产者线程就不入队了，直接就将元素填充到该节点，并唤醒该节点等待的线程，被唤醒的消费者线程取走元素，从调用的方法返回。 &#x3D;&#x3D;一句话总结: 由链表组成的无界阻塞队列。&#x3D;&#x3D;9.4.7 LinkedBlockingDequeLinkedBlockingDeque是一个由链表结构组成的双向阻塞队列，即可以从队列的两端插入和移除元素。对于一些指定的操作，在插入或者获取队列元素时如果队列状态不允许该操作可能会阻塞住该线程直到队列状态变更为允许操作，这里的阻塞一般有两种情况 插入元素时: 如果当前队列已满将会进入阻塞状态，一直等到队列有空的位置时再讲该元素插入，该操作可以通过设置超时参数，超时后返回 false 表示操作失败，也可以不设置超时参数一直阻塞，中断后抛出InterruptedException异常 读取元素时: 如果当前队列为空会阻塞住直到队列不为空然后返回元素，同样可以通过设置超时参数 &#x3D;&#x3D;一句话总结: 由链表组成的双向阻塞队列&#x3D;&#x3D;9 .5 小结 在多线程领域：所谓阻塞，在某些情况下会挂起线程（即阻塞），一旦条件满足，被挂起的线程又会自动被唤起 为什么需要BlockingQueue? 在concurrent包发布以前，在多线程环境下，我们每个程序员都必须去自己控制这些细节，尤其还要兼顾效率和线程安全，而这会给我们的程序带来不小的复杂度。使用后我们不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为这一切BlockingQueue都给你一手包办了 10 ThreadPool 线程池10 .1 线程池简介​ 线程池（英语：thread pool）：一种线程使用模式。线程过多会带来调度开销，进而影响缓存局部性和整体性能。而线程池维护着多个线程，等待着监督管理者分配可并发执行的任务。这避免了在处理短时间任务时创建与销毁线程的代价。线程池不仅能够保证内核的充分利用，还能防止过分调度。 ​ 例子： 10 年前单核CPU电脑，假的多线程，像马戏团小丑玩多个球，CPU需要来回切换。 现在是多核电脑，多个线程各自跑在独立的CPU上，不用切换效率高。 ​ 线程池的优势： 线程池做的工作只要是控制运行的线程数量，处理过程中将任务放入队列，然后在线程创建后启动这些任务，如果线程数量超过了最大数量，超出数量的线程排队等候，等其他线程执行完毕，再从队列中取出任务来执行。 它的主要特点为： 降低资源消耗: 通过重复利用已创建的线程降低线程创建和销毁造成的销耗。 提高响应速度: 当任务到达时，任务可以不需要等待线程创建就能立即执行。 提高线程的可管理性: 线程是稀缺资源，如果无限制的创建，不仅会销耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 Java中的线程池是通过Executor框架实现的，该框架中用到了Executor，Executors，ExecutorService，ThreadPoolExecutor这几个类 10 .2 线程池参数说明本次介绍 5 种类型的线程池 10 .2.1常用参数(重点) corePoolSize线程池的核心线程数 maximumPoolSize能容纳的最大线程数 keepAliveTime空闲线程存活时间 unit 存活的时间单位 workQueue 存放提交但未执行任务的队列 threadFactory 创建线程的工厂类 handler 等待队列满后的拒绝策略 ​ 线程池中，有三个重要的参数，决定影响了拒绝策略：corePoolSize - 核心线程数，也即最小的线程数。workQueue - 阻塞队列 。 maximumPoolSize -最大线程数 ​ 当提交任务数大于 corePoolSize 的时候，会优先将任务放到 workQueue 阻塞队列中。当阻塞队列饱和后，会扩充线程池中线程数，直到达到maximumPoolSize 最大线程数配置。此时，再多余的任务，则会触发线程池的拒绝策略了。 ​ 总结起来，也就是一句话， 当提交的任务数大于（workQueue.size() +maximumPoolSize ），就会触发线程池的拒绝策略 。 10.2.2 拒绝策略(重点)​ CallerRunsPolicy : 当触发拒绝策略，只要线程池没有关闭的话，则使用调用线程直接运行任务。一般并发比较小，性能要求不高，不允许失败。但是，由于调用者自己运行任务，如果任务提交速度过快，可能导致程序阻塞，性能效率上必然的损失较大 ​ AbortPolicy : 丢弃任务，并抛出拒绝执行 RejectedExecutionException 异常信息。线程池默认的拒绝策略。必须处理好抛出的异常，否则会打断当前的执行流程，影响后续的任务执行。 ​ DiscardPolicy : 直接丢弃，其他啥都没有 ​ DiscardOldestPolicy : 当触发拒绝策略，只要线程池没有关闭的话，丢弃阻塞队列 workQueue 中最老的一个任务，并将新任务加入 10.3 线程池的种类与创建10.3.1 newCachedThreadPool(常用)作用 ：创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程. 特点 : 线程池中数量没有固定，可达到最大值（Interger. MAX_VALUE） 线程池中的线程可进行缓存重复利用和回收（回收默认时间为 1 分钟） 当线程池中，没有可用线程，会重新创建一个线程 创建方式：/** * 可缓存线程池 * * @return */public static ExecutorService newCachedThreadPool()&#123; /** * corePoolSize线程池的核心线程数 * maximumPoolSize能容纳的最大线程数 * keepAliveTime空闲线程存活时间 * unit 存活的时间单位 * workQueue 存放提交但未执行任务的队列 * threadFactory 创建线程的工厂类:可以省略 * handler 等待队列满后的拒绝策略:可以省略 */ return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;&gt;(), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); &#125; 场景: 适用于创建一个可无限扩大的线程池，服务器负载压力较轻，执行时间较短，任务多的场景 10.3.2 newFixedThreadPool(常用)作用 ：创建一个可重用固定线程数的线程池，以共享的无界队列方式来运行这些线程。在任意点，在大多数线程会处于处理任务的活动状态。如果在所有线程处于活动状态时提交附加任务，则在有可用线程之前，附加任务将在队列中等待。如果在关闭前的执行期间由于失败而导致任何线程终止，那么一个新线程将代替它执行后续的任务（如果需要）。在某个线程被显式地关闭之前，池中的线程将一直存在。 特征： 线程池中的线程处于一定的量，可以很好的控制线程的并发量 线程可以重复被使用，在显示关闭之前，都将一直存在 超出一定量的线程被提交时候需在队列中等待 创建方式 ： /** * 固定长度线程池 * * @return */public static ExecutorService newFixedThreadPool()&#123; /** * corePoolSize线程池的核心线程数 * maximumPoolSize能容纳的最大线程数 * keepAliveTime空闲线程存活时间 * unit 存活的时间单位 * workQueue 存放提交但未执行任务的队列 * threadFactory 创建线程的工厂类:可以省略 * handler 等待队列满后的拒绝策略:可以省略 */ return new ThreadPoolExecutor(10, 10, 0L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); &#125; 场景: 适用于可以预测线程数量的业务中，或者服务器负载较重，对线程数有严格限制的场景 10.3.3 newSingleThreadExecutor(常用)作用 ：创建一个使用单个 worker 线程的 Executor，以无界队列方式来运行该线程。（注意，如果因为在关闭前的执行期间出现失败而终止了此单个线程，那么如果需要，一个新线程将代替它执行后续的任务）。可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。与其他等效的newFixedThreadPool不同，可保证无需重新配置此方法所返回的执行程序即可使用其他的线程。 特征： 线程池中最多执行 1 个线程，之后提交的线程活动将会排在队列中以此执行 创建方式： #### /**#### * 单一线程池#### * @return#### */#### public static ExecutorService newSingleThreadExecutor()&#123;#### /**#### * corePoolSize线程池的核心线程数#### * maximumPoolSize能容纳的最大线程数#### * keepAliveTime空闲线程存活时间#### * unit 存活的时间单位#### * workQueue 存放提交但未执行任务的队列#### * threadFactory 创建线程的工厂类:可以省略#### * handler 等待队列满后的拒绝策略:可以省略#### */#### return new ThreadPoolExecutor( 1 ,#### 1 ,#### 0L,#### TimeUnit.SECONDS,#### new LinkedBlockingQueue&lt;&gt;(),#### Executors.defaultThreadFactory(),#### new ThreadPoolExecutor.AbortPolicy());#### &#125; 场景: 适用于需要保证顺序执行各个任务，并且在任意时间点，不会同时有多个线程的场景 10 .3.4 newScheduleThreadPool(了解)作用: 线程池支持定时以及周期性执行任务，创建一个corePoolSize为传入参数，最大线程数为整形的最大数的线程池** 特征: （ 1 ）线程池中具有指定数量的线程，即便是空线程也将保留 （ 2 ）可定时或者延迟执行线程活动 创建方式: #### public static ScheduledExecutorService newScheduledThreadPool(int#### corePoolSize,#### ThreadFactory threadFactory) &#123;#### return new ScheduledThreadPoolExecutor(corePoolSize,#### threadFactory);#### &#125; 场景: 适用于需要多个后台线程执行周期任务的场景 10 .3.5 newWorkStealingPooljdk1.8提供的线程池，底层使用的是ForkJoinPool实现，创建一个拥有多个任务队列的线程池，可以减少连接数，创建当前可用cpu核数的线程来并行执行任务 创建方式: public static ExecutorService newWorkStealingPool(int parallelism) &#123;/*** parallelism：并行级别，通常默认为JVM可用的处理器个数* factory：用于创建ForkJoinPool中使用的线程。* handler：用于处理工作线程未处理的异常，默认为null* asyncMode：用于控制WorkQueue的工作模式:队列---反队列##### */return new ForkJoinPool(parallelism,ForkJoinPool.defaultForkJoinWorkerThreadFactory,null,true);&#125; 场景: 适用于大耗时，可并行执行的场景 10 .4 线程池入门案例场景: 火车站 3 个售票口, 10个用户买票 #### package com.atguigu.test;#### import java.util.concurrent.*;#### /**#### * 入门案例#### */#### public class ThreadPoolDemo1 &#123;#### /**#### * 火车站 3 个售票口, 10个用户买票#### * @param args#### */#### public static void main(String[] args) &#123;#### //定时线程次:线程数量为 3 ---窗口数为 3#### ExecutorService threadService = new ThreadPoolExecutor( 3 ,#### 3 ,#### 60L,#### TimeUnit.SECONDS,#### new LinkedBlockingQueue&lt;&gt;(),#### Executors.defaultThreadFactory(),#### new ThreadPoolExecutor.DiscardOldestPolicy());#### try &#123;#### //10个人买票#### for (int i = 1 ; i &lt;= 10 ; i++) &#123;#### threadService.execute(()-&gt;&#123;#### try &#123;#### System.out.println(Thread.currentThread().getName() + &quot;#### 窗口,开始卖票&quot;);#### Thread.sleep( 5000 );#### System.out.println(Thread.currentThread().getName() + &quot;#### 窗口买票结束&quot;);#### &#125;catch (Exception e)&#123;#### e.printStackTrace();#### &#125;#### &#125;);#### &#125;#### &#125;catch (Exception e)&#123;#### e.printStackTrace();#### &#125;finally &#123;#### //完成后结束#### threadService.shutdown();#### &#125;#### &#125;#### &#125; 10 .5 线程池底层工作原理(重要) 在创建了线程池后，线程池中的线程数为零 当调用execute()方法添加一个请求任务时，线程池会做出如下判断： 2.1 如果正在运行的线程数量小于corePoolSize，那么马上创建线程运行这个任务； 2.2 如果正在运行的线程数量大于或等于corePoolSize，那么将这个任务放入队列； 2.3 如果这个时候队列满了且正在运行的线程数量还小于maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务； 2.4 如果队列满了且正在运行的线程数量大于或等于maximumPoolSize，那么线程池会启动饱和拒绝策略来执行。 当一个线程完成任务时，它会从队列中取下一个任务来执行 当一个线程无事可做超过一定的时间（keepAliveTime）时，线程会判断： 4.1 如果当前运行的线程数大于corePoolSize，那么这个线程就被停掉。 4.2所以线程池的所有任务完成后，它最终会收缩到corePoolSize的大小。 10 .6 注意事项(重要) 项目中创建多线程时，使用常见的三种线程池创建方式，单一、可变、定长都有一定问题，原因是FixedThreadPool和SingleThreadExecutor底层都是用LinkedBlockingQueue实现的，这个队列最大长度为Integer.MAX_VALUE，容易导致OOM。所以实际生产一般自己通过ThreadPoolExecutor的 7 个参数，自定义线程池 创建线程池推荐适用ThreadPoolExecutor及其 7 个参数手动创建 corePoolSize线程池的核心线程数 maximumPoolSize能容纳的最大线程数 keepAliveTime空闲线程存活时间 unit 存活的时间单位 workQueue 存放提交但未执行任务的队列 threadFactory 创建线程的工厂类 handler 等待队列满后的拒绝策略 为什么不允许适用不允许Executors.的方式手动创建线程池,如下图 11 Fork&#x2F;Join11 .1 Fork&#x2F;Join框架简介Fork&#x2F;Join它可以将一个大的任务拆分成多个子任务进行并行处理，最后将子任务结果合并成最后的计算结果，并进行输出。Fork&#x2F;Join框架要完成两件事情： ​\tFork：把一个复杂任务进行分拆，大事化小​\tJoin：把分拆任务的结果进行合并 任务分割 ：首先Fork&#x2F;Join框架需要把大的任务分割成足够小的子任务，如果子任务比较大的话还要对子任务进行继续分割 执行任务并合并结果 ：分割的子任务分别放到双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都放在另外一个队列里，启动一个线程从队列里取数据，然后合并这些数据。 在Java的Fork&#x2F;Join框架中，使用两个类完成上述操作 ForkJoinTask :我们要使用Fork&#x2F;Join框架，首先需要创建一个ForkJoin任务。 该类提供了在任务中执行fork和join的机制。通常情况下我们不需要直接集成ForkJoinTask类，只需要继承它的子类，Fork&#x2F;Join框架提供了两个子类： a.RecursiveAction：用于没有返回结果的任务 b.RecursiveTask:用于有返回结果的任务 ForkJoinPool :ForkJoinTask需要通过ForkJoinPool来执行 RecursiveTask : 继承后可以实现递归(自己调自己)调用的任务 Fork&#x2F;Join框架的实现原理 ForkJoinPool由ForkJoinTask数组和ForkJoinWorkerThread数组组成，ForkJoinTask数组负责将存放以及将程序提交给ForkJoinPool，而 ForkJoinWorkerThread负责执行这些任务。 11 .2 Fork方法Fork方法的实现原理： 当我们调用ForkJoinTask的fork方法时，程序会把任务放在ForkJoinWorkerThread的pushTask的 workQueue 中，异步地执行这个任务，然后立即返回结果 public final ForkJoinTask&lt;V&gt; fork() &#123;Thread t;if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread)((ForkJoinWorkerThread)t).workQueue.push(this);elseForkJoinPool.common.externalPush(this);return this;&#125; pushTask方法把当前任务存放在ForkJoinTask数组队列里。然后再调用ForkJoinPool的signalWork()方法唤醒或创建一个工作线程来执行任务。代码如下： final void push(ForkJoinTask&lt;?&gt; task) &#123;ForkJoinTask&lt;?&gt;[] a; ForkJoinPool p;int b = base, s = top, n;if ((a = array) != null) &#123; // ignore if queue removedint m = a.length - 1 ; // fenced write for task visibilityU.putOrderedObject(a, ((m &amp; s) &lt;&lt; ASHIFT) + ABASE, task);U.putOrderedInt(this, QTOP, s + 1 );if ((n = s - b) &lt;= 1 ) &#123;if ((p = pool) != null)p.signalWork(p.workQueues, this);//执行&#125;else if (n &gt;= m)growArray();&#125;&#125; 11 .3 join方法Join方法的主要作用是阻塞当前线程并等待获取结果。让我们一起看看ForkJoinTask的join方法的实现，代码如下： public final V join() &#123;int s;if ((s = doJoin() &amp; DONE_MASK) != NORMAL)reportException(s);return getRawResult();&#125; 它首先调用doJoin方法，通过doJoin()方法得到当前任务的状态来判断返回什么结果，任务状态有 4 种： &#x3D;&#x3D;已完成（NORMAL）、被取消（CANCELLED）、信号（SIGNAL）和出现异常（EXCEPTIONAL）&#x3D;&#x3D; 如果任务状态是已完成，则直接返回任务结果。 如果任务状态是被取消，则直接抛出CancellationException 如果任务状态是抛出异常，则直接抛出对应的异常 让我们分析一下doJoin方法的实现 #### private int doJoin() &#123;#### int s; Thread t; ForkJoinWorkerThread wt; ForkJoinPool.WorkQueue#### w;#### return (s = status) &lt; 0? s :#### ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread)?#### (w = (wt = (ForkJoinWorkerThread)t).workQueue).#### tryUnpush(this) &amp;&amp; (s = doExec()) &lt; 0? s :#### wt.pool.awaitJoin(w, this, 0L) :#### externalAwaitDone();#### &#125;#### final int doExec() &#123;#### int s; boolean completed;#### if ((s = status) &gt;= 0 ) &#123;#### try &#123;#### completed = exec();#### &#125; catch (Throwable rex) &#123;#### return setExceptionalCompletion(rex);#### &#125;#### if (completed)#### s = setCompletion(NORMAL);#### &#125;#### return s;#### &#125; 在doJoin()方法流程如下: 首先通过查看任务的状态，看任务是否已经执行完成，如果执行完成，则直接返回任务状态； 如果没有执行完，则从任务数组里取出任务并执行。 如果任务顺利执行完成，则设置任务状态为NORMAL，如果出现异常，则记录异常，并将任务状态设置为EXCEPTIONAL。 11 .4 Fork&#x2F;Join框架的异常处理ForkJoinTask在执行的时候可能会抛出异常，但是我们没办法在主线程里直接捕获异常，所以ForkJoinTask提供了isCompletedAbnormally()方法来检查任务是否已经抛出异常或已经被取消了，并且可以通过ForkJoinTask的getException方法获取异常。getException方法返回Throwable对象，如果任务被取消了则返回CancellationException。如果任务没有完成或者没有抛出异常则返回null。 11 .5 入门案例场景: 生成一个计算任务，计算1+2+3………+1000 , &#x3D;&#x3D;每 100 个数切分一个子任务&#x3D;&#x3D; package com.atguigu.test;import java.util.concurrent.RecursiveTask;/*** 递归累加 */ public class TaskExample extends RecursiveTask&lt;Long&gt; &#123; private int start; private int end; private long sum; /*** 构造函数* @param start* @param end */ public TaskExample(int start, int end)&#123; this.start = start; this.end = end; &#125; /*** The main computation performed by this task. ** @return the result of the computation */@Overrideprotected Long compute() &#123;System.out.println(&quot;任务&quot; + start + &quot;=========&quot; + end + &quot;累加开始&quot;);//大于 100 个数相加切分,小于直接加if(end - start &lt;= 100 )&#123;for (int i = start; i &lt;= end; i++) &#123;//累加sum += i;&#125;&#125;else &#123;//切分为 2 块int middle = start + 100 ;//递归调用,切分为 2 个小任务TaskExample taskExample1 = new TaskExample(start, middle);TaskExample taskExample2 = new TaskExample(middle + 1 , end);//执行:异步taskExample1.fork();taskExample2.fork();//同步阻塞获取执行结果sum = taskExample1.join() + taskExample2.join();&#125;//加完返回return sum;&#125;&#125;package com.atguigu.test;import java.util.concurrent.ForkJoinPool;import java.util.concurrent.ForkJoinTask;##### /**##### * 分支合并案例##### */public class ForkJoinPoolDemo &#123;##### /**##### * 生成一个计算任务，计算1+2+3.........+1000* @param args*/public static void main(String[] args) &#123;//定义任务TaskExample taskExample = new TaskExample(1, 1000);//定义执行对象ForkJoinPool forkJoinPool = new ForkJoinPool();//加入任务执行ForkJoinTask&lt;Long&gt; result = forkJoinPool.submit(taskExample);//输出结果try &#123;System.out.println(result.get());&#125;catch (Exception e)&#123;e.printStackTrace();&#125;finally &#123;forkJoinPool.shutdown();&#125;&#125;&#125; 12 CompletableFuture12 .1 CompletableFuture简介CompletableFuture在Java里面被用于异步编程，异步通常意味着非阻塞，可以使得我们的任务单独运行在与主线程分离的其他线程中，并且通过回调可以在主线程中得到异步任务的执行状态，是否完成，和是否异常等信息。 CompletableFuture实现了Future, CompletionStage接口，实现了Future接口就可以兼容现在有线程池框架，而CompletionStage接口才是异步编程的接口抽象，里面定义多种异步方法，通过这两者集合，从而打造出了强大的CompletableFuture类。 12 .2 Future与CompletableFutureFutrue在Java里面，通常用来表示一个异步任务的引用，比如我们将任务提交到线程池里面，然后我们会得到一个Futrue，在Future里面有isDone方法来 判断任务是否处理结束，还有get方法可以一直阻塞直到任务结束然后获取结果，但整体来说这种方式，还是同步的，因为需要客户端不断阻塞等待或者不断轮询才能知道任务是否完成。 Future的主要缺点如下： （ 1 ）不支持手动完成 我提交了一个任务，但是执行太慢了，我通过其他路径已经获取到了任务结果，现在没法把这个任务结果通知到正在执行的线程，所以必须主动取消或者一直等待它执行完成 （ 2 ）不支持进一步的非阻塞调用 通过Future的get方法会一直阻塞到任务完成，但是想在获取任务之后执行额外的任务，因为Future不支持回调函数，所以无法实现这个功能 （ 3 ）不支持链式调用 对于Future的执行结果，我们想继续传到下一个Future处理使用，从而形成一个链式的pipline调用，这在Future中是没法实现的。 （ 4 ）不支持多个Future合并 比如我们有 10 个Future并行执行，我们想在所有的Future运行完毕之后，执行某些函数，是没法通过Future实现的。 （ 5 ）不支持异常处理 Future的API没有任何的异常处理的api，所以在异步运行时，如果出了问题是不好定位的。 12 .3 CompletableFuture入门12 .3.1 使用CompletableFuture场景:主线程里面创建一个CompletableFuture，然后主线程调用get方法会阻塞，最后我们在一个子线程中使其终止。 /**主线程里面创建一个CompletableFuture，然后主线程调用get方法会阻塞，最后我们在一个子线程中使其终止@param args*/public static void main(String[] args) throws Exception&#123;CompletableFuture&lt;String&gt; future = new CompletableFuture&lt;&gt;();new Thread(() -&gt; &#123;try&#123;System.out.println(Thread.currentThread().getName() + &quot;子线程开始干活&quot;);//子线程睡 5 秒Thread.sleep( 5000 );//在子线程中完成主线程future.complete(&quot;success&quot;);&#125;catch (Exception e)&#123;e.printStackTrace();&#125;&#125;, &quot;A&quot;).start();//主线程调用get方法阻塞System.out.println(&quot;主线程调用get方法获取结果为: &quot; + future.get());System.out.println(&quot;主线程完成,阻塞结束!!!!!!&quot;);&#125; 12 .3.2 没有返回值的异步任务#### /**#### * 没有返回值的异步任务#### * @param args#### */#### public static void main(String[] args) throws Exception&#123;#### System.out.println(&quot;主线程开始&quot;);#### //运行一个没有返回值的异步任务#### CompletableFuture&lt;Void&gt; future = CompletableFuture.runAsync(() -#### &gt; &#123;#### try &#123;#### System.out.println(&quot;子线程启动干活&quot;);#### Thread.sleep(5000);#### System.out.println(&quot;子线程完成&quot;);#### &#125; catch (Exception e) &#123;#### e.printStackTrace();#### &#125;#### &#125;);#### //主线程阻塞#### future.get();#### System.out.println(&quot;主线程结束&quot;);#### &#125; 12 .3.3 有返回值的异步任务#### /**#### * 没有返回值的异步任务#### * @param args#### */#### public static void main(String[] args) throws Exception&#123;#### System.out.println(&quot;主线程开始&quot;);#### //运行一个有返回值的异步任务#### CompletableFuture&lt;String&gt; future =#### CompletableFuture.supplyAsync(() -&gt; &#123;#### try &#123;#### System.out.println(&quot;子线程开始任务&quot;);#### Thread.sleep(5000);#### &#125; catch (Exception e) &#123;#### e.printStackTrace();#### &#125;#### return &quot;子线程完成了!&quot;;#### &#125;);#### //主线程阻塞#### String s = future.get();#### System.out.println(&quot;主线程结束, 子线程的结果为:&quot; + s);#### &#125; 12 .3.4 线程依赖当一个线程依赖另一个线程时，可以使用 thenApply 方法来把这两个线程串行化。 #### private static Integer num = 10;#### /**#### * 先对一个数加10,然后取平方#### * @param args#### */#### public static void main(String[] args) throws Exception&#123;#### System.out.println(&quot;主线程开始&quot;);#### CompletableFuture&lt;Integer&gt; future =#### CompletableFuture.supplyAsync(() -&gt; &#123;#### try &#123;#### System.out.println(&quot;加 10 任务开始&quot;);#### num += 10;#### &#125; catch (Exception e) &#123;#### e.printStackTrace();#### &#125;#### return num;#### &#125;).thenApply(integer -&gt; &#123;#### return num * num;#### &#125;);#### Integer integer = future.get();#### System.out.println(&quot;主线程结束, 子线程的结果为:&quot; + integer);#### &#125; 12 .3.5 消费处理结果thenAccept 消费处理结果, 接收任务的处理结果，并消费处理，无返回结果。 public static void main(String[] args) throws Exception&#123;System.out.println(&quot;主线程开始&quot;);CompletableFuture.supplyAsync(() -&gt; &#123;try &#123;System.out.println(&quot;加 10 任务开始&quot;);num += 10;&#125; catch (Exception e) &#123;e.printStackTrace();&#125;return num;&#125;).thenApply(integer -&gt; &#123;return num * num;&#125;).thenAccept(new Consumer&lt;Integer&gt;() &#123;@Overridepublic void accept(Integer integer) &#123;System.out.println(&quot;子线程全部处理完成,最后调用了accept,结果为:&quot; +integer);&#125;&#125;);&#125; 12 .3.6 异常处理exceptionally异常处理,出现异常时触发public static void main(String[] args) throws Exception&#123;System.out.println(&quot;主线程开始&quot;);CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123;int i= 1/0;System.out.println(&quot;加 10 任务开始&quot;);num += 10;return num;&#125;).exceptionally(ex -&gt; &#123;System.out.println(ex.getMessage());return -1;&#125;);System.out.println(future.get());&#125;handle类似于thenAccept/thenRun方法,是最后一步的处理调用,但是同时可以处理异常public static void main(String[] args) throws Exception&#123;System.out.println(&quot;主线程开始&quot;);CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123;System.out.println(&quot;加 10 任务开始&quot;);num += 10 ;return num;&#125;).handle((i,ex) -&gt;&#123;System.out.println(&quot;进入handle方法&quot;);if(ex != null)&#123;System.out.println(&quot;发生了异常,内容为:&quot; + ex.getMessage());return - 1 ;&#125;else&#123;System.out.println(&quot;正常完成,内容为: &quot; + i);return i;&#125;&#125;);System.out.println(future.get());&#125; 12 .3.7 结果合并thenCompose合并两个有依赖关系的CompletableFutures的执行结果 public static void main(String[] args) throws Exception&#123;System.out.println(&quot;主线程开始&quot;);//第一步加 10CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123;System.out.println(&quot;加 10 任务开始&quot;);num += 10 ;return num;&#125;);//合并CompletableFuture&lt;Integer&gt; future1 = future.thenCompose(i - &gt;//再来一个CompletableFutureCompletableFuture.supplyAsync(() -&gt; &#123;return i + 1 ;&#125;));System.out.println(future.get());System.out.println(future1.get());&#125; thenCombine合并两个没有依赖关系的CompletableFutures任务 public static void main(String[] args) throws Exception&#123;System.out.println(&quot;主线程开始&quot;);CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; &#123;System.out.println(&quot;加 10 任务开始&quot;);num += 10 ;return num;&#125;);CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; &#123;System.out.println(&quot;乘以 10 任务开始&quot;);num = num * 10 ;return num;&#125;);//合并两个结果CompletableFuture&lt;Object&gt; future = job1.thenCombine(job2, newBiFunction&lt;Integer, Integer, List&lt;Integer&gt;&gt;() &#123;@Overridepublic List&lt;Integer&gt; apply(Integer a, Integer b) &#123;List&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.add(a);list.add(b);return list;&#125;&#125;);System.out.println(&quot;合并结果为:&quot; + future.get());&#125; 合并多个任务的结果allOf与anyOf allOf: 一系列独立的future任务，等其所有的任务执行完后做一些事情 /*** 先对一个数加10,然后取平方* @param args */ public static void main(String[] args) throws Exception&#123; System.out.println(&quot;主线程开始&quot;); List&lt;CompletableFuture&gt; list = new ArrayList&lt;&gt;(); CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(&quot;加 10 任务开始&quot;); num += 10 ; return num; &#125;); list.add(job1);CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; &#123;System.out.println(&quot;乘以 10 任务开始&quot;);num = num * 10 ;return num;&#125;);list.add(job2);CompletableFuture&lt;Integer&gt; job3 = CompletableFuture.supplyAsync(() -&gt; &#123;System.out.println(&quot;减以 10 任务开始&quot;);num = num * 10 ;return num;&#125;);list.add(job3);CompletableFuture&lt;Integer&gt; job4 = CompletableFuture.supplyAsync(() -&gt; &#123;System.out.println(&quot;除以 10 任务开始&quot;);num = num * 10 ;return num;&#125;);list.add(job4);//多任务合并List&lt;Integer&gt; collect =list.stream().map(CompletableFuture&lt;Integer&gt;::join).collect(Collectors.toList());System.out.println(collect);&#125; anyOf : 只要在多个future里面有一个返回，整个任务就可以结束，而不需要等到每一个future结束 /*** 先对一个数加10,然后取平方* @param args */ public static void main(String[] args) throws Exception&#123; System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt;[] futures = new CompletableFuture[ 4 ]; CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; &#123; try&#123; Thread.sleep( 5000 ); System.out.println(&quot;加 10 任务开始&quot;);num += 10 ;return num;&#125;catch (Exception e)&#123;return 0 ;&#125;&#125;);futures[ 0 ] = job1;CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; &#123;try&#123;Thread.sleep( 2000 );System.out.println(&quot;乘以 10 任务开始&quot;);num = num * 10 ;return num;&#125;catch (Exception e)&#123;return 1 ;&#125;&#125;);futures[ 1 ] = job2;CompletableFuture&lt;Integer&gt; job3 = CompletableFuture.supplyAsync(() -&gt; &#123;try&#123;Thread.sleep( 3000 );System.out.println(&quot;减以 10 任务开始&quot;);num = num * 10 ;return num;&#125;catch (Exception e)&#123;return 2 ;&#125;&#125;);futures[ 2 ] = job3;CompletableFuture&lt;Integer&gt; job4 = CompletableFuture.supplyAsync(() -&gt; &#123;try&#123;Thread.sleep( 4000 );System.out.println(&quot;除以 10 任务开始&quot;);num = num * 10 ;return num;&#125;catch (Exception e)&#123;return 3 ;&#125;&#125;);futures[ 3 ] = job4;CompletableFuture&lt;Object&gt; future = CompletableFuture.anyOf(futures);System.out.println(future.get());&#125;","categories":["JUC"]},{"title":"Redis","path":"/2023/11/27/Redis7/Redis/","content":"1. NoSQL 数据库简介1. 1. 技术发展技术的分类 1 、解决功能性的问题：Java、Jsp、RDBMS、Tomcat、HTML、Linux、JDBC、SVN 2 、解决扩展性的问题：Struts、Spring、SpringMVC、Hibernate、Mybatis 3 、解决性能的问题：NoSQL、Java线程、Hadoop、Nginx、MQ、ElasticSearch1. 1. 1. Web 1. 0 时代Web 1. 0 的时代，数据访问量很有限，用一夫当关的高性能的单点服务器可以解决大部分问题。 1. 1. 2. Web 2. 0 时代随着 Web 2. 0 的时代的到来，用户访问量大幅度提升，同时产生了大量的用户数据。加上后来的智能移动设备的普及，所有的互联网平台都面临了巨大的性能挑战。 1. 1. 3. 解决 CPU 及内存压力1. 1. 4. 解决 IO 压力1. 2 .NoSQL 数据库1. 2. 1. NoSQL 数据库概述NoSQL(NoSQL&#x3D; NotOnlySQL )，意即“不仅仅是 SQL”，泛指非关系型的数据库。 NoSQL不依赖业务逻辑方式存储，而以简单的key-value模式存储。因此大大的增加了数据库的扩展能力。  不遵循SQL标准。  不支持ACID。  远超于SQL的性能。 1. 2. 2. NoSQL 适用场景 对数据高并发的读写  海量数据的读写  对数据高可扩展性的 1. 2. 3. NoSQL 不适用场景 需要事务支持  基于sql的结构化查询存储，处理复杂的关系,需要即席查询。  （用不着 sql 的和用了 sql 也不行的情况，请考虑用 NoSql ） 1. 2. 4. Memcache很早出现的 NoSql数据库数据都在内存中，一般不持久化支持简单的 key-value模式，支持类型单一一般是作为缓存数据库辅助持久化的数据库 1. 2. 5. Redis几乎覆盖了 Memcached的绝大部分功能数据都在内存中，支持持久化，主要用作备份恢复除了支持简单的 key-value 模式，还支持多种数据结构的存储，比如list、set、hash、zset 等。一般是作为缓存数据库辅助持久化的数据库 1. 2. 6. MongoDB高性能、开源、模式自由(schemafree)的文档型数据库数据都在内存中，如果内存不足，把不常用的数据保存到硬盘虽然是 key-value模式，但是对 value（尤其是 json）提供了丰富的查询功能支持二进制数据及大型对象可以根据数据的特点替代 RDBMS，成为独立的数据库。或者配合 RDBMS，存储特定的数据。 1. 3. 行式存储数据库（大数据时代）1. 3. 1. 行式数据库1. 3. 2. 列式数据库1. 3. 2. 1 .HbaseHBase是 Hadoop 项目中的数据库。它用于需要对大量的数据进行随机、实时的读写操作的场景中。 HBase的目标就是处理数据量非常庞大的表，可以用普通的计算机处理超过 10 亿行数据，还可处理有数百万列元素的数据表。 1. 3. 2. 2 .Cassandra[kəˈsændrə]ApacheCassandra是一款免费的开源 NoSQL数据库，其设计目的在于管理由大量商用服务器构建起来的庞大集群上的海量数据集 ( 数据量通常达到 PB 级别 ) 。在众多显著特性当中，Cassandra最为卓越的长处是对写入及读取操作进行规模调整，而且其不强调主集群的设计思路能够以相对直观的方式简化各集群的创建与扩展流程。 计算机存储单位计算机存储单位一般用B，KB，MB，GB，TB，EB，ZB，YB，BB来表示，它们之间的关系是： 位bit(比特)(BinaryDigits)：存放一位二进制数，即 0 或 1 ，最小的存储单位。 字节byte： 8 个二进制位为一个字节(B)，最常用的单位。 1 KB(Kilobyte千字节)&#x3D; 1024 B， 1 MB(Megabyte兆字节简称“兆”)&#x3D; 1024 KB， 1 GB(Gigabyte吉字节又称“千兆”)&#x3D; 1024 MB， 1 TB(Trillionbyte万亿字节太字节)&#x3D; 1024 GB，其中 1024 &#x3D; 2 ^ 10 ( 2 的 10 次方)， 1 PB（Petabyte千万亿字节拍字节）&#x3D; 1024 TB， 1 EB（Exabyte百亿亿字节艾字节）&#x3D; 1024 PB， 1 ZB(Zettabyte十万亿亿字节泽字节)&#x3D; 1024 EB, 1 YB(Jottabyte一亿亿亿字节尧字节)&#x3D; 1024 ZB, 1 BB(Brontobyte一千亿亿亿字节)&#x3D; 1024 YB. 注：“兆”为百万级数量单位。 1. 4. 图关系型数据库主要应用：社会关系，公共交通网络，地图及网络拓谱(n*(n- 1 )&#x2F; 2 ) 1. 5 .DB-Engines 数据库排名http://db-engines.com/en/ranking 2 .Redis 概述安装 Redis是一个开源的key-value存储系统。  和Memcached类似，它支持存储的 value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sortedset–有序集合)和 hash（哈希类型）。  这些数据类型都支持push&#x2F;pop、add&#x2F;remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。  在此基础上，Redis支持各种不同方式的排序。  与memcached一样，为了保证效率，数据都是缓存在内存中。  区别的是Redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件。  并且在此基础上实现了 master-slave(主从)同步。 2. 1. 应用场景2. 1. 1. 配合关系型数据库做高速缓存 高频次，热门访问的数据，降低数据库IO  分布式架构，做session共享 2. 1. 2. 多样的数据结构存储持久化数据2. 2 .Redis 安装Redis官方网站 Redis中文官方网站http://redis.io http://redis.cn/ 2. 2. 1. 安装版本 6. 2. 1 forLinux（redis- 6. 2. 1 .tar.gz）  不用考虑在windows环境下对Redis的支持 2. 2. 2. 安装步骤2. 2. 2. 1. 准备工作：下载安装最新版的 gcc 编译器安装C语言的编译环境 yuminstallcentos-release-sclscl-utils-buildyuminstall-ydevtoolset- 8 - toolchain sclenabledevtoolset- 8 bash 测试gcc版本 gcc–version 2. 2. 2. 2. 下载 redis- 6. 2. 1 .tar.gz 放&#x2F;opt 目录2. 2. 2. 3. 解压命令：tar-zxvfredis- 6. 2. 1 .tar.gz2. 2. 2. 4. 解压完成后进入目录：cdredis- 6. 2. 12. 2. 2. 5. 在 redis- 6. 2. 1 目录下再次执行 make 命令（只是编译好）2. 2. 2. 6. 如果没有准备好 C 语言编译环境，make 会报错—Jemalloc&#x2F;jemalloc.h：没有那个文件2. 2. 2. 7. 解决方案：运行 makedistclean2. 2. 2. 8. 在 redis- 6. 2. 1 目录下再次执行 make 命令（只是编译好）2. 2. 2. 9. 跳过 maketest继续执行:makeinstall2. 2. 3. 安装目录：&#x2F;usr&#x2F;local&#x2F;bin查看默认安装目录： redis-benchmark:性能测试工具，可以在自己本子运行，看看自己本子性能如何 redis-check-aof：修复有问题的AOF文件，rdb和 aof后面讲 redis-check-dump：修复有问题的dump.rdb文件 redis-sentinel：Redis集群使用 redis-server：Redis服务器启动命令 redis-cli：客户端，操作入口 2. 2. 4. 前台启动（不推荐）前台启动，命令行窗口不能关闭，否则服务器停止 2. 2. 5. 后台启动（推荐）2. 2. 5. 1. 备份 redis.conf拷贝一份redis.conf到其他目录 cp &#x2F;opt&#x2F;redis- 3. 2. 5 &#x2F;redis.conf &#x2F;myredis 2. 2. 5. 2. 后台启动设置 daemonizeno 改成 yes修改redis.conf( 128 行)文件将里面的daemonizeno改成yes，让服务在后台启动 2. 2. 5. 3 .Redis 启动redis-server&#x2F;myredis&#x2F;redis.conf 2. 2. 5. 4. 用客户端访问： redis-cli2. 2. 5. 5. 多个端口可以： redis-cli-p 63792. 2. 5. 6. 测试验证： ping2. 2. 5. 7 .Redis 关闭单实例关闭：redis-clishutdown 也可以进入终端后再关闭 多实例关闭，指定端口关闭：redis-cli-p 6379 shutdown 2. 2. 6. Redis 介绍相关知识端口 6379 从何而来 Alessia Merz 默认 16 个数据库，类似数组下标从 0 开始，初始默认使用 0 号库 使用命令select &lt;dbid&gt;来切换数据库。如:select 8 统一密码管理，所有库同样密码。 dbsize查看当前数据库的key的数量 flushdb清空当前库 flushall通杀全部库 Redis是单线程+多路IO复用技术 多路复用是指使用一个线程来检查多个文件描述符（Socket）的就绪状态，比如调用select和 poll函数，传入多个文件描述符，如果有一个文件描述符就绪，则返回，否则阻塞直到超时。得到就绪状态后进行真正的操作可以在同一个线程里执行，也可以启动线程执行（比如使用线程池） 串行 vs 多线程+锁（memcached）vs 单线程+多路IO复用(Redis) （与Memcache三点不同:支持多数据类型，支持持久化，单线程+多路IO复用） 3. 常用五大数据类型哪里去获得redis常见数据类型操作命令 http://www.redis.cn/commands.html 3. 1 .Redis 键 (key)keys查看当前库所有 key (匹配：keys 1 ) existskey判断某个key是否存在 typekey查看你的key是什么类型 delkey 删除指定的key数据 unlinkkey 根据value选择非阻塞删除 仅将keys从keyspace元数据中删除，真正的删除会在后续异步操作。 expirekey^1010 秒钟：为给定的key设置过期时间 ttlkey查看还有多少秒过期，- 1 表示永不过期，- 2 表示已过期 select命令切换数据库 dbsize查看当前数据库的key的数量 flushdb清空当前库 flushall通杀全部库 3. 2 .Redis 字符串 (String)3. 2. 1. 简介String是Redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。 String类型是二进制安全的。意味着Redis的string可以包含任何数据。比如jpg图片或者序列化的对象。 String类型是Redis最基本的数据类型，一个Redis中字符串 value最多可以是 512 M 3. 2. 2. 常用命令set 添加键值对 *NX：当数据库中 key不存在时，可以将key-value添加数据库 *XX：当数据库中key存在时，可以将key-value添加数据库，与 NX参数互斥 *EX：key的超时秒数 *PX：key的超时毫秒数，与EX互斥 get 查询对应键值 append 将给定的追加到原值的末尾 strlen 获得值的长度 setnx 只有在key不存在时 设置key的值 incr 将key中储存的数字值增 1 只能对数字值操作，如果为空，新增值为 1 decr 将key中储存的数字值减 1 只能对数字值操作，如果为空，新增值为- 1 incrby&#x2F;decrby &lt;步长&gt;将key中储存的数字值增减。自定义步长。 原子性 所谓原子操作是指不会被线程调度机制打断的操作； 这种操作一旦开始，就一直运行到结束，中间不会有任何contextswitch（切换到另一个线程）。 （ 1 ）在单线程中，能够在单条指令中完成的操作都可以认为是&quot;原子操作&quot;，因为中断只能发生于指令之间。 （ 2 ）在多线程中，不能被其它进程（线程）打断的操作就叫原子操作。 Redis单命令的原子性主要得益于Redis的单线程。 案例： java中的i++是否是原子操作？不是 i= 0 ;两个线程分别对i进行++ 100 次,值是多少？ 2 ~ 200 i= 0i++i= 99 i= 1 i++i= 2 i= 0 i++i= 1 i++i= 100 mset &lt;key 1 &gt;&lt;value 1 &gt;&lt;key 2 &gt;&lt;value 2 &gt; ….. 同时设置一个或多个key-value对 mget &lt;key 1 &gt;&lt;key 2 &gt;&lt;key 3 &gt;….. 同时获取一个或多个value msetnx&lt;key 1 &gt;&lt;value 1 &gt;&lt;key 2 &gt;&lt;value 2 &gt; ….. 同时设置一个或多个key-value对，当且仅当所有给定key都不存在。 原子性，有一个失败则都失败 getrange &lt;起始位置&gt;&lt;结束位置&gt; 获得值的范围，类似java中的substring，前包，后包 setrange &lt;起始位置&gt; 用 覆写所储存的字符串值，从&lt;起始位置&gt;开始(索引从 0 开始)。 setex &lt;** 过期时间 **&gt; 设置键值的同时，设置过期时间，单位秒。 getset 以新换旧，设置了新值同时获得旧值。 3. 2. 3. 数据结构String的数据结构为简单动态字符串(SimpleDynamicString,缩写 SDS)。是可以修改的字符串，内部结构实现上类似于Java的 ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配. 如图中所示，内部为当前字符串实际分配的空间capacity一般要高于实际字符串长度len。当字符串长度小于 1 M时，扩容都是加倍现有的空间，如果超过 1 M，扩容时一次只会多扩 1 M的空间。需要注意的是字符串最大长度为 512 M。 3. 3 .Redis 列表 (List)3. 3. 1. 简介单键多值 Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 它的底层实际是个双向链表，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。 3. 3. 2. 常用命令lpush&#x2F;rpush &lt;value 1 &gt;&lt;value 2 &gt;&lt;value 3 &gt;….从左边&#x2F;右边插入一个或多个值。 lpop&#x2F;rpop 从左边&#x2F;右边吐出一个值。值在键在，值光键亡。 rpoplpush &lt;key 1 &gt;&lt;key 2 &gt;从&lt;key 1 &gt;列表右边吐出一个值，插到&lt;key 2 &gt;列表左边。 lrange 按照索引下标获得元素(从左到右) lrangemylist 0 - 1 0 左边第一个，- 1 右边第一个，（ 0 - 1 表示获取所有） lindex按照索引下标获得元素(从左到右) llen获得列表长度 linsert before在的后面插入插入值 lrem从左边删除 n个 value(从左到右) lset将列表 key下标为index的值替换成value 3. 3. 3. 数据结构List的数据结构为快速链表quickList。 首先在列表元素较少的情况下会使用一块连续的内存存储，这个结构是ziplist，也即是压缩列表。 它将所有的元素紧挨着一起存储，分配的是一块连续的内存。 当数据量比较多的时候才会改成quicklist。 因为普通的链表需要的附加指针空间太大，会比较浪费空间。比如这个列表里存的只是int类型的数据，结构上还需要两个额外的指针 prev和 next。 Redis将链表和ziplist结合起来组成了quicklist。也就是将多个 ziplist使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。 3. 4 .Redis 集合 (Set)3. 4. 1. 简介Redisset对外提供的功能与 list类似是一个列表的功能，特殊之处在于 set是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。 Redis的Set是 string类型的无序集合。它底层其实是一个value为 null的 hash表，所以添加，删除，查找的复杂度都是 O( 1 ) 。 一个算法，随着数据的增加，执行时间的长短，如果是O( 1 )，数据增加，查找数据的时间不变 3. 4. 2. 常用命令sadd&lt;value 1 &gt;&lt;value 2 &gt;….. 将一个或多个member元素加入到集合key中，已经存在的member元素将被忽略 smembers取出该集合的所有值。 sismember判断集合是否为含有该值，有 1 ，没有 0 scard返回该集合的元素个数。 srem&lt;value 1 &gt;&lt;value 2 &gt;….删除集合中的某个元素。 spop随机从该集合中吐出一个值。 srandmember随机从该集合中取出 n个值。不会从集合中删除。 smovevalue把集合中一个值从一个集合移动到另一个集合 sinter&lt;key 1 &gt;&lt;key 2 &gt;返回两个集合的交集元素。 sunion&lt;key 1 &gt;&lt;key 2 &gt;返回两个集合的并集元素。 sdiff&lt;key 1 &gt;&lt;key 2 &gt;返回两个集合的差集元素(key 1 中的，不包含key 2 中的) 3. 4. 3. 数据结构Set数据结构是 dict字典，字典是用哈希表实现的。 Java中HashSet的内部实现使用的是HashMap，只不过所有的 value都指向同一个对象。Redis的 set结构也是一样，它的内部也使用hash结构，所有的value都指向同一个内部值。 3. 5 .Redis 哈希 (Hash)3. 5. 1. 简介Redishash是一个键值对集合。 Redishash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 类似Java里面的 Map&lt;String,Object&gt; 用户ID为查找的 key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key&#x2F;value结构来存储 主要有以下 2 种存储方式： 每次修改用户的某个属性需要，先反序列化 用户 ID数据冗余 改好后再序列化回去。开销较大。 通过key(用户 ID)+field( 属性标签 ) 就可以操作对应属性数据了，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题 3. 5. 2. 常用命令hset给集合中的 键赋值 hget&lt;key 1 &gt;从&lt;key 1 &gt;集合取出value hmset&lt;key 1 &gt;&lt;field 1 &gt;&lt;value 1 &gt;&lt;field 2 &gt;&lt;value 2 &gt;…批量设置 hash 的值 hexists&lt;key 1 &gt;查看哈希表key中，给定域field是否存在。 hkeys列出该 hash 集合的所有field hvals列出该 hash集合的所有 value hincrby为哈希表key中的域field的值加上增量 1 - 1 hsetnx将哈希表key中的域field的值设置为value，当且仅当域field不存在. 3. 5. 3. 数据结构Hash类型对应的数据结构是两种：ziplist（压缩列表），hashtable（哈希表）。当field-value长度较短且个数较少时，使用ziplist，否则使用 hashtable。 3. 6 .Redis 有序集合 Zset(sortedset)3. 6. 1. 简介Redis有序集合zset与普通集合 set非常相似，是一个没有重复元素的字符串集合。 不同之处是有序集合的每个成员都关联了一个评分（ score ）,这个评分（score）被用来按照从最低分到最高分的方式排序集合中的成员。集合的成员是唯一的，但是评分可以是重复了。 因为元素是有序的,所以你也可以很快的根据评分（score）或者次序（position）来获取一个范围的元素。 访问有序集合的中间元素也是非常快的,因此你能够使用有序集合作为一个没有重复成员的智能列表。 3. 6. 2. 常用命令zadd &lt;score 1 &gt;&lt;value 1 &gt;&lt;score 2 &gt;&lt;value 2 &gt;… 将一个或多个member元素及其score值加入到有序集key当中。 zrange[WITHSCORES] 返回有序集key中，下标在之间的元素 带WITHSCORES，可以让分数一起和值返回到结果集。 zrangebyscorekeyminmax[withscores][limitoffsetcount] 返回有序集key中，所有score值介于min和max之间(包括等于min或max)的成员。有序集成员按score值递增(从小到大)次序排列。 zrevrangebyscorekeymaxmin[withscores][limitoffsetcount] 同上，改为从大到小排列。 zincrby 为元素的score加上增量 zrem 删除该集合下，指定值的元素 zcount统计该集合，分数区间内的元素个数 zrank返回该值在集合中的排名，从 0 开始。 案例：如何利用zset实现一个文章访问量的排行榜？ 3. 6. 3. 数据结构SortedSet(zset)是Redis提供的一个非常特别的数据结构，一方面它等价于 Java的数据结构Map&lt;String,Double&gt;，可以给每一个元素 value赋予一个权重 score，另一方面它又类似于TreeSet，内部的元素会按照权重 score 进行排序，可以得到每个元素的名次，还可以通过score的范围来获取元素的列表。 zset底层使用了两个数据结构 （ 1 ）hash，hash的作用就是关联元素 value和权重 score，保障元素 value的唯一性，可以通过元素value找到相应的score值。 （ 2 ）跳跃表，跳跃表的目的在于给元素value 排序，根据score的范围获取元素列表。 3. 6. 4. 跳跃表（跳表）1 、简介 有序集合在生活中比较常见，例如根据成绩对学生排名，根据得分对玩家排名等。对于有序集合的底层实现，可以用数组、平衡树、链表等。数组不便元素的插入、删除；平衡树或红黑树虽然效率高但结构复杂；链表查询需要遍历所有效率低。Redis采用的是跳跃表。跳跃表效率堪比红黑树，实现远比红黑树简单。 2 、实例 对比有序链表和跳跃表，从链表中查询出 51 （ 1 ） 有序链表 要查找值为 51 的元素，需要从第一个元素开始依次查找、比较才能找到。共需要 6 次比较。 （ 2 ） 跳跃表 从第 2 层开始， 1 节点比 51 节点小，向后比较。 21 节点比 51 节点小，继续向后比较，后面就是 NULL了，所以从 21 节点向下到第 1 层 在第 1 层， 41 节点比 51 节点小，继续向后， 61 节点比 51 节点大，所以从 41 向下 在第 0 层， 51 节点为要查找的节点，节点被找到，共查找 4 次。 从此可以看出跳跃表比有序链表效率要高 4. Redis 配置文件介绍自定义目录：&#x2F;myredis&#x2F;redis.conf 4. 1 .###Units 单位配置大小单位,开头定义了一些基本的度量单位，只支持 bytes，不支持bit 大小写不敏感 4. 2 .###INCLUDES 包含类似jsp中的include，多实例的情况可以把公用的配置文件提取出来 4. 3 .### 网络相关配置4. 3. 1. b ind默认情况bind&#x3D; 127. 0. 0. 1 只能接受本机的访问请求 不写的情况下，无限制接受任何ip地址的访问 生产环境肯定要写你应用服务器的地址；服务器是需要远程访问的，所以需要将其注释掉 如果开启了protected-mode，那么在没有设定bindip且没有设密码的情况下，Redis只允许接受本机的响应 保存配置，停止服务，重启启动查看进程，不再是本机访问了。 4. 3. 2. p rotected- mode将本机访问保护模式设置no 4. 3. 3. P ort端口号，默认 6379 4. 3. 4. t cp-backlog设置tcp的backlog，backlog其实是一个连接队列，backlog队列总和&#x3D;未完成三次握手队列+已经完成三次握手队列。 在高并发环境下你需要一个高backlog值来避免慢客户端连接问题。 注意Linux内核会将这个值减小到&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;somaxconn的值（ 128 ），所以需要确认增大&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;somaxconn和&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv 4 &#x2F;tcp_max_syn_backlog（ 128 ）两个值来达到想要的效果 4. 3. 5. t imeout一个空闲的客户端维持多少秒会关闭， 0 表示关闭该功能。即永不关闭。 4. 3. 6. t cp-keepalive对访问客户端的一种心跳检测，每个n秒检测一次。 单位为秒，如果设置为 0 ，则不会进行 Keepalive检测，建议设置成 60 4. 4 .###GENERAL 通用4. 4. 1. daemonize是否为后台进程，设置为yes 守护进程，后台启动 4. 4. 2. pidfile存放pid文件的位置，每个实例会产生一个不同的 pid文件 4. 4. 3. loglevel指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为 notice 四个级别根据使用阶段来选择，生产环境选择notice或者warning 4. 4. 4. logfile日志文件名称 4. 4. 5. d atabases 16设定库的数量默认 16 ，默认数据库为 0 ，可以使用 SELECT命令在连接上指定数据库id 4. 5 .###SECURITY 安全4. 5. 1. 设置密码访问密码的查看、设置和取消 在命令中设置密码，只是临时的。重启redis服务器，密码就还原了。 永久设置，需要再配置文件中进行设置。 4. 6 .####LIMITS 限制4. 6. 1. m axclients 设置redis同时可以与多少个客户端进行连接。  默认情况下为 10000 个客户端。  如果达到了此限制，redis则会拒绝新的连接请求，并且向这些连接请求方发出“maxnumberofclientsreached”以作回应。 4. 6. 2. m axmemory 建议必须设置，否则，将内存占满，造成服务器宕机  设置redis可以使用的内存量。一旦到达内存使用上限，redis将会试图移除内部数据，移除规则可以通过maxmemory-policy来指定。  如果redis无法根据移除规则来移除内存中的数据，或者设置了“不允许移除”，那么redis则会针对那些需要申请内存的指令返回错误信息，比如SET、LPUSH等。  但是对于无内存申请的指令，仍然会正常响应，比如GET等。如果你的redis是主redis（说明你的redis有从redis），那么在设置内存使用上限时，需要在系统中留出一些内存空间给同步队列缓存，只有在你设置的是“不移除”的情况下，才不用考虑这个因素。 4. 6. 3. m axmemory-policy volatile-lru：使用LRU算法移除 key，只对设置了过期时间的键；（最近最少使用）  allkeys-lru：在所有集合key中，使用LRU算法移除 key  volatile-random：在过期集合中移除随机的 key，只对设置了过期时间的键  allkeys-random：在所有集合key中，移除随机的 key  volatile-ttl：移除那些TTL值最小的key，即那些最近要过期的 key  noeviction：不进行移除。针对写操作，只是返回错误信息 4. 6. 4. m axmemory-samples 设置样本数量，LRU算法和最小TTL算法都并非是精确的算法，而是估算值，所以你可以设置样本的大小，redis默认会检查这么多个 key并选择其中LRU的那个。  一般设置 3 到^7 的数字，数值越小样本越不准确，但性能消耗越小。 5. Redis 的发布和订阅5. 1. 什么是发布和订阅Redis发布订阅(pub&#x2F;sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。 Redis客户端可以订阅任意数量的频道。 5. 2 .Redis 的发布和订阅1 、客户端可以订阅频道如下图 2 、当给这个频道发布消息后，消息就会发送给订阅的客户端 5. 3. 发布订阅命令行实现1 、打开一个客户端订阅channel 1 SUBSCRIBEchannel 1 2 、打开另一个客户端，给channel 1 发布消息hello publishchannel 1 hello 返回的 1 是订阅者数量 3 、打开第一个客户端可以看到发送的消息 注：发布的消息没有持久化，如果在订阅的客户端收不到hello，只能收到订阅后发布的消息 6. Redis 新数据类型6. 1 .Bitmaps6. 1. 1. 简介现代计算机用二进制（位）作为信息的基础单位， 1 个字节等于 8 位，例如“abc”字符串是由 3 个字节组成，但实际在计算机存储时将其用二进制表示，“abc”分别对应的 ASCII码分别是 97 、 98 、 99 ，对应的二进制分别是 01100001 、 01100010和 01100011 ，如下图 合理地使用操作位能够有效地提高内存使用率和开发效率。Redis提供了Bitmaps这个“数据类型”可以实现对位的操作：（ 1 ） Bitmaps本身不是一种数据类型，实际上它就是字符串（key-value），但是它可以对字符串的位进行操作。（ 2 ） Bitmaps单独提供了一套命令，所以在 Redis中使用 Bitmaps和使用字符串的方法不太相同。可以把 Bitmaps想象成一个以位为单位的数组，数组的每个单元只能存储 0 和 1 ，数组的下标在Bitmaps中叫做偏移量。 6. 1. 2. 命令1 、setbit （ 1 ）格式setbit设置Bitmaps中某个偏移量的值（ 0 或 1 ） *offset:偏移量从 0 开始 （ 2 ）实例每个独立用户是否访问过网站存放在Bitmaps中，将访问的用户记做 1 ，没有访问的用户记做 0 ，用偏移量作为用户的id。设置键的第offset个位的值（从 0 算起），假设现在有 20 个用户，userid&#x3D; 1 ，6 ， 11 ， 15 ， 19 的用户对网站进行了访问，那么当前 Bitmaps初始化结果如图 unique:users: 20201106 代表 2020 - 11 - 06 这天的独立访问用户的Bitmaps 注：很多应用的用户 id以一个指定数字（例如 10000 ）开头，直接将用户 id和Bitmaps的偏移量对应势必会造成一定的浪费，通常的做法是每次做setbit操作时将用户id减去这个指定数字。在第一次初始化Bitmaps时，假如偏移量非常大，那么整个初始化过程执行会比较慢，可能会造成 Redis的阻塞。 2 、getbit（ 1 ）格式getbit获取Bitmaps中某个偏移量的值 获取键的第offset位的值（从 0 开始算） （ 2 ）实例获取id&#x3D; 8 的用户是否在 2020 - 11 - 06 这天访问过，返回 0 说明没有访问过： 注：因为 100 根本不存在，所以也是返回 0 3 、bitcount统计字符串被设置为 1 的bit数。一般情况下，给定的整个字符串都会被进行计数，通过指定额外的start或end参数，可以让计数只在特定的位上进行。start和end参数的设置，都可以使用负数值：比如- 1 表示最后一个位，而- 2 表示倒数第二个位，start、end是指 bit组的字节的下标数，二者皆包含。 （ 1 ）格式bitcount[startend]统计字符串从 start字节到 end字节比特值为 1 的数量 （ 2 ）实例计算 2022 - 11 - 06 这天的独立访问用户数量 start和end代表起始和结束字节数，下面操作计算用户 id在第 1 个字节到第 3 个字节之间的独立访问用户数，对应的用户 id是 11 ， 15 ， 19 。 举例：K 1 【 0100000101000000 0000000000100001 】，对应【 0 ， 1 ， 2 ， 3 】bitcountK 112 ：统计下标 1 、 2 字节组中 bit&#x3D; 1 的个数，即 01000000 00000000 -》bitcountK 112 - -》 1 bitcountK 113 ：统计下标 1 、 2 字节组中 bit&#x3D; 1 的个数，即 01000000 0000000000100001 -》bitcountK 113 - -》 3 bitcountK 10 - 2 ： 统计下标 0 到下标倒数第 2 ，字节组中 bit&#x3D; 1 的个数，即01000001 01000000 00000000 -》bitcountK 10 - 2 - -》 3 注意：redis的setbit设置或清除的是bit位置，而bitcount计算的是byte位置。 4 、bitop( 1 )格式bitop and(or&#x2F;not&#x2F;xor)[key…] bitop是一个复合操作，它可以做多个Bitmaps的and（交集）、or（并集）、not（非）、xor（异或）操作并将结果保存在destkey中。 ( 2 )实例2020 - 11 - 04 日访问网站的 userid&#x3D; 1 , 2 , 5 , 9 。setbitunique:users: 2020110411setbitunique:users: 2020110421setbitunique:users: 2020110451setbitunique:users: 2020110491 2020 - 11 - 03 日访问网站的 userid&#x3D; 0 , 1 , 4 , 9 。setbitunique:users: 2020110301setbitunique:users: 2020110311setbitunique:users: 2020110341setbitunique:users: 2020110391 计算出两天都访问过网站的用户数量bitopandunique:users:and: 20201104 _ 03unique:users: 20201103 unique:users: 20201104 计算出任意一天都访问过网站的用户数量（例如月活跃就是类似这种），可以使用or求并集 6. 1. 3. Bitmaps 与 set 对比假设网站有 1 亿用户，每天独立访问的用户有 5 千万，如果每天用集合类型和Bitmaps分别存储活跃用户可以得到表 set和Bitmaps存储一天活跃用户对比数据类型 每个用户id占用空间 需要存储的用户量 全部内存量集合类型^64 位^5000000064 位*^50000000 =^400 MBBitmaps 1 位 100000000 1 位* 100000000 = 12. 5 MB 很明显，这种情况下使用Bitmaps能节省很多的内存空间，尤其是随着时间推移节省的内存还是非常可观的 set和Bitmaps存储独立用户空间对比数据类型 一天 一个月 一年集合类型^400 MB^12 GB^144 GBBitmaps 12. 5 MB 375 MB 4. 5 GB 但 Bitmaps并不是万金油，假如该网站每天的独立访问用户很少，例如只有10 万（大量的僵尸用户），那么两者的对比如下表所示，很显然，这时候使用Bitmaps就不太合适了，因为基本上大部分位都是 0 。 set和Bitmaps存储一天活跃用户对比（独立用户比较少）数据类型 每个userid占用空间 需要存储的用户量 全部内存量集合类型^64 位^10000064 位*^100000 =^800 KBBitmaps 1 位 100000000 1 位* 100000000 = 12. 5 MB 6. 2 .HyperLogLog6. 2. 1. 简介在工作当中，我们经常会遇到与统计相关的功能需求，比如统计网站 PV（PageView页面访问量）,可以使用 Redis的 incr、incrby轻松实现。但像UV（UniqueVisitor，独立访客）、独立 IP数、搜索记录数等需要去重和计数的问题如何解决？这种求集合中不重复元素个数的问题称为基数问题。解决基数问题有很多种方案：（ 1 ）数据存储在 MySQL表中，使用distinctcount计算不重复个数 （ 2 ）使用 Redis提供的 hash、set、bitmaps等数据结构来处理以上的方案结果精确，但随着数据不断增加，导致占用空间越来越大，对于非常大的数据集是不切实际的。能否能够降低一定的精度来平衡存储空间？Redis推出了HyperLogLogRedisHyperLogLog是用来做基数统计的算法，HyperLogLog的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。在Redis里面，每个HyperLogLog键只需要花费 12 KB内存，就可以计算接近 2 ^ 64 个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。但是，因为HyperLogLog只会根据输入元素来计算基数，而不会储存输入元素本身，所以HyperLogLog不能像集合那样，返回输入的各个元素。 什么是基数?比如数据集{ 1 , 3 , 5 , 7 , 5 , 7 , 8 }，那么这个数据集的基数集为{ 1 , 3 , 5 , 7 , 8 },基数(不重复元素)为 5 。基数估计就是在误差可接受的范围内，快速计算基数。 6. 2. 2. 命令1 、pfadd（ 1 ）格式pfadd[element…] 添加指定元素到HyperLogLog中 （ 2 ）实例 将所有元素添加到指定 HyperLogLog数据结构中。如果执行命令后 HLL估计的近似基数发生变化，则返回 1 ，否则返回 0 。 2 、pfcount（ 1 ）格式pfcount[key…]计算 HLL的近似基数，可以计算多个HLL，比如用 HLL存储每天的UV，计算一周的 UV可以使用 7 天的UV合并计算即可 （ 2 ）实例 3 、pfmerge（ 1 ）格式pfmerge[sourcekey…] 将一个或多个 HLL合并后的结果存储在另一个HLL中，比如每月活跃用户可以使用每天的活跃用户来合并计算可得 （ 2 ）实例 6. 3 .Geospatial6. 3. 1. 简介Redis 3. 2 中增加了对GEO类型的支持。GEO，Geographic，地理信息的缩写。该类型，就是元素的 2 维坐标，在地图上就是经纬度。redis基于该类型，提供了经纬度设置，查询，范围查询，距离查询，经纬度Hash等常见操作。 6. 3. 2. 命令1 、geoadd （ 1 ）格式geoadd[longitudelatitudemember…] 添加地理位置（经度，纬度，名称） （ 2 ）实例geoaddchina:city 121. 4731. 23 shanghaigeoadd china:city 106. 50 29. 53 chongqing 114. 05 22. 52 shenzhen 116. 38 39. 90beijing 两极无法直接添加，一般会下载城市数据，直接通过Java程序一次性导入。有效的经度从 - 180 度到 180 度。有效的纬度从 - 85. 05112878 度到85. 05112878 度。当坐标位置超出指定范围时，该命令将会返回一个错误。已经添加的数据，是无法再次往里面添加的。2 、geopos （ 1 ）格式geopos [member…] 获得指定地区的坐标值 （ 2 ）实例 3 、geodist （ 1 ）格式geodist&lt;member 1 &gt;&lt;member 2 &gt; [m|km|ft|mi] 获取两个位置之间的直线距离 （ 2 ）实例获取两个位置之间的直线距离 单位：m表示单位为米[默认值]。 km表示单位为千米。mi表示单位为英里。ft表示单位为英尺。如果用户没有显式地指定单位参数，那么GEODIST默认使用米作为单位 4 、georadius（ 1 ）格式georadiusradius m|km|ft|mi 以给定的经纬度为中心，找出某一半径内的元素 经度纬度距离单位 （ 2 ）实例 7. Redis_Jedis_测试7. 1 .Jedis 所需要的 jar 包 redis.clients jedis 3. 2. 0 7. 2. 连接 Redis 注意事项禁用Linux的防火墙：Linux(CentOS 7 )里执行命令 systemctlstop&#x2F;disablefirewalld.service redis.conf中注释掉bind 127. 0. 0. 1 ,然后protected-modeno 7. 3 .Jedis 常用操作7. 3. 1. 创建动态的工程7. 3. 2. 创建测试程序packagecom.atguigu.jedis;importredis.clients.jedis.Jedis;publicclassDemo 01 &#123;publicstaticvoidmain(String[]args)&#123;Jedisjedis=newJedis(&quot; 192. 168. 137. 3 &quot;, 6379 );Stringpong=jedis.ping();System.out.println(&quot;连接成功：&quot;+pong); jedis.close();&#125;&#125; 7. 4. 测试相关数据类型7. 4. 1. Jedis-API: Keyjedis. set (&quot;k 1 &quot;,&quot;v 1 &quot;);jedis.set(&quot;k 2 &quot;,&quot;v 2 &quot;);jedis.set(&quot;k 3 &quot;,&quot;v 3 &quot;);Set&lt;String&gt;keys=jedis. keys (&quot;*&quot;);System.out.println(keys.size());for(Stringkey:keys)&#123;System.out.println(key);&#125;System.out.println(jedis. exists (&quot;k 1 &quot;));System.out.println(jedis. ttl (&quot;k 1 &quot;));System.out.println(jedis. get (&quot;k 1 &quot;)); 7. 4. 2. Jedis-API: Stringjedis. mset (&quot;str 1 &quot;,&quot;v 1 &quot;,&quot;str 2 &quot;,&quot;v 2 &quot;,&quot;str 3 &quot;,&quot;v 3 &quot;); System.out.println(jedis. mget (&quot;str 1 &quot;,&quot;str 2 &quot;,&quot;str 3 &quot;)); 7. 4. 3. Jedis-API: ListList&lt;String&gt;list=jedis. lrange (&quot;mylist&quot;, 0 ,- 1 );for(Stringelement:list)&#123;System.out.println(element);&#125; 7. 4. 4. Jedis-API: setjedis.sadd(&quot;orders&quot;,&quot;order 01 &quot;);jedis.sadd(&quot;orders&quot;,&quot;order 02 &quot;);jedis.sadd(&quot;orders&quot;,&quot;order 03 &quot;);jedis.sadd(&quot;orders&quot;,&quot;order 04 &quot;);Set&lt;String&gt;smembers=jedis. smembers (&quot;orders&quot;);for(Stringorder:smembers)&#123;System.out.println(order);&#125;jedis. srem (&quot;orders&quot;,&quot;order 02 &quot;); 7. 4. 5. Jedis-API: hashjedis. hset (&quot;hash 1 &quot;,&quot;userName&quot;,&quot;lisi&quot;);System.out.println(jedis. hget (&quot;hash 1 &quot;,&quot;userName&quot;));Map&lt;String,String&gt;map=newHashMap&lt;String,String&gt;();map.put(&quot;telphone&quot;,&quot; 13810169999 &quot;);map.put(&quot;address&quot;,&quot;atguigu&quot;);map.put(&quot;email&quot;,&quot;abc@ 163 .com&quot;);jedis. hmset (&quot;hash 2 &quot;,map);List&lt;String&gt;result=jedis. hmget (&quot;hash 2 &quot;,&quot;telphone&quot;,&quot;email&quot;);for(Stringelement:result)&#123; System.out.println(element);&#125; 7. 4. 6. Jedis-API: zsetjedis. zadd (&quot;zset 01 &quot;, 100 d,&quot;z 3 &quot;);jedis.zadd(&quot;zset 01 &quot;, 90 d,&quot;l 4 &quot;);jedis.zadd(&quot;zset 01 &quot;, 80 d,&quot;w 5 &quot;);jedis.zadd(&quot;zset 01 &quot;, 70 d,&quot;z 6 &quot;); Set&lt;String&gt;zrange=jedis. zrange (&quot;zset 01 &quot;, 0 ,- 1 );for(Stringe:zrange)&#123;System.out.println(e);&#125; 8. Redis_Jedis_实例8. 1. 完成一个手机验证码功能要求： 1 、输入手机号，点击发送后随机生成 6 位数字码， 2 分钟有效 2 、输入验证码，点击验证，返回成功或失败 3 、每个手机号每天只能输入 3 次 9. Redis 与 SpringBoot 整合SpringBoot整合 Redis 非常简单，只需要按如下步骤整合即可 9. 1. 整合步骤1 、在 pom.xml文件中引入 redis相关依赖 &lt; **dependency** &gt; &lt; **groupId** &gt;org.springframework.boot&lt;&#x2F; **groupId** &gt; &lt; **artifactId** &gt;spring-boot-starter-data-redis&lt;&#x2F; **artifactId** &gt; &lt;&#x2F; **dependency** &gt;","categories":["Redis"]}]